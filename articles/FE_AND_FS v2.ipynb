{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-13 10:32:19,294\tWARNING worker.py:1373 -- WARNING: Not updating worker name since `setproctitle` is not installed. Install this with `pip install setproctitle` (or ray[debug]) to enable monitoring of worker processes.\n",
      "2020-09-13 10:32:19,296\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-09-13_10-32-19_296032_109325/logs.\n",
      "2020-09-13 10:32:19,408\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:10449 to respond...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Not monitoring node memory since `psutil` is not installed. Install this with `pip install psutil` (or ray[debug]) to enable debugging of memory-related crashes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-09-13 10:32:19,532\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:13518 to respond...\n",
      "2020-09-13 10:32:19,534\tINFO services.py:809 -- Starting Redis shard with 10.0 GB max memory.\n",
      "2020-09-13 10:32:19,552\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2020-09-13_10-32-19_296032_109325/logs.\n",
      "2020-09-13 10:32:19,562\tWARNING services.py:1301 -- Warning: Capping object memory store to 20.0GB. To increase this further, specify `object_store_memory` when calling ray.init() or ray start.\n",
      "2020-09-13 10:32:19,579\tINFO services.py:1475 -- Starting the Plasma object store with 20.0 GB memory using /dev/shm.\n",
      "2020-09-13 10:32:19,594\tWARNING services.py:912 -- Failed to start the reporter. The reporter requires 'pip install psutil'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '10.96.22.5',\n",
       " 'redis_address': '10.96.22.5:10449',\n",
       " 'object_store_address': '/tmp/ray/session_2020-09-13_10-32-19_296032_109325/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2020-09-13_10-32-19_296032_109325/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2020-09-13_10-32-19_296032_109325'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import minimize \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from fangorn.files_prep import get_data, data_to_pandas\n",
    "from fangorn.preprocessing import splitting, feature_selection\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "from category_encoders import OneHotEncoder\n",
    "import pymit\n",
    "\n",
    "import ray\n",
    "ray.init(num_cpus=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ML_CHALLENGE files ready!\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "all_datasets = get_data.get_all_data(only='ml_challenge')\n",
    "this_dataset = 'madeline'\n",
    "\n",
    "# configure dataset\n",
    "X_all, y_all = data_to_pandas.read_prepare_data(this_dataset)\n",
    "# X_all = feature_selection.extra_trees_feature_selection(X_all, y_all)\n",
    "dataset_split_dict = splitting.simple_train_test_val_split(X_all, y_all)\n",
    "\n",
    "X_train = dataset_split_dict['train']['X']\n",
    "y_train = dataset_split_dict['train']['y']\n",
    "X_test = dataset_split_dict['test']['X']\n",
    "y_test = dataset_split_dict['test']['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_discretize(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    feat_count = 0\n",
    "    for feat in numeric_features:\n",
    "        if feat_count % 50 == 0:\n",
    "            print(f\" Working in {feat}\")\n",
    "        X_train_np = X_train[[feat]].to_numpy()\n",
    "        X_test_np = X_test[[feat]].to_numpy()\n",
    "        for gran in range(3, max_gran+1):\n",
    "            try:\n",
    "                D_train = np.zeros([X_train.shape[0], 1])\n",
    "                D_test = np.zeros([X_test.shape[0], 1])\n",
    "                # calc numpy histogram and apply to features\n",
    "                hist, bin_edges = np.histogram(X_train_np[:, 0], bins=gran)\n",
    "                D_train[:, 0] = np.digitize(X_train_np[:,0], bin_edges, right=False)\n",
    "                D_test[:, 0] = np.digitize(X_test_np[:,0], bin_edges, right=False)\n",
    "\n",
    "                # apply back to pandas\n",
    "                X_train[f\"{feat}_{gran}\"] = D_train\n",
    "                X_test[f\"{feat}_{gran}\"] = D_test\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        \n",
    "        feat_count += 1\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 259 features...\n",
      " Working in 0\n",
      " Working in 50\n",
      " Working in 100\n",
      " Working in 150\n",
      " Working in 200\n",
      " Working in 250\n"
     ]
    }
   ],
   "source": [
    "X_train_discrete, X_test_discrete = numpy_discretize(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjmi_selector(X, y, bins, max_features):\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    Y = y.to_numpy().ravel()\n",
    "    bins = 10\n",
    "\n",
    "    [tmp, features] = X.shape\n",
    "    D = np.zeros([tmp, features])\n",
    "\n",
    "    for i in range(features):\n",
    "        N, E = np.histogram(X[:,i], bins=bins)\n",
    "        D[:,i] = np.digitize(X[:,i], E, right=False)\n",
    "\n",
    "    selected_features = []\n",
    "    j_h = 0\n",
    "    hjmi = None\n",
    "\n",
    "    for i in range(0,max_features):\n",
    "        JMI = np.zeros([features], dtype=np.float)\n",
    "        for X_k in range(features):\n",
    "            if X_k in selected_features:\n",
    "                continue\n",
    "            jmi_1 = pymit.I(D[:,X_k], Y, bins=[bins,2])\n",
    "            jmi_2 = 0\n",
    "            for X_j in selected_features:\n",
    "                tmp1 = pymit.I(D[:,X_k], D[:,X_j], bins=[bins,bins])\n",
    "                tmp2 = pymit.I_cond(D[:,X_k], D[:,X_j], Y, bins=[bins,bins,2])\n",
    "                jmi_2 += tmp1 - tmp2\n",
    "            if len(selected_features) == 0:\n",
    "                JMI[X_k] += j_h + jmi_1\n",
    "            else:\n",
    "                JMI[X_k] += j_h + jmi_1 - jmi_2/len(selected_features)\n",
    "        \n",
    "        f = JMI.argmax()\n",
    "        j_h = JMI[f]\n",
    "        if (hjmi == None) or ((j_h - hjmi)/hjmi > 0.03):\n",
    "            r = 0\n",
    "            if hjmi != None:\n",
    "                r = ((j_h - hjmi)/hjmi) \n",
    "\n",
    "            hjmi = j_h\n",
    "            selected_features.append(f)\n",
    "            print(\"{:0>3d} {:>3d} {} - {}\".format(len(selected_features), f, j_h, r))\n",
    "        else:\n",
    "            return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 1902 0.05330720180283708 - 0\n",
      "002 687 0.16296850394591655 - 2.0571573527470948\n",
      "003 1479 0.24124708922089674 - 0.48032953226936453\n",
      "004 1535 0.3256292677095313 - 0.3497749082119306\n",
      "005 1303 0.40522943912027903 - 0.24445029763649154\n",
      "006 1903 0.47796290479927034 - 0.17948712175721954\n",
      "007 686 0.5569395618224394 - 0.16523595498762994\n",
      "008 423 0.6293285495907539 - 0.12997637936051878\n",
      "009 567 0.7025048318410445 - 0.11627675607260533\n",
      "010 1534 0.7778947246681412 - 0.10731583529400575\n",
      "011 684 0.8504461209951012 - 0.0932663431519107\n",
      "012 1901 0.9213396957940156 - 0.08336045405905593\n",
      "013 1471 0.9905527228656312 - 0.07512215894699666\n",
      "014 685 1.059457156533011 - 0.06956160139365622\n",
      "015 1533 1.1295119070972939 - 0.06612325013078524\n",
      "016 1087 1.1961512331162913 - 0.058998338663159657\n",
      "017 1487 1.2633314029794234 - 0.05616360875046699\n",
      "018 1900 1.3317803462321318 - 0.054181304360264776\n",
      "019 1863 1.3988168468659294 - 0.05033600384887572\n",
      "020 1470 1.4668449383850863 - 0.048632593803523944\n",
      "021 1531 1.5353419831691155 - 0.046696854583307607\n",
      "022 1486 1.6001822034593525 - 0.04223177702494631\n",
      "023 1899 1.6657466509877354 - 0.04097311380331723\n",
      "024 1478 1.7306037139430572 - 0.03893573066280132\n",
      "025 671 1.794030123338562 - 0.03664987477173053\n",
      "026 1532 1.8591799713271902 - 0.03631480159730478\n",
      "027 1302 1.922164677238381 - 0.03387768095749689\n",
      "028 1431 1.9856102513248528 - 0.033007356153077184\n",
      "029 1583 2.047134742003399 - 0.03098517981436455\n",
      "030 2022 2.108741928150347 - 0.03009434839968426\n"
     ]
    }
   ],
   "source": [
    "selected_features = hjmi_selector(X_train_discrete.copy(), y_train.copy(), bins=10, max_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = [1902, 687, 1479, 1535, 1303, 1903, 686, 423, 567, 1534, 684, 1901, 1471, 685, 1533, 1087, 1487, 1900, 1863, 1470, 1531, 1486, 1478, 671, 1523, 1302, 1431, 1583, 2022]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = X_train_discrete[X_train_discrete.columns[selected_features]]\n",
    "filtered_test = X_test_discrete[X_test_discrete.columns[selected_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = X_train_discrete.copy()\n",
    "filtered_test = X_test_discrete.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(final_X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dummies(X_train, X_test):\n",
    "    ohe = OneHotEncoder(cols=X_train.columns).fit(X=X_train)\n",
    "    X_train = ohe.transform(X_train)\n",
    "    X_test = ohe.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "final_X_train, final_X_test = get_dummies(filtered_train.copy(), filtered_test.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>237_9_1</th>\n",
       "      <th>237_9_2</th>\n",
       "      <th>237_9_3</th>\n",
       "      <th>237_9_4</th>\n",
       "      <th>237_9_5</th>\n",
       "      <th>237_9_6</th>\n",
       "      <th>237_9_7</th>\n",
       "      <th>237_9_8</th>\n",
       "      <th>237_9_9</th>\n",
       "      <th>237_9_10</th>\n",
       "      <th>...</th>\n",
       "      <th>252_9_1</th>\n",
       "      <th>252_9_2</th>\n",
       "      <th>252_9_3</th>\n",
       "      <th>252_9_4</th>\n",
       "      <th>252_9_5</th>\n",
       "      <th>252_9_6</th>\n",
       "      <th>252_9_7</th>\n",
       "      <th>252_9_8</th>\n",
       "      <th>252_9_9</th>\n",
       "      <th>252_9_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2953</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2098</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1636</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1854</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2164</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2009 rows × 289 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      237_9_1  237_9_2  237_9_3  237_9_4  237_9_5  237_9_6  237_9_7  237_9_8  \\\n",
       "2953        1        0        0        0        0        0        0        0   \n",
       "1560        0        1        0        0        0        0        0        0   \n",
       "2098        1        0        0        0        0        0        0        0   \n",
       "71          0        0        1        0        0        0        0        0   \n",
       "1636        0        0        1        0        0        0        0        0   \n",
       "...       ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "2680        1        0        0        0        0        0        0        0   \n",
       "1854        0        0        0        1        0        0        0        0   \n",
       "217         0        0        1        0        0        0        0        0   \n",
       "2164        0        0        0        0        0        1        0        0   \n",
       "1697        0        0        1        0        0        0        0        0   \n",
       "\n",
       "      237_9_9  237_9_10  ...  252_9_1  252_9_2  252_9_3  252_9_4  252_9_5  \\\n",
       "2953        0         0  ...        1        0        0        0        0   \n",
       "1560        0         0  ...        1        0        0        0        0   \n",
       "2098        0         0  ...        0        1        0        0        0   \n",
       "71          0         0  ...        0        0        1        0        0   \n",
       "1636        0         0  ...        0        0        1        0        0   \n",
       "...       ...       ...  ...      ...      ...      ...      ...      ...   \n",
       "2680        0         0  ...        0        0        0        0        1   \n",
       "1854        0         0  ...        0        0        1        0        0   \n",
       "217         0         0  ...        0        1        0        0        0   \n",
       "2164        0         0  ...        0        0        0        1        0   \n",
       "1697        0         0  ...        0        0        0        0        0   \n",
       "\n",
       "      252_9_6  252_9_7  252_9_8  252_9_9  252_9_10  \n",
       "2953        0        0        0        0         0  \n",
       "1560        0        0        0        0         0  \n",
       "2098        0        0        0        0         0  \n",
       "71          0        0        0        0         0  \n",
       "1636        0        0        0        0         0  \n",
       "...       ...      ...      ...      ...       ...  \n",
       "2680        0        0        0        0         0  \n",
       "1854        0        0        0        0         0  \n",
       "217         0        0        0        0         0  \n",
       "2164        0        0        0        0         0  \n",
       "1697        1        0        0        0         0  \n",
       "\n",
       "[2009 rows x 289 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGLOSS: 0.6593040135656834 - AUC: 0.6934654471544716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "def run_initial_logit(X_train, y_train, X_test, y_test):\n",
    "    clf = LogisticRegression().fit(X_train, y_train[y_train.columns[0]])\n",
    "    all_preds = clf.predict_proba(X_test)[:, 1]\n",
    "    logloss = metrics.log_loss(y_test[y_test.columns[0]], all_preds)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[y_test.columns[0]], all_preds, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return clf, logloss, auc\n",
    "\n",
    "start_classifier, logloss, auc = run_initial_logit(final_X_train.copy(), y_train.copy(), final_X_test.copy(), y_test.copy())\n",
    "print(f\"LOGLOSS: {logloss} - AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_iterator(obj_ids):\n",
    "    \"\"\"\n",
    "    # https://github.com/ray-project/ray/issues/5554\n",
    "    \"\"\"\n",
    "    while obj_ids:\n",
    "        done, obj_ids = ray.wait(obj_ids)\n",
    "        yield ray.get(done[0])\n",
    "\n",
    "    \n",
    "def score_one_pair_parallel(combined_features, bsum, y_train):\n",
    "\n",
    "    def _calc_logloss(y_true, preds):\n",
    "        return metrics.log_loss(y_true[y_true.columns[0]], preds['preds'])\n",
    "\n",
    "    def _obj(x, \n",
    "             y_train,\n",
    "             combined_features,\n",
    "             bsum):\n",
    "\n",
    "        # add x to bsum\n",
    "        this_w = np.array(x).reshape(-1,1)\n",
    "        this_value = np.array(combined_features).reshape(1,-1)\n",
    "        bsum_with_new_feature = bsum + np.matmul(this_w.T, this_value)\n",
    "        this_preds = 1/(1 + np.exp(-bsum_with_new_feature)) \n",
    "        preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])\n",
    "        return _calc_logloss(y_train, preds)\n",
    "    \n",
    "    \n",
    "    result = minimize(_obj, 1, args=(y_train,\n",
    "                             combined_features['feature_value'],\n",
    "                             bsum))\n",
    "\n",
    "    this_coef = result['x'][0]\n",
    "    this_logloss = result['fun']\n",
    "    dict_result_combination = {\"coef\":this_coef,\n",
    "                               \"logloss\" : this_logloss,\n",
    "                               \"feature\": combined_features['feature_name']}\n",
    "\n",
    "    return dict_result_combination\n",
    "\n",
    "\n",
    "def iter_one_level(X_train, y_train, X_test, y_test, coef_dict, intercept):\n",
    "    \n",
    "    def _combine_all_features(current_training_set, pairwise_cols):\n",
    "        all_features_combined = []\n",
    "        for feature_pair in pairwise_cols:\n",
    "            feature_name = str(feature_pair)\n",
    "            if feature_name not in current_training_set.columns:\n",
    "                combined_dict = {}\n",
    "                combined_features = current_training_set[feature_pair[0]] | current_training_set[feature_pair[1]]\n",
    "                combined_dict['feature_name'] = str(feature_pair)\n",
    "                combined_dict['feature_value'] = combined_features\n",
    "                if combined_features.equals(current_training_set[feature_pair[0]]) or combined_features.equals(current_training_set[feature_pair[1]]):\n",
    "                    continue\n",
    "                else:\n",
    "                    all_features_combined.append(combined_dict)\n",
    "        return all_features_combined\n",
    "    \n",
    "    def _chunker(seq, size):\n",
    "        return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "    \n",
    "    all_columns = list(X_train)\n",
    "    pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "    all_results = {}\n",
    "    col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "    bsum = np.add(np.matmul(col_coefs, X_train.values.T), intercept)\n",
    "    \n",
    "    print(\"Combinando features do nivel...\")\n",
    "    all_features_combined = _combine_all_features(X_train, pairwise_cols)\n",
    "    print(f\"{len(all_features_combined)} pares criados\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    print(\"Iniciando paralelismo do nivel...\")\n",
    "    chunk = 0\n",
    "    chuncksize = int(10e3)\n",
    "    for group in _chunker(all_features_combined, chuncksize):\n",
    "        print(f\"\\t working in chunk {chunk}\")\n",
    "        score_one_pair_parallel_ray = ray.remote(score_one_pair_parallel)\n",
    "        results = [score_one_pair_parallel_ray.remote(ray.put(one_pair), bsum, ray.put(y_train)) for one_pair in group]\n",
    "        for x in tqdm(to_iterator(results), total=len(results)):\n",
    "            pass\n",
    "\n",
    "        all_results.append(ray.get(results))\n",
    "        chunk += chuncksize\n",
    "        \n",
    "    return sum(all_results, [])\n",
    "\n",
    "def predict_logit(coef_dict, intercept, X_test, y_test):\n",
    "    col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "    bsum = np.add(np.matmul(col_coefs, X_test.values.T), intercept)\n",
    "    this_preds = 1/(1 + np.exp(-bsum)) \n",
    "    preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])\n",
    "    logloss = metrics.log_loss(y_test[y_test.columns[0]], preds['preds'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[y_test.columns[0]],  preds['preds'], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return logloss, auc\n",
    "\n",
    "def beam_search(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def _choose_best_feature(level_results):\n",
    "        min_logloss = 9999\n",
    "        bsf_feature = None\n",
    "        bst_coef = None\n",
    "        for dict_feature in level_results:\n",
    "            if  dict_feature['logloss'] < min_logloss:\n",
    "                min_logloss = dict_feature['logloss']\n",
    "                bsf_feature = dict_feature['feature']\n",
    "                bst_coef = dict_feature['coef']\n",
    "        print(f\"Level - choose {bsf_feature} -{bst_coef} -{min_logloss}\")\n",
    "        return bsf_feature, bst_coef, min_logloss\n",
    "    \n",
    "    current_training_set = X_train.copy()\n",
    "    current_test_set = X_test.copy()\n",
    "    start_classifier, start_logloss, start_auc = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "    \n",
    "    coef_dict = dict(list(zip(current_training_set.columns,start_classifier.coef_[0])))\n",
    "    intercept = start_classifier.intercept_[0]\n",
    "    \n",
    "    print(f\"Start logloss : {start_logloss} - Start AUC {start_auc}\")\n",
    "    last_logloss = start_logloss\n",
    "    this_logloss = -np.inf\n",
    "    accepted_features = []\n",
    "    while this_logloss <= last_logloss:\n",
    "        # eval one level\n",
    "        level_results = iter_one_level(current_training_set, y_train, current_test_set, y_test, coef_dict, intercept)\n",
    "        bst_feature, this_coef, _ = _choose_best_feature(level_results)\n",
    "        \n",
    "        # update X_train an X_test\n",
    "        current_training_set[str(bst_feature)] = current_training_set[str(eval(bst_feature)[0])] | current_training_set[str(eval(bst_feature)[1])]\n",
    "        current_test_set[str(bst_feature)] = current_test_set[str(eval(bst_feature)[0])] | current_test_set[str(eval(bst_feature)[1])]\n",
    "        \n",
    "        # retrain logit with new feature\n",
    "#         this_clf, this_logloss, this_auc = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "#         coef_dict = dict(list(zip(current_training_set.columns,this_clf.coef_[0])))\n",
    "#         intercept = start_classifier.intercept_[0]\n",
    "        coef_dict[bst_feature] = this_coef\n",
    "        \n",
    "        this_logloss, this_auc = predict_logit(coef_dict, intercept, current_test_set, y_test)\n",
    "        print(f\" new  logloss: {this_logloss} - new auc {this_auc}\")\n",
    "        \n",
    "        coef_dict[bst_feature] = this_logloss\n",
    "    \n",
    "        current_logloss_diff = last_logloss - this_logloss\n",
    "        \n",
    "        if this_logloss > last_logloss:\n",
    "            current_training_set =  current_training_set.drop([str(bst_feature)], axis=1)\n",
    "            current_test_set =  current_test_set.drop([str(bst_feature)], axis=1)\n",
    "            coef_dict.pop(str(bst_feature), None)\n",
    "            \n",
    "        else:\n",
    "            last_logloss = this_logloss\n",
    "            \n",
    "        print(f\"logloss gain with {bst_feature}: {current_logloss_diff}\")\n",
    "    return current_training_set, current_test_set, coef_dict, intercept   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logloss : 1.6233913150147528 - Start AUC 0.6328353658536585\n",
      "Combinando features do nivel...\n"
     ]
    }
   ],
   "source": [
    "complete_X_train, complete_X_test, coef_dict, intercept = beam_search(final_X_train, y_train, final_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
