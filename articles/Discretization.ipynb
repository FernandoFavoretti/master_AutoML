{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automate discretization and spare its dependence\n",
    "on human experts, we propose a multi-granularity discretization\n",
    "method. The basic idea is simple: instead of using a fine-tuned\n",
    "granularity, we discretize each numerical feature into several, rather\n",
    "than only one, categorical features, each with a different granularity.\n",
    "Figure 5 gives an illustration of discretizing a numerical feature\n",
    "with four levels of granularity. Since more levels of granularity are\n",
    "considered, it is more likely to get a promising result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar p = 5 primeiramente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import minimize \n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from fangorn.files_prep import get_data, data_to_pandas\n",
    "from fangorn.preprocessing import splitting, feature_selection\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ML_CHALLENGE files ready!\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "all_datasets = get_data.get_all_data(only='ml_challenge')\n",
    "this_dataset = 'madeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dataset\n",
    "X_all, y_all = data_to_pandas.read_prepare_data(this_dataset)\n",
    "X_all = feature_selection.extra_trees_feature_selection(X_all, y_all)\n",
    "dataset_split_dict = splitting.simple_train_test_val_split(X_all, y_all)\n",
    "\n",
    "X_train = dataset_split_dict['train']['X']\n",
    "y_train = dataset_split_dict['train']['y']\n",
    "X_test = dataset_split_dict['test']['X']\n",
    "y_test = dataset_split_dict['test']['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remaining problem is how to determine the\n",
    "levels of granularity. For an experienced user, it can set a group of\n",
    "potentially good values. If no values are specified, AutoCross will\n",
    "use {10^p\n",
    "}\n",
    "P\n",
    "p=1\n",
    "as default values, where P is an integer determined\n",
    "by a rule-based mechanism that considers the available memory,\n",
    "data size and feature numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_numeric_features(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    for feat in numeric_features:\n",
    "        print(f\" Working in {feat}\")\n",
    "        for gran in range(2, max_gran+1):\n",
    "            try:\n",
    "                # calcula retibins no treino e aplica no teste\n",
    "                X_train[f\"{feat}_{gran}\"], this_bins = pd.qcut(X_train[feat],\n",
    "                                               gran,\n",
    "                                               labels= [f\"bin_{i}\" for i in range(gran)],\n",
    "                                               retbins = True\n",
    "                                              )\n",
    "                # aumenta range dos bins para garantir abrangencia\n",
    "                this_bins = np.concatenate(([-np.inf], this_bins[1:-1], [np.inf]))\n",
    "                \n",
    "                # aplicando no teste\n",
    "                X_test[f\"{feat}_{gran}\"] = pd.cut(X_test[feat], bins=this_bins, labels=[f\"bin_{i}\" for i in range(gran)])\n",
    "                \n",
    "                discrete_features.append(f\"{feat}_{gran}\")\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test, discrete_features\n",
    "\n",
    "def get_dummies(X_train, X_test):\n",
    "    ohe = OneHotEncoder(cols=X_train.columns).fit(X=X_train)\n",
    "    X_train = ohe.transform(X_train)\n",
    "    X_test = ohe.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def run_field_wise_minibatch_gradient_descent_lr(this_feature,\n",
    "                                                 X_all,\n",
    "                                                 y_all,\n",
    "                                                 y_feature,\n",
    "                                                 all_features=False,\n",
    "                                                 return_clf = False):\n",
    "    \"\"\"\n",
    "    Run field wise logistic regression\n",
    "    \"\"\"\n",
    "        \n",
    "    clf = linear_model.SGDClassifier(loss='log',\n",
    "                                     n_jobs= -1,\n",
    "                                     warm_start = True)\n",
    "    if all_features:\n",
    "        this_X = X_all\n",
    "        \n",
    "    else:\n",
    "        this_X = pd.get_dummies(X_all[this_feature], columns=this_feature)\n",
    "        \n",
    "#     for i in range(0, 1000):\n",
    "#         this_batch_samples = this_X.sample(frac=0.8)\n",
    "#         this_y = y_all.loc[y_all.index.isin(this_batch_samples.index)]\n",
    "#         clf.partial_fit(this_batch_samples, this_y[y_feature], classes=this_y[y_feature].unique())\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000).fit(this_X, y_all[y_feature])\n",
    "\n",
    "    if return_clf:\n",
    "        all_preds = clf.predict_proba(this_X)\n",
    "        y_all['preds'] = all_preds\n",
    "        final_score = metrics.log_loss(y_all[y_feature], y_all['preds'])\n",
    "        return clf, final_score, y_all\n",
    "   \n",
    "    return clf.score(this_X, y_all)\n",
    "\n",
    "\n",
    "def run_initial_logit(X_train, y_train, X_test, y_test):\n",
    "    clf = LogisticRegression(random_state=0).fit(X_train, y_train[y_train.columns[0]])\n",
    "    all_preds = clf.predict_proba(X_test)\n",
    "    final_score = metrics.log_loss(y_test[y_test.columns[0]], all_preds)\n",
    "    return clf, final_score\n",
    "    \n",
    "\n",
    "def measure_and_clean_discrete_features(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    In order to avoid the dramatic increase in feature number caused\n",
    "    by discretization, once these features are generated, we use fieldwise LR to evaluate them and keep\n",
    "    only the best half. \n",
    "    \"\"\"\n",
    "    \n",
    "    def _select_discrete_features(feature_score, abs=True):\n",
    "        \"\"\"\n",
    "        Select only the best half of measured features\n",
    "        \"\"\"\n",
    "        if abs:\n",
    "            feature_score = {k: np.abs(v) for k,v in feature_score.items()}\n",
    "            \n",
    "        sorted_score = sorted(feature_score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        half_features = int(np.floor(len(sorted_score)*0.1))\n",
    "        return [k[0] for k in sorted_score[:half_features]]\n",
    "    \n",
    "    feature_score = {}\n",
    "    # while what you describe is properly called minibatch learning.\n",
    "    # That's implemented in sklearn.linear_model.SGDClassifier,\n",
    "    # which fits a logistic regression model if you give it the option loss=\"log\".\n",
    "    this_classifier, final_score = run_initial_logit(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    coef_dict = dict(list(zip(X_train.columns, this_classifier.coef_[0])))\n",
    "    selected_features = _select_discrete_features(coef_dict)\n",
    "    \n",
    "    return X_train[selected_features], X_test[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 23 features...\n",
      " Working in 0\n",
      " Working in 1\n",
      " Working in 2\n",
      " Working in 3\n",
      " Working in 4\n",
      " Working in 5\n",
      " Working in 6\n",
      " Working in 7\n",
      " Working in 8\n",
      " Working in 9\n",
      " Working in 10\n",
      " Working in 11\n",
      " Working in 12\n",
      " Working in 13\n",
      " Working in 14\n",
      " Working in 15\n",
      " Working in 16\n",
      " Working in 17\n",
      " Working in 18\n",
      " Working in 19\n",
      " Working in 20\n",
      " Working in 21\n",
      " Working in 22\n"
     ]
    }
   ],
   "source": [
    "X_train_disretized, X_test_discretized, discrete_features = discretize_numeric_features(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy, X_test_dummy = get_dummies(X_train_disretized.copy(), X_test_discretized.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/favoretti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "final_X_train, final_X_test = measure_and_clean_discrete_features(X_train_dummy.copy(), y_train.copy(), X_test_dummy.copy(), y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6552710559170588\n"
     ]
    }
   ],
   "source": [
    "start_classifier, final_score = run_initial_logit(final_X_train.copy(), y_train.copy(), final_X_test.copy(), y_test.copy())\n",
    "print(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_one_pair(X_train, y_train, X_test, y_test, feature_pair, coef_dict, intercept):\n",
    "    \n",
    "    def _calc_logloss(y_true, preds):\n",
    "        return metrics.log_loss(y_true[y_true.columns[0]], preds['preds'])\n",
    "\n",
    "    def _obj(x, \n",
    "             X_train,\n",
    "             y_train,\n",
    "             feature_pair,\n",
    "             coef_dict,\n",
    "             intercept):\n",
    "\n",
    "        combined_features = X_train[feature_pair[0]] | X_train[feature_pair[1]]\n",
    "        col_coefs = np.array(list(coef_dict.values())).reshape(-1,1)\n",
    "        bsum = np.add(X_train.values.dot(col_coefs), intercept)\n",
    "        # add x to bsum\n",
    "        bsum_with_new_feature = bsum + np.sum(np.dot(combined_features.values.reshape(-1,1), x))\n",
    "        this_preds = 1/(1 + np.exp(-bsum_with_new_feature)) \n",
    "        preds = pd.DataFrame(this_preds, columns=['preds'])\n",
    "        return _calc_logloss(y_train, preds)\n",
    "    \n",
    "    \n",
    "\n",
    "#     result = minimize(_obj, 0, args=(X_test,\n",
    "#                                  y_test,\n",
    "#                                  feature_pair,\n",
    "#                                  coef_dict,\n",
    "#                                  intercept), method='Nelder-Mead')\n",
    "    \n",
    "#     this_coef = result['x'][0]\n",
    "#     this_logloss = result['fun']\n",
    "\n",
    "    start_classifier, start_logloss = run_initial_logit(X_train, y_train, X_test, y_test)\n",
    "    coef_dict = dict(list(zip(X_train.columns,start_classifier.coef_[0])))\n",
    "    intercept = start_classifier.intercept_[0]\n",
    "    this_coef = 0\n",
    "    dict_result_combination = {\"coef\":this_coef,\n",
    "                               \"logloss\" : start_logloss}\n",
    "    \n",
    "    return dict_result_combination\n",
    "\n",
    "def iter_one_level(X_train, y_train, X_test, y_test, coef_dict, intercept):\n",
    "    \n",
    "    all_columns = list(X_train)\n",
    "    pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "    all_results = {}\n",
    "    with tqdm(total=len(pairwise_cols)) as pbar:\n",
    "        for feature_pair in pairwise_cols:\n",
    "            feature_name = str(feature_pair)\n",
    "            if feature_name not in X_train.columns:\n",
    "                all_results[feature_name] = score_one_pair(X_train, y_train, X_test, y_test, feature_pair, coef_dict, intercept)\n",
    "                pbar.update(1)\n",
    "    return all_results\n",
    "\n",
    "\n",
    "def beam_search(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def _choose_best_feature(dict_level_results):\n",
    "        min_logloss = 9999\n",
    "        min_key = None\n",
    "        this_coef = None\n",
    "        for key, val in dict_level_results.items():\n",
    "            if val['logloss'] < min_logloss:\n",
    "                min_logloss = val['logloss']\n",
    "                min_key = key\n",
    "                this_coef = val['coef']\n",
    "        print(f\"Level - choose {min_key}\")\n",
    "        return min_key, this_coef, min_logloss\n",
    "    \n",
    "    current_training_set = X_train.copy()\n",
    "    current_test_set = X_test.copy()\n",
    "    start_classifier, start_logloss = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "    \n",
    "    coef_dict = dict(list(zip(current_training_set.columns,start_classifier.coef_[0])))\n",
    "    intercept = start_classifier.intercept_[0]\n",
    "    \n",
    "    print(f\"Start score : {start_logloss}\")\n",
    "    last_logloss = start_logloss\n",
    "    accepted_features = []\n",
    "    while last_logloss <= start_logloss:\n",
    "        # eval one level\n",
    "        dict_level_results = iter_one_level(current_training_set, y_train, current_test_set, y_test, coef_dict, intercept)\n",
    "        bst_feature, this_coef, this_logloss = _choose_best_feature(dict_level_results)\n",
    "        \n",
    "        # update X_train an X_test\n",
    "        current_training_set[str(bst_feature)] = current_training_set[str(eval(bst_feature)[0])] | current_training_set[str(eval(bst_feature)[1])]\n",
    "        current_test_set[str(bst_feature)] = current_test_set[str(eval(bst_feature)[0])] | current_test_set[str(eval(bst_feature)[1])]\n",
    "        \n",
    "        # retrain logit with new feature\n",
    "        this_clf, this_logloss = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "        print(f\" retrained log loss of {this_logloss}\")\n",
    "        coef_dict = dict(list(zip(current_training_set.columns,this_clf.coef_[0])))\n",
    "        intercept = start_classifier.intercept_[0]\n",
    "        \n",
    "        current_logloss_diff = last_logloss - this_logloss\n",
    "        if this_logloss >= last_logloss:\n",
    "            current_training_set =  current_training_set.drop([str(bst_feature)], axis=1)\n",
    "            current_test_set =  current_test_set.drop([str(bst_feature)], axis=1)\n",
    "            \n",
    "        last_logloss = this_logloss\n",
    "        print(f\"logloss gain with {bst_feature}: {current_logloss_diff}\")\n",
    "    return current_training_set, current_test_set   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/7626 [00:00<04:12, 30.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start score : 0.6552710559170588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7626/7626 [04:15<00:00, 29.82it/s]\n",
      "  0%|          | 3/7750 [00:00<04:42, 27.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('5_7_1', '19_8_1')\n",
      " retrained log loss of 0.6529391895209425\n",
      "logloss gain with ('5_7_1', '19_8_1'): 0.0023318663961162667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1242/7750 [00:45<04:00, 27.11it/s]"
     ]
    }
   ],
   "source": [
    "complete_X_train, complete_X_test = beam_search(final_X_train, y_train, final_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NOX_10_bin_8', 'NOX_5_bin_4', 'RM_8_bin_2', 'AGE_8_bin_0',\n",
       "       'CRIM_8_bin_7', 'CRIM_8_bin_5', 'NOX_10_bin_5', 'AGE_10_bin_0',\n",
       "       'NOX_5_bin_2', 'NOX_9_bin_5',\n",
       "       ...\n",
       "       'CRIM_10_bin_1', '('NOX_10_bin_8', 'NOX_5_bin_4')',\n",
       "       '('NOX_10_bin_8', 'RM_8_bin_2')', '('NOX_10_bin_8', 'AGE_8_bin_0')',\n",
       "       '('NOX_10_bin_8', 'CRIM_8_bin_7')', '('NOX_10_bin_8', 'CRIM_8_bin_5')',\n",
       "       '('NOX_10_bin_8', 'NOX_10_bin_5')', '('NOX_10_bin_8', 'AGE_10_bin_0')',\n",
       "       '('NOX_10_bin_8', 'NOX_5_bin_2')', '('NOX_10_bin_8', 'NOX_9_bin_5')'],\n",
       "      dtype='object', length=168)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = t.columns\n",
    "pairwise_cols = list(itertools.combinations(all_columns, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(random_state=0),\n",
       " 1.228669507272974,\n",
       "      bin_lstat  preds\n",
       " 0            0      0\n",
       " 1            0      0\n",
       " 2            0      0\n",
       " 3            0      0\n",
       " 4            0      0\n",
       " ..         ...    ...\n",
       " 501          0      0\n",
       " 502          0      0\n",
       " 503          0      0\n",
       " 504          0      0\n",
       " 505          0      1\n",
       " \n",
       " [506 rows x 2 columns])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_field_wise_minibatch_gradient_descent_lr(t.columns,\n",
    "                                            'bin_lstat',\n",
    "                                            t.copy(),\n",
    "                                            y_all.copy(),\n",
    "                                            all_features=True,\n",
    "                                            return_clf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5351057228771896\n",
      "0.514191844462476\n",
      "0.44325648735939416\n",
      "0.47713712949262566\n",
      "0.5020683181391501\n",
      "0.3120932970582152\n",
      "0.38352186848678665\n",
      "0.2823015607846881\n",
      "0.42472912987446904\n",
      "0.5869091371931332\n",
      "0.3718836014191844\n",
      "0.4879480693045678\n",
      "0.4160421936900387\n",
      "0.35687238477081445\n",
      "0.3774521502553578\n",
      "0.5063799659523014\n",
      "0.3926781537874088\n",
      "0.5935675305872432\n",
      "0.48568087442126867\n",
      "0.6574228755986189\n",
      "0.4016116971345839\n",
      "0.3151003134297488\n",
      "0.25449063688288553\n",
      "0.5003818433487662\n",
      "0.3852162983469365\n",
      "0.5034922756272573\n",
      "0.3169140693363881\n",
      "0.3631250696068605\n",
      "0.3733155139770576\n",
      "0.5591300335703944\n",
      "0.5270154169252065\n",
      "0.47917362735271196\n",
      "0.5523523141297949\n",
      "0.46956390307543\n",
      "0.4224698900609359\n",
      "0.6431355702989515\n",
      "0.6554500182966605\n",
      "0.44380538717324547\n",
      "0.6431992108570792\n",
      "0.5859465737514519\n",
      "0.38883585509044916\n",
      "0.364692218350755\n",
      "0.5630598380347795\n",
      "0.5472053839912177\n",
      "0.436820835918731\n",
      "0.4804543935850317\n",
      "0.633637216998393\n",
      "0.4551572717292731\n",
      "0.5200785960892876\n",
      "0.4042050498782874\n",
      "0.6557523109477671\n",
      "0.3814217300685727\n",
      "0.46355782540212875\n",
      "0.5212957217634799\n",
      "0.6625379854581324\n",
      "0.49316659507103877\n",
      "0.4394062335926686\n",
      "0.4245700284791498\n",
      "0.5813564984964918\n",
      "0.6189839784894914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-aa9f82f566c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                             \u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                             \u001b[0mall_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                             return_clf=True)\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin_lstat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-7a827757bd18>\u001b[0m in \u001b[0;36mrun_field_wise_minibatch_gradient_descent_lr\u001b[1;34m(this_feature, y_feature, X_all, y_all, all_features, return_clf)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mthis_batch_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mthis_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_batch_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_batch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_feature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthis_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_feature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_clf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[0;32m    692\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m                                  \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                                  coef_init=None, intercept_init=None)\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    523\u001b[0m                              \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                              \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                              max_iter=max_iter)\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[1;34m(self, X, y, alpha, C, sample_weight, learning_rate, max_iter)\u001b[0m\n\u001b[0;32m    582\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                                               \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                                               random_state=self.random_state)\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit_binary\u001b[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0my_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     dataset, intercept_decay = make_dataset(\n\u001b[0;32m    415\u001b[0m         X, y_i, sample_weight, random_state=random_state)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_random_state\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.BitGenerator.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.SeedSequence.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.SeedSequence.get_assembled_entropy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in pairwise_cols:\n",
    "    t['x'] = t[col[0]] | t[col[1]]\n",
    "    _,_, preds = run_field_wise_minibatch_gradient_descent_lr(t.columns,\n",
    "                                            'bin_lstat',\n",
    "                                            t.copy(),\n",
    "                                            y_all.copy(),\n",
    "                                            all_features=True,\n",
    "                                            return_clf=True)\n",
    "    print(roc_auc_score(preds['bin_lstat'], preds['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4837637026076718"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "all_columns = list(original_feature_set)\n",
    "pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "all_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cols = pairwise_cols[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(original_feature_set, new_coef_dict):\n",
    "    col_coefs = np.array(list(new_coef_dict.values())).reshape(-1,1)\n",
    "    this_pred = np.add(np.round(original_feature_set.values.dot(col_coefs),3), intercept)\n",
    "    return 1/(1 + np.exp(-this_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.218"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coef_dict = coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_set['teste_feature'] = original_feature_set['DIS_9_bin_1'] | original_feature_set['DIS_9_bin_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coef_dict['teste_feature'] = 0.218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all['new_preds'] = make_preds(original_feature_set, new_coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.60009471884184"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.log_loss(y_all['bin_lstat'], y_all['new_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_lstat</th>\n",
       "      <th>new_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bin_lstat  new_preds\n",
       "0            0        0.0\n",
       "1            0        0.0\n",
       "2            0        0.0\n",
       "3            0        1.0\n",
       "4            0        0.0\n",
       "..         ...        ...\n",
       "501          0        1.0\n",
       "502          0        1.0\n",
       "503          0        1.0\n",
       "504          0        1.0\n",
       "505          0        0.0\n",
       "\n",
       "[506 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -18.59017554414894\n",
       " hess_inv: array([[1]])\n",
       "      jac: array([0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 1\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-5.05])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.552074997367714e-17\n",
       " hess_inv: array([[0.50000004]])\n",
       "      jac: array([-1.28826571e-12])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 21\n",
       "      nit: 4\n",
       "     njev: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-7.45122473e-09])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x, a,b,c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "minimize(fun, 100, args=(1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.5860022592398136\n",
       " hess_inv: array([[1]])\n",
       "      jac: array([0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 3\n",
       "      nit: 0\n",
       "     njev: 1\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['teste'] = np.round(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5860022592398136"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(preds['bin_lstat'], preds['teste'], pos_label=1)\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_lstat</th>\n",
       "      <th>preds</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bin_lstat  preds  residual\n",
       "169          1  1.000     0.000\n",
       "447          1  0.484     0.516\n",
       "490          1  1.000     0.000\n",
       "473          1  1.000     0.000\n",
       "438          1  1.000     0.000\n",
       "149          1  1.000     0.000\n",
       "165          0  1.000    -1.000\n",
       "496          1  1.000     0.000\n",
       "337          1  0.000     1.000\n",
       "227          0  1.000    -1.000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals.sort_values(['residual']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000    295\n",
       " 1.000    133\n",
       "-1.000     53\n",
       " 0.996      2\n",
       "-0.199      2\n",
       " 0.990      1\n",
       " 0.125      1\n",
       "-0.108      1\n",
       "-0.044      1\n",
       " 0.218      1\n",
       "-0.036      1\n",
       "-0.120      1\n",
       "-0.790      1\n",
       "-0.976      1\n",
       "-0.998      1\n",
       "-0.999      1\n",
       " 0.516      1\n",
       "-0.001      1\n",
       "-0.846      1\n",
       "-0.958      1\n",
       "-0.917      1\n",
       "-0.996      1\n",
       "-0.952      1\n",
       " 0.979      1\n",
       " 0.998      1\n",
       " 0.975      1\n",
       "Name: residual, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals['residual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/ (1 * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
