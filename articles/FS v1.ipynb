{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import minimize \n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from fangorn.files_prep import get_data, data_to_pandas\n",
    "from fangorn.preprocessing import splitting, feature_selection\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('arcene_train.data', sep=' ', header=None)\n",
    "y_train = pd.read_csv('arcene_train.labels', sep=' ', header=None)\n",
    "\n",
    "X_valid = pd.read_csv('arcene_valid.data', sep=' ', header=None)\n",
    "y_valid = pd.read_csv('arcene_valid.labels', sep=' ', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_discretize(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    feat_count = 0\n",
    "    for feat in numeric_features:\n",
    "        if feat_count % 50 == 0:\n",
    "            print(f\" Working in {feat}\")\n",
    "        X_train_np = X_train[[feat]].to_numpy()\n",
    "        X_test_np = X_test[[feat]].to_numpy()\n",
    "        for gran in range(10, max_gran+1):\n",
    "            try:\n",
    "                D_train = np.zeros([X_train.shape[0], 1])\n",
    "                D_test = np.zeros([X_test.shape[0], 1])\n",
    "                # calc numpy histogram and apply to features\n",
    "                hist, bin_edges = np.histogram(X_train_np[:, 0], bins=gran)\n",
    "                D_train[:, 0] = np.digitize(X_train_np[:,0], bin_edges, right=False)\n",
    "                D_test[:, 0] = np.digitize(X_test_np[:,0], bin_edges, right=False)\n",
    "\n",
    "                # apply back to pandas\n",
    "                X_train[f\"{feat}_{gran}\"] = D_train\n",
    "                X_test[f\"{feat}_{gran}\"] = D_test\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        \n",
    "        feat_count += 1\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 10001 features...\n",
      " Working in 0\n",
      " Working in 50\n",
      " Working in 100\n",
      " Working in 150\n",
      " Working in 200\n",
      " Working in 250\n",
      " Working in 300\n",
      " Working in 350\n",
      " Working in 400\n",
      " Working in 450\n",
      " Working in 500\n",
      " Working in 550\n",
      " Working in 600\n",
      " Working in 650\n",
      " Working in 700\n",
      " Working in 750\n",
      " Working in 800\n",
      " Working in 850\n",
      " Working in 900\n",
      " Working in 950\n",
      " Working in 1000\n",
      " Working in 1050\n",
      " Working in 1100\n",
      " Working in 1150\n",
      " Working in 1200\n",
      " Working in 1250\n",
      " Working in 1300\n",
      " Working in 1350\n",
      " Working in 1400\n",
      " Working in 1450\n",
      " Working in 1500\n",
      " Working in 1550\n",
      " Working in 1600\n",
      " Working in 1650\n",
      " Working in 1700\n",
      " Working in 1750\n",
      " Working in 1800\n",
      " Working in 1850\n",
      " Working in 1900\n",
      " Working in 1950\n",
      " Working in 2000\n",
      " Working in 2050\n",
      " Working in 2100\n",
      " Working in 2150\n",
      " Working in 2200\n",
      " Working in 2250\n",
      " Working in 2300\n",
      " Working in 2350\n",
      " Working in 2400\n",
      " Working in 2450\n",
      " Working in 2500\n",
      " Working in 2550\n",
      " Working in 2600\n",
      " Working in 2650\n",
      " Working in 2700\n",
      " Working in 2750\n",
      " Working in 2800\n",
      " Working in 2850\n",
      " Working in 2900\n",
      " Working in 2950\n",
      " Working in 3000\n",
      " Working in 3050\n",
      " Working in 3100\n",
      " Working in 3150\n",
      " Working in 3200\n",
      " Working in 3250\n",
      " Working in 3300\n",
      " Working in 3350\n",
      " Working in 3400\n",
      " Working in 3450\n",
      " Working in 3500\n",
      " Working in 3550\n",
      " Working in 3600\n",
      " Working in 3650\n",
      " Working in 3700\n",
      " Working in 3750\n",
      " Working in 3800\n",
      " Working in 3850\n",
      " Working in 3900\n",
      " Working in 3950\n",
      " Working in 4000\n",
      " Working in 4050\n",
      " Working in 4100\n",
      " Working in 4150\n",
      " Working in 4200\n",
      " Working in 4250\n",
      " Working in 4300\n",
      " Working in 4350\n",
      " Working in 4400\n",
      " Working in 4450\n",
      " Working in 4500\n",
      " Working in 4550\n",
      " Working in 4600\n",
      " Working in 4650\n",
      " Working in 4700\n",
      " Working in 4750\n",
      " Working in 4800\n",
      " Working in 4850\n",
      " Working in 4900\n",
      " Working in 4950\n",
      " Working in 5000\n",
      " Working in 5050\n",
      " Working in 5100\n",
      " Working in 5150\n",
      " Working in 5200\n",
      " Working in 5250\n",
      " Working in 5300\n",
      " Working in 5350\n",
      " Working in 5400\n",
      " Working in 5450\n",
      " Working in 5500\n",
      " Working in 5550\n",
      " Working in 5600\n",
      " Working in 5650\n",
      " Working in 5700\n",
      " Working in 5750\n",
      " Working in 5800\n",
      " Working in 5850\n",
      " Working in 5900\n",
      " Working in 5950\n",
      " Working in 6000\n",
      " Working in 6050\n",
      " Working in 6100\n",
      " Working in 6150\n",
      " Working in 6200\n",
      " Working in 6250\n",
      " Working in 6300\n",
      " Working in 6350\n",
      " Working in 6400\n",
      " Working in 6450\n",
      " Working in 6500\n",
      " Working in 6550\n",
      " Working in 6600\n",
      " Working in 6650\n",
      " Working in 6700\n",
      " Working in 6750\n",
      " Working in 6800\n",
      " Working in 6850\n",
      " Working in 6900\n",
      " Working in 6950\n",
      " Working in 7000\n",
      " Working in 7050\n",
      " Working in 7100\n",
      " Working in 7150\n",
      " Working in 7200\n",
      " Working in 7250\n",
      " Working in 7300\n",
      " Working in 7350\n",
      " Working in 7400\n",
      " Working in 7450\n",
      " Working in 7500\n",
      " Working in 7550\n",
      " Working in 7600\n",
      " Working in 7650\n",
      " Working in 7700\n",
      " Working in 7750\n",
      " Working in 7800\n",
      " Working in 7850\n",
      " Working in 7900\n",
      " Working in 7950\n",
      " Working in 8000\n",
      " Working in 8050\n",
      " Working in 8100\n",
      " Working in 8150\n",
      " Working in 8200\n",
      " Working in 8250\n",
      " Working in 8300\n",
      " Working in 8350\n",
      " Working in 8400\n",
      " Working in 8450\n",
      " Working in 8500\n",
      " Working in 8550\n",
      " Working in 8600\n",
      " Working in 8650\n",
      " Working in 8700\n",
      " Working in 8750\n",
      " Working in 8800\n",
      " Working in 8850\n",
      " Working in 8900\n",
      " Working in 8950\n",
      " Working in 9000\n",
      " Working in 9050\n",
      " Working in 9100\n",
      " Working in 9150\n",
      " Working in 9200\n",
      " Working in 9250\n",
      " Working in 9300\n",
      " Working in 9350\n",
      " Working in 9400\n",
      " Working in 9450\n",
      " Working in 9500\n",
      " Working in 9550\n",
      " Working in 9600\n",
      " Working in 9650\n",
      " Working in 9700\n",
      " Working in 9750\n",
      " Working in 9800\n",
      " Working in 9850\n",
      " Working in 9900\n",
      " Working in 9950\n",
      " Working in 10000\n",
      "Not possible to correct work on cut 10000 > 10\n"
     ]
    }
   ],
   "source": [
    "X_train_discrete, X_test_discrete = numpy_discretize(X_train.copy(), X_valid.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymit\n",
    "\n",
    "def hjmi_selector(X, y, bins, max_features):\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    Y = y.to_numpy().ravel()\n",
    "    bins = 10\n",
    "\n",
    "    [tmp, features] = X.shape\n",
    "    D = np.zeros([tmp, features])\n",
    "\n",
    "    for i in range(features):\n",
    "        N, E = np.histogram(X[:,i], bins=bins)\n",
    "        D[:,i] = np.digitize(X[:,i], E, right=False)\n",
    "\n",
    "    selected_features = []\n",
    "    j_h = 0\n",
    "    hjmi = None\n",
    "\n",
    "    for i in range(0,max_features):\n",
    "        JMI = np.zeros([features], dtype=np.float)\n",
    "        for X_k in range(features):\n",
    "            if X_k in selected_features:\n",
    "                continue\n",
    "            jmi_1 = pymit.I(D[:,X_k], Y, bins=[bins,2])\n",
    "            jmi_2 = 0\n",
    "            for X_j in selected_features:\n",
    "                tmp1 = pymit.I(D[:,X_k], D[:,X_j], bins=[bins,bins])\n",
    "                tmp2 = pymit.I_cond(D[:,X_k], D[:,X_j], Y, bins=[bins,bins,2])\n",
    "                jmi_2 += tmp1 - tmp2\n",
    "            if len(selected_features) == 0:\n",
    "                JMI[X_k] += j_h + jmi_1\n",
    "            else:\n",
    "                JMI[X_k] += j_h + jmi_1 - jmi_2/len(selected_features)\n",
    "        \n",
    "        f = JMI.argmax()\n",
    "        j_h = JMI[f]\n",
    "        if (hjmi == None) or ((j_h - hjmi)/hjmi > 0.03):\n",
    "            r = 0\n",
    "            if hjmi != None:\n",
    "                r = ((j_h - hjmi)/hjmi) \n",
    "\n",
    "            hjmi = j_h\n",
    "            selected_features.append(f)\n",
    "            print(\"{:0>3d} {:>3d} {} - {}\".format(len(selected_features), f, j_h, r))\n",
    "        else:\n",
    "            break\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 7647 0.34408924535842605 - 0\n",
      "002 3920 0.728588866520957 - 1.1174415543328324\n",
      "003 2146 1.1466533521469633 - 0.573800266290483\n",
      "004 4367 1.6147014196891383 - 0.40818619390578176\n",
      "005 131 2.096194570800886 - 0.298193303876852\n",
      "006 5573 2.54426955247536 - 0.2137563887990034\n",
      "007 829 2.9948120955917648 - 0.17708129340229095\n",
      "008 5872 3.4476955788856 - 0.15122267068456824\n",
      "009 5231 3.895558287992757 - 0.1299020458331533\n",
      "010 3780 4.336451342146714 - 0.1131784000031311\n",
      "011 6845 4.780944406909994 - 0.10250156860822474\n",
      "012 4465 5.215701414840886 - 0.09093538241158583\n",
      "013 2097 5.667638058867056 - 0.08664925540795305\n",
      "014 7563 6.111956398123292 - 0.07839567993603555\n",
      "015 4240 6.543160249579703 - 0.07055087166341935\n",
      "016 8825 6.972529246750594 - 0.06562104255332449\n",
      "017 6984 7.402789314584174 - 0.061707890007645955\n",
      "018 731 7.833103224711253 - 0.05812861771971825\n",
      "019 9407 8.258159770742989 - 0.054264131831022126\n",
      "020 8359 8.682040943611577 - 0.051328768713135686\n",
      "021 9465 9.110715668400303 - 0.04937487942903028\n",
      "022 895 9.536501569872813 - 0.04673462733002527\n",
      "023 2060 9.965336202876758 - 0.04496770958007233\n",
      "024 6424 10.390581941335427 - 0.04267249290956288\n",
      "025 4112 10.808501320897868 - 0.04022097914457419\n",
      "026 9644 11.229640545513481 - 0.03896370200754429\n",
      "027 1136 11.649181394555663 - 0.037360131639280196\n",
      "028 1515 12.071578717361236 - 0.03625982878101489\n",
      "029 5674 12.49749140790956 - 0.03528226924749957\n",
      "030 8151 12.924022501874898 - 0.0341293368439837\n",
      "031 3789 13.345798742378502 - 0.032635059281459516\n",
      "032 7424 13.771513727421617 - 0.03189880150757042\n",
      "033 9123 14.19663788421583 - 0.030869820501119864\n",
      "034 8915 14.624906880756011 - 0.030166931074317328\n"
     ]
    }
   ],
   "source": [
    "selected_features = hjmi_selector(X_train_discrete.copy(), y_train.copy(), bins=10, max_features=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = X_train_discrete[X_train_discrete.columns[selected_features]]\n",
    "filtered_test = X_test_discrete[X_test_discrete.columns[selected_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/ipykernel_launcher.py:3: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(filtered_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_valid, neigh.predict(filtered_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MADELON_TRAIN = \"madelon_train.data\"\n",
    "MADELON_TRAIN_LABELS = \"madelon_train.labels\"\n",
    "\n",
    "data_raw = np.loadtxt(MADELON_TRAIN,dtype=np.float)\n",
    "labels = np.loadtxt(MADELON_TRAIN_LABELS,dtype=np.float)\n",
    "\n",
    "X = data_raw\n",
    "Y = labels\n",
    "[tmp, features] = X.shape\n",
    "D = np.zeros([tmp, features])\n",
    "for i in range(features):\n",
    "    N, E = np.histogram(X[:,i], bins=10)\n",
    "    D[:,i] = np.digitize(X[:,i], E, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-3b21fbdec187>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pymit\n",
    "\n",
    "MADELON_TRAIN = \"madelon_train.data\"\n",
    "MADELON_TRAIN_LABELS = \"madelon_train.labels\"\n",
    "\n",
    "data_raw = numpy.loadtxt(MADELON_TRAIN,dtype=numpy.float)\n",
    "labels = numpy.loadtxt(MADELON_TRAIN_LABELS,dtype=numpy.float)\n",
    "\n",
    "X = data_raw\n",
    "Y = labels\n",
    "bins = 10\n",
    "\n",
    "[tmp, features] = X.shape\n",
    "D = numpy.zeros([tmp, features])\n",
    "\n",
    "for i in range(features):\n",
    "    N, E = numpy.histogram(X[:,i], bins=bins)\n",
    "    D[:,i] = numpy.digitize(X[:,i], E, right=False)\n",
    "\n",
    "# max_features = 200\n",
    "# selected_features = []\n",
    "# j_h = 0\n",
    "# hjmi = None\n",
    "\n",
    "# for i in range(0,max_features):\n",
    "#     JMI = numpy.zeros([features], dtype=numpy.float)\n",
    "#     for X_k in range(features):\n",
    "#         if X_k in selected_features:\n",
    "#             continue\n",
    "#         jmi_1 = pymit.I(D[:,X_k], Y, bins=[bins,2])\n",
    "#         jmi_2 = 0\n",
    "#         for X_j in selected_features:\n",
    "#             tmp1 = pymit.I(D[:,X_k], D[:,X_j], bins=[bins,bins])\n",
    "#             tmp2 = pymit.I_cond(D[:,X_k], D[:,X_j], Y, bins=[bins,bins,2])\n",
    "#             jmi_2 += tmp1 - tmp2\n",
    "#         if len(selected_features) == 0:\n",
    "#             JMI[X_k] += j_h + jmi_1\n",
    "#         else:\n",
    "#             JMI[X_k] += j_h + jmi_1 - jmi_2/len(selected_features)\n",
    "#     f = JMI.argmax()\n",
    "#     j_h = JMI[f]\n",
    "#     if (hjmi == None) or ((j_h - hjmi)/hjmi > 0.03):\n",
    "#         r = 0\n",
    "#         if hjmi != None:\n",
    "#             r = ((j_h - hjmi)/hjmi) \n",
    "        \n",
    "#         hjmi = j_h\n",
    "#         selected_features.append(f)\n",
    "#         print(\"{:0>3d} {:>3d} {} - {}\".format(len(selected_features), f, j_h, r))\n",
    "#     else:\n",
    "#         break    \n",
    "\n",
    "# expected_features=[241, 338, 378, 105, 472, 475, 433, 64, 128, 442, 453, 336, 48, 493, 281, 318, 153, 28, 451, 455]\n",
    "# assert(expected_features == selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
