{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automate discretization and spare its dependence\n",
    "on human experts, we propose a multi-granularity discretization\n",
    "method. The basic idea is simple: instead of using a fine-tuned\n",
    "granularity, we discretize each numerical feature into several, rather\n",
    "than only one, categorical features, each with a different granularity.\n",
    "Figure 5 gives an illustration of discretizing a numerical feature\n",
    "with four levels of granularity. Since more levels of granularity are\n",
    "considered, it is more likely to get a promising result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar p = 5 primeiramente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "import operator\n",
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn import metrics\n",
    "from scipy.optimize import minimize \n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from fangorn.files_prep import get_data, data_to_pandas\n",
    "from fangorn.preprocessing import splitting, feature_selection\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "from category_encoders import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All ML_CHALLENGE files ready!\n"
     ]
    }
   ],
   "source": [
    "# read dataset\n",
    "all_datasets = get_data.get_all_data(only='ml_challenge')\n",
    "this_dataset = 'madeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configure dataset\n",
    "X_all, y_all = data_to_pandas.read_prepare_data(this_dataset)\n",
    "new_X_all = X_all.copy()\n",
    "for col in X_all.columns:\n",
    "    new_X_all[col] = pd.cut(X_all[col], bins=10, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "\n",
    "X_all = new_X_all.copy()\n",
    "# X_all = feature_selection.extra_trees_feature_selection(X_all, y_all)\n",
    "dataset_split_dict = splitting.simple_train_test_val_split(X_all, y_all)\n",
    "\n",
    "X_train = dataset_split_dict['train']['X']\n",
    "y_train = dataset_split_dict['train']['y']\n",
    "X_test = dataset_split_dict['test']['X']\n",
    "y_test = dataset_split_dict['test']['y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remaining problem is how to determine the\n",
    "levels of granularity. For an experienced user, it can set a group of\n",
    "potentially good values. If no values are specified, AutoCross will\n",
    "use {10^p\n",
    "}\n",
    "P\n",
    "p=1\n",
    "as default values, where P is an integer determined\n",
    "by a rule-based mechanism that considers the available memory,\n",
    "data size and feature numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_numeric_features(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    for feat in numeric_features:\n",
    "        print(f\" Working in {feat}\")\n",
    "        for gran in range(2, max_gran+1):\n",
    "            try:\n",
    "                # calcula retibins no treino e aplica no teste\n",
    "                X_train[f\"{feat}_{gran}\"], this_bins = pd.qcut(X_train[feat],\n",
    "                                               gran,\n",
    "                                               labels= [f\"bin_{i}\" for i in range(gran)],\n",
    "                                               retbins = True\n",
    "                                              )\n",
    "                X_train[f\"{feat}_{gran}\"] = X_train[f\"{feat}_{gran}\"].cat.codes\n",
    "                X_train[f\"{feat}_{gran}\"] = X_train[f\"{feat}_{gran}\"].astype(int)\n",
    "                # aumenta range dos bins para garantir abrangencia\n",
    "                this_bins = np.concatenate(([-np.inf], this_bins[1:-1], [np.inf]))\n",
    "                \n",
    "                # aplicando no teste\n",
    "                X_test[f\"{feat}_{gran}\"] = pd.cut(X_test[feat], bins=this_bins, labels=[f\"bin_{i}\" for i in range(gran)])\n",
    "                \n",
    "                discrete_features.append(f\"{feat}_{gran}\")\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test, discrete_features\n",
    "\n",
    "def get_dummies(X_train, X_test):\n",
    "    ohe = OneHotEncoder(cols=X_train.columns).fit(X=X_train)\n",
    "    X_train = ohe.transform(X_train)\n",
    "    X_test = ohe.transform(X_test)\n",
    "    return X_train, X_test\n",
    "\n",
    "def run_field_wise_minibatch_gradient_descent_lr(this_feature,\n",
    "                                                 X_all,\n",
    "                                                 y_all,\n",
    "                                                 y_feature,\n",
    "                                                 all_features=False,\n",
    "                                                 return_clf = False):\n",
    "    \"\"\"\n",
    "    Run field wise logistic regression\n",
    "    \"\"\"\n",
    "        \n",
    "    clf = linear_model.SGDClassifier(loss='log',\n",
    "                                     n_jobs= -1,\n",
    "                                     warm_start = True)\n",
    "    if all_features:\n",
    "        this_X = X_all\n",
    "        \n",
    "    else:\n",
    "        this_X = pd.get_dummies(X_all[this_feature], columns=this_feature)\n",
    "        \n",
    "#     for i in range(0, 1000):\n",
    "#         this_batch_samples = this_X.sample(frac=0.8)\n",
    "#         this_y = y_all.loc[y_all.index.isin(this_batch_samples.index)]\n",
    "#         clf.partial_fit(this_batch_samples, this_y[y_feature], classes=this_y[y_feature].unique())\n",
    "    \n",
    "    clf = LogisticRegression(random_state=0, max_iter=10000).fit(this_X, y_all[y_feature])\n",
    "\n",
    "    if return_clf:\n",
    "        all_preds = clf.predict_proba(this_X)\n",
    "        y_all['preds'] = all_preds\n",
    "        final_score = metrics.log_loss(y_all[y_feature], y_all['preds'])\n",
    "        return clf, final_score, y_all\n",
    "   \n",
    "    return clf.score(this_X, y_all)\n",
    "\n",
    "\n",
    "def run_initial_logit(X_train, y_train, X_test, y_test):\n",
    "    clf = LogisticRegression().fit(X_train, y_train[y_train.columns[0]])\n",
    "    all_preds = clf.predict_proba(X_test)[:, 1]\n",
    "    logloss = metrics.log_loss(y_test[y_test.columns[0]], all_preds)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[y_test.columns[0]], all_preds, pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return clf, logloss, auc\n",
    "    \n",
    "\n",
    "def measure_and_clean_discrete_features(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    In order to avoid the dramatic increase in feature number caused\n",
    "    by discretization, once these features are generated, we use fieldwise LR to evaluate them and keep\n",
    "    only the best half. \n",
    "    \"\"\"\n",
    "    \n",
    "    def _select_discrete_features(feature_score, abs=True):\n",
    "        \"\"\"\n",
    "        Select only the best half of measured features\n",
    "        \"\"\"\n",
    "        if abs:\n",
    "            feature_score = {k: np.abs(v) for k,v in feature_score.items()}\n",
    "            \n",
    "        sorted_score = sorted(feature_score.items(), key=operator.itemgetter(1), reverse=True)\n",
    "        half_features = int(np.floor(len(sorted_score)*0.2))\n",
    "        return [k[0] for k in sorted_score[:half_features]]\n",
    "    \n",
    "    feature_score = {}\n",
    "    # while what you describe is properly called minibatch learning.\n",
    "    # That's implemented in sklearn.linear_model.SGDClassifier,\n",
    "    # which fits a logistic regression model if you give it the option loss=\"log\".\n",
    "    this_classifier, logloss, auc = run_initial_logit(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    coef_dict = dict(list(zip(X_train.columns, this_classifier.coef_[0])))\n",
    "    print(coef_dict, auc)\n",
    "#     selected_features = _select_discrete_features(coef_dict)\n",
    "    \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 259 features...\n",
      " Working in 0\n",
      " Working in 1\n",
      " Working in 2\n",
      " Working in 3\n",
      " Working in 4\n",
      " Working in 5\n",
      " Working in 6\n",
      " Working in 7\n",
      " Working in 8\n",
      " Working in 9\n",
      " Working in 10\n",
      " Working in 11\n",
      " Working in 12\n",
      " Working in 13\n",
      " Working in 14\n",
      " Working in 15\n",
      " Working in 16\n",
      " Working in 17\n",
      " Working in 18\n",
      " Working in 19\n",
      " Working in 20\n",
      " Working in 21\n",
      "Not possible to correct work on cut 21 > 4\n",
      " Working in 22\n",
      " Working in 23\n",
      " Working in 24\n",
      " Working in 25\n",
      " Working in 26\n",
      " Working in 27\n",
      " Working in 28\n",
      " Working in 29\n",
      " Working in 30\n",
      " Working in 31\n",
      " Working in 32\n",
      " Working in 33\n",
      " Working in 34\n",
      " Working in 35\n",
      " Working in 36\n",
      " Working in 37\n",
      " Working in 38\n",
      " Working in 39\n",
      " Working in 40\n",
      " Working in 41\n",
      " Working in 42\n",
      " Working in 43\n",
      " Working in 44\n",
      " Working in 45\n",
      " Working in 46\n",
      " Working in 47\n",
      " Working in 48\n",
      " Working in 49\n",
      " Working in 50\n",
      " Working in 51\n",
      " Working in 52\n",
      " Working in 53\n",
      " Working in 54\n",
      " Working in 55\n",
      " Working in 56\n",
      " Working in 57\n",
      " Working in 58\n",
      " Working in 59\n",
      " Working in 60\n",
      " Working in 61\n",
      " Working in 62\n",
      " Working in 63\n",
      " Working in 64\n",
      " Working in 65\n",
      " Working in 66\n",
      " Working in 67\n",
      " Working in 68\n",
      " Working in 69\n",
      " Working in 70\n",
      " Working in 71\n",
      " Working in 72\n",
      " Working in 73\n",
      " Working in 74\n",
      " Working in 75\n",
      " Working in 76\n",
      " Working in 77\n",
      " Working in 78\n",
      " Working in 79\n",
      " Working in 80\n",
      " Working in 81\n",
      " Working in 82\n",
      " Working in 83\n",
      " Working in 84\n",
      " Working in 85\n",
      " Working in 86\n",
      " Working in 87\n",
      " Working in 88\n",
      " Working in 89\n",
      " Working in 90\n",
      " Working in 91\n",
      " Working in 92\n",
      " Working in 93\n",
      " Working in 94\n",
      " Working in 95\n",
      " Working in 96\n",
      " Working in 97\n",
      " Working in 98\n",
      " Working in 99\n",
      " Working in 100\n",
      " Working in 101\n",
      " Working in 102\n",
      " Working in 103\n",
      " Working in 104\n",
      " Working in 105\n",
      " Working in 106\n",
      " Working in 107\n",
      " Working in 108\n",
      " Working in 109\n",
      " Working in 110\n",
      " Working in 111\n",
      " Working in 112\n",
      " Working in 113\n",
      " Working in 114\n",
      " Working in 115\n",
      " Working in 116\n",
      " Working in 117\n",
      " Working in 118\n",
      " Working in 119\n",
      " Working in 120\n",
      " Working in 121\n",
      " Working in 122\n",
      " Working in 123\n",
      " Working in 124\n",
      " Working in 125\n",
      " Working in 126\n",
      " Working in 127\n",
      " Working in 128\n",
      " Working in 129\n",
      " Working in 130\n",
      "Not possible to correct work on cut 130 > 5\n",
      " Working in 131\n",
      " Working in 132\n",
      " Working in 133\n",
      " Working in 134\n",
      " Working in 135\n",
      " Working in 136\n",
      " Working in 137\n",
      " Working in 138\n",
      "Not possible to correct work on cut 138 > 5\n",
      " Working in 139\n",
      " Working in 140\n",
      " Working in 141\n",
      " Working in 142\n",
      " Working in 143\n",
      " Working in 144\n",
      " Working in 145\n",
      " Working in 146\n",
      " Working in 147\n",
      " Working in 148\n",
      " Working in 149\n",
      " Working in 150\n",
      " Working in 151\n",
      " Working in 152\n",
      " Working in 153\n",
      " Working in 154\n",
      " Working in 155\n",
      "Not possible to correct work on cut 155 > 5\n",
      " Working in 156\n",
      " Working in 157\n",
      " Working in 158\n",
      " Working in 159\n",
      " Working in 160\n",
      " Working in 161\n",
      " Working in 162\n",
      " Working in 163\n",
      " Working in 164\n",
      " Working in 165\n",
      " Working in 166\n",
      " Working in 167\n",
      " Working in 168\n",
      " Working in 169\n",
      " Working in 170\n",
      " Working in 171\n",
      " Working in 172\n",
      " Working in 173\n",
      " Working in 174\n",
      " Working in 175\n",
      " Working in 176\n",
      " Working in 177\n",
      " Working in 178\n",
      " Working in 179\n",
      " Working in 180\n",
      " Working in 181\n",
      "Not possible to correct work on cut 181 > 5\n",
      " Working in 182\n",
      " Working in 183\n",
      " Working in 184\n",
      " Working in 185\n",
      " Working in 186\n",
      " Working in 187\n",
      " Working in 188\n",
      " Working in 189\n",
      " Working in 190\n",
      " Working in 191\n",
      " Working in 192\n",
      " Working in 193\n",
      " Working in 194\n",
      " Working in 195\n",
      " Working in 196\n",
      " Working in 197\n",
      " Working in 198\n",
      " Working in 199\n",
      " Working in 200\n",
      " Working in 201\n",
      " Working in 202\n",
      " Working in 203\n",
      " Working in 204\n",
      " Working in 205\n",
      " Working in 206\n",
      " Working in 207\n",
      " Working in 208\n",
      " Working in 209\n",
      " Working in 210\n",
      " Working in 211\n",
      " Working in 212\n",
      " Working in 213\n",
      " Working in 214\n",
      " Working in 215\n",
      " Working in 216\n",
      " Working in 217\n",
      " Working in 218\n",
      " Working in 219\n",
      " Working in 220\n",
      "Not possible to correct work on cut 220 > 5\n",
      " Working in 221\n",
      " Working in 222\n",
      " Working in 223\n",
      " Working in 224\n",
      " Working in 225\n",
      " Working in 226\n",
      " Working in 227\n",
      " Working in 228\n",
      " Working in 229\n",
      " Working in 230\n",
      " Working in 231\n",
      " Working in 232\n",
      " Working in 233\n",
      " Working in 234\n",
      " Working in 235\n",
      " Working in 236\n",
      " Working in 237\n",
      " Working in 238\n",
      " Working in 239\n",
      " Working in 240\n",
      " Working in 241\n",
      " Working in 242\n",
      " Working in 243\n",
      " Working in 244\n",
      " Working in 245\n",
      " Working in 246\n",
      " Working in 247\n",
      " Working in 248\n",
      " Working in 249\n",
      " Working in 250\n",
      " Working in 251\n",
      " Working in 252\n",
      " Working in 253\n",
      " Working in 254\n",
      " Working in 255\n",
      " Working in 256\n",
      " Working in 257\n",
      " Working in 258\n"
     ]
    }
   ],
   "source": [
    "X_train_discretized, X_test_discretized, discrete_features = discretize_numeric_features(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1.0020876033345303\n",
      "2\n",
      "0.5044357151149288\n",
      "3\n",
      "0.3268466944321082\n",
      "4\n",
      "0.2379796925699773\n",
      "5\n",
      "0.18562080534721795\n",
      "6\n",
      "0.14953347665871117\n",
      "7\n",
      "0.12743935271008086\n",
      "8\n",
      "0.11326428565679861\n",
      "9\n",
      "0.09700921463608089\n",
      "10\n",
      "0.08703989783382106\n",
      "11\n",
      "0.0695966146160475\n",
      "12\n",
      "0.06815196148635208\n",
      "13\n",
      "0.061634644594839506\n",
      "14\n",
      "0.059187102855009464\n",
      "15\n",
      "0.05464931310521193\n",
      "16\n",
      "0.04657591944818497\n",
      "17\n",
      "0.04234338418536994\n",
      "18\n",
      "0.044149785200028165\n",
      "19\n",
      "0.03461888111723691\n",
      "20\n",
      "0.04014900846079075\n",
      "21\n",
      "0.03454116847279897\n",
      "22\n",
      "0.029788053122393434\n"
     ]
    }
   ],
   "source": [
    "from pyitlib import discrete_random_variable as drv\n",
    "def hjmi_selector(X, y, n_iter):\n",
    "    max_features = n_iter\n",
    "    y[y.columns[0]] = y[y.columns[0]].astype(int)\n",
    "    selected_features = []\n",
    "    collected_hjmi = []\n",
    "    j_h = 0\n",
    "    all_jmi = {}\n",
    "    feat_mi_selected = {}\n",
    "    for i in range(n_iter):\n",
    "        print(i)\n",
    "        count = 1\n",
    "        for feat in X.columns:\n",
    "            X[feat] = X[feat].astype(int)\n",
    "            if feat in selected_features:\n",
    "                if feat in all_jmi:\n",
    "                    all_jmi.pop(feat)\n",
    "                continue\n",
    "                \n",
    "            mi = drv.information_mutual(X[feat].values, y[y.columns[0]].values)\n",
    "            jmi_2 = 0\n",
    "            for selected_feature in selected_features:\n",
    "                \n",
    "                if f'{feat}_&_{selected_feature}' in feat_mi_selected:\n",
    "                    jmi_2 = feat_mi_selected[f'{feat}_&_{selected_feature}']\n",
    "                else:\n",
    "                    tmp_mi = drv.information_mutual(X[feat].values, X[selected_feature].values)\n",
    "                    tmp_cmi = drv.information_mutual_conditional(X[feat].values, X[selected_feature].values, y[y.columns[0]].values)\n",
    "                    jmi_2 = jmi_2 + tmp_mi - tmp_cmi\n",
    "                    feat_mi_selected[f'{feat}_&_{selected_feature}'] = jmi_2\n",
    "\n",
    "            try:\n",
    "                all_jmi[feat] = j_h + mi - jmi_2 / count - len(selected_feature)\n",
    "            except:\n",
    "                all_jmi[feat] = j_h + mi - jmi_2 / count - 1\n",
    "            count += 1\n",
    "            \n",
    "        j_h_max_feat = max(all_jmi.items(), key=operator.itemgetter(1))[0]\n",
    "        j_h_max_value = max(all_jmi.items(), key=operator.itemgetter(1))[1]\n",
    "        j_h = j_h_max_value\n",
    "        \n",
    "        if not selected_features:\n",
    "            hjmi = j_h_max_value\n",
    "            selected_features.append(j_h_max_feat)\n",
    "            collected_hjmi.append(hjmi)\n",
    "        \n",
    "        else:\n",
    "            print(((j_h_max_value-hjmi)/hjmi))\n",
    "            if (((j_h_max_value-hjmi)/hjmi) > 0.03) and len(selected_features) < max_features:\n",
    "                hjmi = j_h_max_value\n",
    "                selected_features.append(j_h_max_feat)\n",
    "                collected_hjmi.append(hjmi)\n",
    "            else:\n",
    "                return selected_features, collected_hjmi  \n",
    "\n",
    "t = hjmi_selector(X_train.copy(), y_train.copy(), n_iter=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_keep = t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_discretized = X_train[features_to_keep]\n",
    "X_test_discretized = X_test[features_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_dummy, X_test_dummy = get_dummies(X_train_discretized.copy(), X_test_discretized.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'191_1': -0.5215963026061161, '191_2': 0.055769789495850104, '191_3': -0.16952481873889605, '191_4': -0.4659260524121722, '191_5': -0.3002992474980817, '191_6': 0.3641007260452931, '191_7': 0.28862965900992105, '191_8': 0.5260745736918807, '191_9': 0.6715897321724065, '191_10': -0.43568371513139853, '237_1': 0.2737812978897422, '237_2': 0.2592135557927553, '237_3': -0.676332767687875, '237_4': -0.2595636537496368, '237_5': -0.03670278999619111, '237_6': 0.1975930698781436, '237_7': 0.26573528925816603, '237_8': 0.3951275193318306, '237_9': -0.35834707932310284, '237_10': -0.04737009736513244, '0_1': 0.46437269580441015, '0_2': 0.30519150238584825, '0_3': -0.22437602476417537, '0_4': -0.08989178770107276, '0_5': -0.2956756533950654, '0_6': -0.12388030068803356, '0_7': -0.1442572155272359, '0_8': -0.006048269245042318, '0_9': 0.11946019536035628, '0_10': 0.008239201798535574, '1_1': -0.09881835660518387, '1_2': -0.2856241658311096, '1_3': 0.061226488933176185, '1_4': 0.028068641046460247, '1_5': 0.04437397055874975, '1_6': -0.008787767741191674, '1_7': 0.009482591170466756, '1_8': 0.21303825558512016, '1_9': -0.09872746641687243, '1_10': 0.14890215332891468, '2_1': -0.2909308514201186, '2_2': 0.8156059918074445, '2_3': -0.23399628565386227, '2_4': 0.08253975859922691, '2_5': 0.21319739485129705, '2_6': -0.10083212852989411, '2_7': 0.04782091758616559, '2_8': -0.30678464233362535, '2_9': -0.18288230580429962, '2_10': -0.030603505073742782, '3_1': -0.16050009361599865, '3_2': -0.33761013056142264, '3_3': -0.2338256670385234, '3_4': -0.0781679042899164, '3_5': -0.17553829954967845, '3_6': -0.21491559225205162, '3_7': -0.007565470097325522, '3_8': 0.07654448890250884, '3_9': 0.39598012448265335, '3_10': 0.7487328880483465, '4_1': 0.2817664655419815, '4_2': 0.15938272847564708, '4_3': -0.21728784462542258, '4_4': -0.06714288924658408, '4_5': 0.07404497264176976, '4_6': -0.049335307637455084, '4_7': 0.17671221491261907, '4_8': 0.005978935387378265, '4_9': 0.29195445716998236, '4_10': -0.6429393885912245, '5_1': -0.2474703032525825, '5_2': -0.20886449799273446, '5_3': 0.3192537944310222, '5_4': 0.10228966242651928, '5_5': 0.17004364293097626, '5_6': 0.24754349214695415, '5_7': 0.1250365040527028, '5_8': 0.019690591330553876, '5_9': -0.07961194087365789, '5_10': -0.43477660117119155, '6_1': 0.3049658715862463, '6_2': -0.2105894512962746, '6_3': -0.1007574370026764, '6_4': -0.004478971929454135, '6_5': -0.10335889217043197, '6_6': -0.02757027504468158, '6_7': -0.1688603373094343, '6_8': -0.38177270059189283, '6_9': 0.055523985769003616, '6_10': 0.6500325520181748, '7_1': -0.11736678598520225, '7_2': -0.018073324279371073, '7_3': -0.36154250176739644, '7_4': 0.30765086275561465, '7_5': 0.02650254051810842, '7_6': -0.0991515582187502, '7_7': 0.01162137713651754, '7_8': -0.08134198442283096, '7_9': 0.12199181009702796, '7_10': 0.22284390819502842, '8_1': -1.0793151591211905, '8_2': 0.22517679030381774, '8_3': 0.15143832026606027, '8_4': 0.0939747444093885, '8_5': 0.16238822271841238, '8_6': 0.05721392860501388, '8_7': 0.22197011249147647, '8_8': 0.1422209628537414, '8_9': -0.18645448790259778, '8_10': 0.22452090940443206, '9_1': 0.16176241126660548, '9_2': 0.14720437348296914, '9_3': 0.09799412299924595, '9_4': -0.03363178938336795, '9_5': -0.1681066014749379, '9_6': -0.016233472283584627, '9_7': 0.02795547533859909, '9_8': 0.2942953152999534, '9_9': 0.40714370215780293, '9_10': -0.9052491933746424, '10_1': -0.13210758919346566, '10_2': -0.17571979415016872, '10_3': -0.11159524936400073, '10_4': 0.2501893579941566, '10_5': -0.06143058059926989, '10_6': 0.08211632529923991, '10_7': -0.11590728959332045, '10_8': -0.16073936494772756, '10_9': 0.2778920907330302, '10_10': 0.1604364378501953, '11_1': 0.1998102924683152, '11_2': 0.03356100539121012, '11_3': 0.21646451286582838, '11_4': -0.2036155876029781, '11_5': 0.005719769400445784, '11_6': -0.08960761701633646, '11_7': -0.14507385231747505, '11_8': -0.09659553013112576, '11_9': -0.04399459675202645, '11_10': 0.13646594772289872, '12_1': -0.34286436572451606, '12_2': 0.31828023016819984, '12_3': -0.15139739899617505, '12_4': 0.19720704518409476, '12_5': -0.024987988353086632, '12_6': 0.06392338045750755, '12_7': 0.0007092021084359492, '12_8': 0.44651094330243774, '12_9': 0.25698469330051243, '12_10': -0.7512313974186777, '13_1': 0.6148180979565522, '13_2': 0.26222091642371714, '13_3': 0.003937955791386772, '13_4': -0.032003410163745916, '13_5': -0.20383078791381282, '13_6': -0.1617249715054583, '13_7': -0.3537646077668248, '13_8': 0.029831808120007058, '13_9': -0.10556910101966095, '13_10': -0.04078155589365347, '14_1': 0.0060301302926256215, '14_2': 0.1632481312290357, '14_3': -0.0995099438835558, '14_4': -0.0031329225543384335, '14_5': -0.2119116019687217, '14_6': -0.12979830691822203, '14_7': -0.006829169743673257, '14_8': -0.03387058287875085, '14_9': 0.22258666195837315, '14_10': 0.10632194849579928, '15_1': -0.36716690846892225, '15_2': 0.06926501168126728, '15_3': 0.2529908195630445, '15_4': 0.22227067234419443, '15_5': 0.2729801035993326, '15_6': 0.21179124514500436, '15_7': 0.3506553392956498, '15_8': 0.39304558380424764, '15_9': -0.04020736267259488, '15_10': -1.3524901602626132, '16_1': 0.508098404022047, '16_2': 0.3620564096309221, '16_3': -0.7482095705295785, '16_4': -0.07033158258741612, '16_5': 0.11739167008274985, '16_6': -0.13827931656203604, '16_7': -0.13766338440899556, '16_8': -0.26166326259810185, '16_9': 0.1591012654692048, '16_10': 0.22263371150973713, '17_1': -0.08198420779325513, '17_2': 0.2700672476606216, '17_3': -0.1628709919724224, '17_4': 0.18737050892463772, '17_5': -0.04827101098151045, '17_6': -0.028636378477809327, '17_7': -0.029858650304539137, '17_8': -0.09910447659634, '17_9': -0.25835213624500436, '17_10': 0.2647744398142664, '18_1': -0.24430576384061464, '18_2': 0.018262800928306268, '18_3': 0.2365588138125914, '18_4': 0.05872511237090595, '18_5': 0.29906454984134057, '18_6': 0.1276222756922387, '18_7': 0.10170841876463635, '18_8': -0.16612296430254694, '18_9': -0.4183788992383931, '19_1': 0.6447918940225431, '19_2': -0.23907565689783794, '19_3': 0.09132807798889465, '19_4': -0.16843021176935835, '19_5': -0.20346493466375032, '19_6': -0.21667689259056044, '19_7': -0.08648868517526352, '19_8': -0.29440491761543375, '19_9': 0.48555567072932915} 0.6006808943089431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/master/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "final_X_train, final_X_test = measure_and_clean_discrete_features(X_train_dummy.copy(), y_train.copy(), X_test_dummy.copy(), y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOGLOSS: 0.7035338290326201 - AUC: 0.6006808943089431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/master/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "start_classifier, logloss, auc = run_initial_logit(final_X_train.copy(), y_train.copy(), final_X_test.copy(), y_test.copy())\n",
    "print(f\"LOGLOSS: {logloss} - AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calc_logloss(y_true, preds):\n",
    "#     return metrics.log_loss(y_true[y_true.columns[0]], preds['preds'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true[y_true.columns[0]],  preds['preds'], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return -1 * auc\n",
    "\n",
    "def _obj(x, \n",
    "         X_train,\n",
    "         y_train,\n",
    "         combined_features,\n",
    "         bsum):\n",
    "\n",
    "    # add x to bsum\n",
    "    this_w = np.array(x).reshape(-1,1)\n",
    "    this_value = np.array(combined_features).reshape(1,-1)\n",
    "    bsum_with_new_feature = bsum + np.matmul(this_w.T, this_value)\n",
    "    this_preds = 1/(1 + np.exp(-bsum_with_new_feature)) \n",
    "    preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])\n",
    "    logloss = _calc_logloss(y_train, preds)\n",
    "    return logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_training_set = final_X_train.copy()\n",
    "current_test_set = final_X_test.copy()\n",
    "start_classifier, start_logloss, start_auc = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "\n",
    "coef_dict = dict(list(zip(current_training_set.columns,start_classifier.coef_[0])))\n",
    "intercept = start_classifier.intercept_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_pair = ('month_8','poutcome_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "bsum = np.add(np.matmul(col_coefs, current_training_set.values.T), intercept)\n",
    "\n",
    "combined_features = current_training_set[feature_pair[0]] | current_training_set[feature_pair[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " final_simplex: (array([[0.000125 ],\n",
       "       [0.0001875]]), array([-0.90176082, -0.90176079]))\n",
       "           fun: -0.9017608175039736\n",
       "       message: 'Optimization terminated successfully.'\n",
       "          nfev: 8\n",
       "           nit: 4\n",
       "        status: 0\n",
       "       success: True\n",
       "             x: array([0.000125])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize(_obj, 0, args=(current_training_set,\n",
    "                             y_train,\n",
    "                             combined_features,\n",
    "                             bsum), method='Nelder-Mead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_one_pair(X_train, y_train, X_test, y_test, feature_pair, coef_dict, intercept):\n",
    "    \n",
    "    def _calc_logloss(y_true, preds):\n",
    "        return metrics.log_loss(y_true[y_true.columns[0]], preds['preds'])\n",
    "\n",
    "    def _obj(x, \n",
    "             X_train,\n",
    "             y_train,\n",
    "             combined_features,\n",
    "             bsum):\n",
    "        \n",
    "        # add x to bsum\n",
    "        this_w = np.array(x).reshape(-1,1)\n",
    "        this_value = np.array(combined_features).reshape(1,-1)\n",
    "        bsum_with_new_feature = bsum + np.matmul(this_w.T, this_value)\n",
    "        this_preds = 1/(1 + np.exp(-bsum_with_new_feature)) \n",
    "        preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])\n",
    "        return _calc_logloss(y_train, preds)\n",
    "\n",
    "    col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "    bsum = np.add(np.matmul(col_coefs, X_train.values.T), intercept)\n",
    "    \n",
    "    combined_features = X_train[feature_pair[0]] | X_train[feature_pair[1]]\n",
    "        \n",
    "    if combined_features.equals(X_train[feature_pair[0]]) or combined_features.equals(X_train[feature_pair[1]]):\n",
    "        return  {\"coef\": 0, \"logloss\" : 99}\n",
    "\n",
    "    result = minimize(_obj, 1, args=(X_train,\n",
    "                                 y_train,\n",
    "                                 combined_features,\n",
    "                                 bsum))\n",
    "    \n",
    "    this_coef = result['x'][0]\n",
    "    this_logloss = result['fun']\n",
    "\n",
    "#     start_classifier, start_logloss = run_initial_logit(X_train, y_train, X_test, y_test)\n",
    "#     coef_dict = dict(list(zip(X_train.columns,start_classifier.coef_[0])))\n",
    "#     intercept = start_classifier.intercept_[0]\n",
    "#     this_coef = 0\n",
    "    dict_result_combination = {\"coef\":this_coef,\n",
    "                               \"logloss\" : this_logloss}\n",
    "    \n",
    "    return dict_result_combination\n",
    "\n",
    "def iter_one_level(X_train, y_train, X_test, y_test, coef_dict, intercept):\n",
    "    \n",
    "    all_columns = list(X_train)\n",
    "    pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "    all_results = {}\n",
    "    with tqdm(total=len(pairwise_cols)) as pbar:\n",
    "        for feature_pair in pairwise_cols:\n",
    "            feature_name = str(feature_pair)\n",
    "            if feature_name not in X_train.columns:\n",
    "                all_results[feature_name] = score_one_pair(X_train, y_train, X_test, y_test, feature_pair, coef_dict, intercept)\n",
    "                pbar.update(1)\n",
    "    return all_results\n",
    "\n",
    "def predict_logit(coef_dict, intercept, X_test, y_test):\n",
    "    col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "    bsum = np.add(np.matmul(col_coefs, X_test.values.T), intercept)\n",
    "    this_preds = 1/(1 + np.exp(-bsum)) \n",
    "    preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])\n",
    "    logloss = metrics.log_loss(y_test[y_test.columns[0]], preds['preds'])\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test[y_test.columns[0]],  preds['preds'], pos_label=1)\n",
    "    auc = metrics.auc(fpr, tpr)\n",
    "    return logloss, auc\n",
    "\n",
    "def beam_search(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def _choose_best_feature(dict_level_results):\n",
    "        min_logloss = 9999\n",
    "        bsf_feature = None\n",
    "        bst_coef = None\n",
    "        for key, val in dict_level_results.items():\n",
    "            if val['logloss'] < min_logloss:\n",
    "                min_logloss = val['logloss']\n",
    "                bsf_feature = key\n",
    "                bst_coef = val['coef']\n",
    "        print(f\"Level - choose {bsf_feature} -{bst_coef} -{min_logloss}\")\n",
    "        return bsf_feature, bst_coef, min_logloss\n",
    "    \n",
    "    current_training_set = X_train.copy()\n",
    "    current_test_set = X_test.copy()\n",
    "    start_classifier, start_logloss, start_auc = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "    \n",
    "    coef_dict = dict(list(zip(current_training_set.columns,start_classifier.coef_[0])))\n",
    "    intercept = start_classifier.intercept_[0]\n",
    "    \n",
    "    print(f\"Start logloss : {start_logloss} - Start AUC {start_auc}\")\n",
    "    last_logloss = start_logloss\n",
    "    this_logloss = -np.inf\n",
    "    accepted_features = []\n",
    "    while this_logloss <= last_logloss:\n",
    "        # eval one level\n",
    "        dict_level_results = iter_one_level(current_training_set, y_train, current_test_set, y_test, coef_dict, intercept)\n",
    "        bst_feature, this_coef, this_logloss = _choose_best_feature(dict_level_results)\n",
    "        \n",
    "        # update X_train an X_test\n",
    "        current_training_set[str(bst_feature)] = current_training_set[str(eval(bst_feature)[0])] | current_training_set[str(eval(bst_feature)[1])]\n",
    "        current_test_set[str(bst_feature)] = current_test_set[str(eval(bst_feature)[0])] | current_test_set[str(eval(bst_feature)[1])]\n",
    "        \n",
    "        # retrain logit with new feature\n",
    "#         this_clf, this_logloss, this_auc = run_initial_logit(current_training_set, y_train, current_test_set, y_test)\n",
    "#         coef_dict = dict(list(zip(current_training_set.columns,this_clf.coef_[0])))\n",
    "#         intercept = start_classifier.intercept_[0]\n",
    "        coef_dict[bst_feature] = this_coef\n",
    "        \n",
    "        this_logloss, this_auc = predict_logit(coef_dict, intercept, current_test_set, y_test)\n",
    "        print(f\" new  logloss: {this_logloss} - new auc {this_auc}\")\n",
    "        \n",
    "        coef_dict[bst_feature] = this_logloss\n",
    "    \n",
    "        current_logloss_diff = last_logloss - this_logloss\n",
    "        \n",
    "        if this_logloss > last_logloss:\n",
    "            current_training_set =  current_training_set.drop([str(bst_feature)], axis=1)\n",
    "            current_test_set =  current_test_set.drop([str(bst_feature)], axis=1)\n",
    "            coef_dict.pop(str(bst_feature), None)\n",
    "            \n",
    "        else:\n",
    "            last_logloss = this_logloss\n",
    "            \n",
    "        print(f\"logloss gain with {bst_feature}: {current_logloss_diff}\")\n",
    "    return current_training_set, current_test_set, coef_dict, intercept   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/favoretti/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "  0%|          | 1/1326 [00:00<04:18,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start logloss : 0.2327655985368599 - Start AUC 0.9061106658764809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1326/1326 [03:42<00:00,  5.96it/s]\n",
      "  0%|          | 0/1378 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('contact_3', 'month_3') -0.16735013114575673 -0.2278955972596195\n",
      " new  logloss: 0.23231604045654392 - new auc 0.9066090048552927\n",
      "logloss gain with ('contact_3', 'month_3'): 0.00044955808031599265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1377/1378 [03:57<00:00,  5.80it/s]\n",
      "  0%|          | 0/1431 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('contact_3', 'duration_9_9') --0.1780470059229591 -0.22749771691697013\n",
      " new  logloss: 0.23184214232056638 - new auc 0.906786992382682\n",
      "logloss gain with ('contact_3', 'duration_9_9'): 0.00047389813597753516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1429/1431 [03:59<00:00,  5.97it/s]\n",
      "  0%|          | 0/1485 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('month_6', \"('contact_3', 'duration_9_9')\") --0.38356909435351466 -0.22738999798148193\n",
      " new  logloss: 0.23160808367028515 - new auc 0.9071357905210332\n",
      "logloss gain with ('month_6', \"('contact_3', 'duration_9_9')\"): 0.00023405865028122697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1482/1485 [04:08<00:00,  5.97it/s]\n",
      "  0%|          | 0/1540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('duration_9_1', '(\\'month_6\\', \"(\\'contact_3\\', \\'duration_9_9\\')\")') --0.6138723049671846 -0.227392864340058\n",
      " new  logloss: 0.2315266823035643 - new auc 0.9071753288060764\n",
      "logloss gain with ('duration_9_1', '(\\'month_6\\', \"(\\'contact_3\\', \\'duration_9_9\\')\")'): 8.140136672085685e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1536/1540 [04:13<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level - choose ('duration_10_1', '(\\'month_6\\', \"(\\'contact_3\\', \\'duration_9_9\\')\")') --0.8449532393229401 -0.22740935562055742\n",
      " new  logloss: 0.23154991996186344 - new auc 0.907156364348124\n",
      "logloss gain with ('duration_10_1', '(\\'month_6\\', \"(\\'contact_3\\', \\'duration_9_9\\')\")'): -2.3237658299146702e-05\n"
     ]
    }
   ],
   "source": [
    "complete_X_train, complete_X_test, coef_dict, intercept = beam_search(final_X_train, y_train, final_X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_coefs = np.array(list(coef_dict.values())).reshape(1,-1)\n",
    "bsum = np.add(np.matmul(col_coefs, complete_X_test.values.T), intercept)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_preds = 1/(1 + np.exp(-bsum)) \n",
    "preds = pd.DataFrame(this_preds.reshape(-1,1), columns=['preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds =  preds['preds']\n",
    "logloss = metrics.log_loss(y_test[y_test.columns[0]], all_preds)\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test[y_test.columns[0]], all_preds, pos_label=1)\n",
    "auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['NOX_10_bin_8', 'NOX_5_bin_4', 'RM_8_bin_2', 'AGE_8_bin_0',\n",
       "       'CRIM_8_bin_7', 'CRIM_8_bin_5', 'NOX_10_bin_5', 'AGE_10_bin_0',\n",
       "       'NOX_5_bin_2', 'NOX_9_bin_5',\n",
       "       ...\n",
       "       'CRIM_10_bin_1', '('NOX_10_bin_8', 'NOX_5_bin_4')',\n",
       "       '('NOX_10_bin_8', 'RM_8_bin_2')', '('NOX_10_bin_8', 'AGE_8_bin_0')',\n",
       "       '('NOX_10_bin_8', 'CRIM_8_bin_7')', '('NOX_10_bin_8', 'CRIM_8_bin_5')',\n",
       "       '('NOX_10_bin_8', 'NOX_10_bin_5')', '('NOX_10_bin_8', 'AGE_10_bin_0')',\n",
       "       '('NOX_10_bin_8', 'NOX_5_bin_2')', '('NOX_10_bin_8', 'NOX_9_bin_5')'],\n",
       "      dtype='object', length=168)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.columnsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_columns = t.columns\n",
    "pairwise_cols = list(itertools.combinations(all_columns, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(random_state=0),\n",
       " 1.228669507272974,\n",
       "      bin_lstat  preds\n",
       " 0            0      0\n",
       " 1            0      0\n",
       " 2            0      0\n",
       " 3            0      0\n",
       " 4            0      0\n",
       " ..         ...    ...\n",
       " 501          0      0\n",
       " 502          0      0\n",
       " 503          0      0\n",
       " 504          0      0\n",
       " 505          0      1\n",
       " \n",
       " [506 rows x 2 columns])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_field_wise_minibatch_gradient_descent_lr(t.columns,\n",
    "                                            'bin_lstat',\n",
    "                                            t.copy(),\n",
    "                                            y_all.copy(),\n",
    "                                            all_features=True,\n",
    "                                            return_clf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5351057228771896\n",
      "0.514191844462476\n",
      "0.44325648735939416\n",
      "0.47713712949262566\n",
      "0.5020683181391501\n",
      "0.3120932970582152\n",
      "0.38352186848678665\n",
      "0.2823015607846881\n",
      "0.42472912987446904\n",
      "0.5869091371931332\n",
      "0.3718836014191844\n",
      "0.4879480693045678\n",
      "0.4160421936900387\n",
      "0.35687238477081445\n",
      "0.3774521502553578\n",
      "0.5063799659523014\n",
      "0.3926781537874088\n",
      "0.5935675305872432\n",
      "0.48568087442126867\n",
      "0.6574228755986189\n",
      "0.4016116971345839\n",
      "0.3151003134297488\n",
      "0.25449063688288553\n",
      "0.5003818433487662\n",
      "0.3852162983469365\n",
      "0.5034922756272573\n",
      "0.3169140693363881\n",
      "0.3631250696068605\n",
      "0.3733155139770576\n",
      "0.5591300335703944\n",
      "0.5270154169252065\n",
      "0.47917362735271196\n",
      "0.5523523141297949\n",
      "0.46956390307543\n",
      "0.4224698900609359\n",
      "0.6431355702989515\n",
      "0.6554500182966605\n",
      "0.44380538717324547\n",
      "0.6431992108570792\n",
      "0.5859465737514519\n",
      "0.38883585509044916\n",
      "0.364692218350755\n",
      "0.5630598380347795\n",
      "0.5472053839912177\n",
      "0.436820835918731\n",
      "0.4804543935850317\n",
      "0.633637216998393\n",
      "0.4551572717292731\n",
      "0.5200785960892876\n",
      "0.4042050498782874\n",
      "0.6557523109477671\n",
      "0.3814217300685727\n",
      "0.46355782540212875\n",
      "0.5212957217634799\n",
      "0.6625379854581324\n",
      "0.49316659507103877\n",
      "0.4394062335926686\n",
      "0.4245700284791498\n",
      "0.5813564984964918\n",
      "0.6189839784894914\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-83-aa9f82f566c9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                             \u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                             \u001b[0mall_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                             return_clf=True)\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'bin_lstat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preds'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-72-7a827757bd18>\u001b[0m in \u001b[0;36mrun_field_wise_minibatch_gradient_descent_lr\u001b[1;34m(this_feature, y_feature, X_all, y_all, all_features, return_clf)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[0mthis_batch_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mthis_X\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mthis_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_all\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_batch_samples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis_batch_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthis_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_feature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthis_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_feature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_clf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y, classes, sample_weight)\u001b[0m\n\u001b[0;32m    692\u001b[0m                                  \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    693\u001b[0m                                  \u001b[0mclasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 694\u001b[1;33m                                  coef_init=None, intercept_init=None)\n\u001b[0m\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    696\u001b[0m     def fit(self, X, y, coef_init=None, intercept_init=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_partial_fit\u001b[1;34m(self, X, y, alpha, C, loss, learning_rate, max_iter, classes, sample_weight, coef_init, intercept_init)\u001b[0m\n\u001b[0;32m    523\u001b[0m                              \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m                              \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m                              max_iter=max_iter)\n\u001b[0m\u001b[0;32m    526\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             raise ValueError(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit_binary\u001b[1;34m(self, X, y, alpha, C, sample_weight, learning_rate, max_iter)\u001b[0m\n\u001b[0;32m    582\u001b[0m                                               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_expanded_class_weight\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m                                               \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m                                               random_state=self.random_state)\n\u001b[0m\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt_\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mn_iter_\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py\u001b[0m in \u001b[0;36mfit_binary\u001b[1;34m(est, i, X, y, alpha, C, learning_rate, max_iter, pos_weight, neg_weight, sample_weight, validation_mask, random_state)\u001b[0m\n\u001b[0;32m    411\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0my_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m     \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m     dataset, intercept_decay = make_dataset(\n\u001b[0;32m    415\u001b[0m         X, y_i, sample_weight, random_state=random_state)\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\master\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_random_state\u001b[1;34m(seed)\u001b[0m\n\u001b[0;32m    863\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmtrand\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 865\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    866\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomState\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_mt19937.pyx\u001b[0m in \u001b[0;36mnumpy.random._mt19937.MT19937.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.BitGenerator.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.SeedSequence.__init__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m_bit_generator.pyx\u001b[0m in \u001b[0;36mnumpy.random._bit_generator.SeedSequence.get_assembled_entropy\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for col in pairwise_cols:\n",
    "    t['x'] = t[col[0]] | t[col[1]]\n",
    "    _,_, preds = run_field_wise_minibatch_gradient_descent_lr(t.columns,\n",
    "                                            'bin_lstat',\n",
    "                                            t.copy(),\n",
    "                                            y_all.copy(),\n",
    "                                            all_features=True,\n",
    "                                            return_clf=True)\n",
    "    print(roc_auc_score(preds['bin_lstat'], preds['preds']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4837637026076718"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dict = _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processors:  4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "print(\"Number of processors: \", mp.cpu_count())\n",
    "all_columns = list(original_feature_set)\n",
    "pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "all_results = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_cols = pairwise_cols[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_preds(original_feature_set, new_coef_dict):\n",
    "    col_coefs = np.array(list(new_coef_dict.values())).reshape(-1,1)\n",
    "    this_pred = np.add(np.round(original_feature_set.values.dot(col_coefs),3), intercept)\n",
    "    return 1/(1 + np.exp(-this_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.218"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coef_dict = coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_feature_set['teste_feature'] = original_feature_set['DIS_9_bin_1'] | original_feature_set['DIS_9_bin_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_coef_dict['teste_feature'] = 0.218"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_all['new_preds'] = make_preds(original_feature_set, new_coef_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.60009471884184"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.log_loss(y_all['bin_lstat'], y_all['new_preds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_lstat</th>\n",
       "      <th>new_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>502</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>505</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     bin_lstat  new_preds\n",
       "0            0        0.0\n",
       "1            0        0.0\n",
       "2            0        0.0\n",
       "3            0        1.0\n",
       "4            0        0.0\n",
       "..         ...        ...\n",
       "501          0        1.0\n",
       "502          0        1.0\n",
       "503          0        1.0\n",
       "504          0        1.0\n",
       "505          0        0.0\n",
       "\n",
       "[506 rows x 2 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -18.59017554414894\n",
       " hess_inv: array([[1]])\n",
       "      jac: array([0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 9\n",
       "      nit: 1\n",
       "     njev: 3\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-5.05])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: 5.552074997367714e-17\n",
       " hess_inv: array([[0.50000004]])\n",
       "      jac: array([-1.28826571e-12])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 21\n",
       "      nit: 4\n",
       "     njev: 7\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([-7.45122473e-09])"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fun(x, a,b,c):\n",
    "    return a*x**2 + b*x + c\n",
    "\n",
    "minimize(fun, 100, args=(1,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      fun: -0.5860022592398136\n",
       " hess_inv: array([[1]])\n",
       "      jac: array([0.])\n",
       "  message: 'Optimization terminated successfully.'\n",
       "     nfev: 3\n",
       "      nit: 0\n",
       "     njev: 1\n",
       "   status: 0\n",
       "  success: True\n",
       "        x: array([0.])"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds['teste'] = np.round(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5860022592398136"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve(preds['bin_lstat'], preds['teste'], pos_label=1)\n",
    "metrics.auc(fpr, tpr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bin_lstat</th>\n",
       "      <th>preds</th>\n",
       "      <th>residual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447</th>\n",
       "      <td>1</td>\n",
       "      <td>0.484</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>0</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-1.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bin_lstat  preds  residual\n",
       "169          1  1.000     0.000\n",
       "447          1  0.484     0.516\n",
       "490          1  1.000     0.000\n",
       "473          1  1.000     0.000\n",
       "438          1  1.000     0.000\n",
       "149          1  1.000     0.000\n",
       "165          0  1.000    -1.000\n",
       "496          1  1.000     0.000\n",
       "337          1  0.000     1.000\n",
       "227          0  1.000    -1.000"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals.sort_values(['residual']).sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.000    295\n",
       " 1.000    133\n",
       "-1.000     53\n",
       " 0.996      2\n",
       "-0.199      2\n",
       " 0.990      1\n",
       " 0.125      1\n",
       "-0.108      1\n",
       "-0.044      1\n",
       " 0.218      1\n",
       "-0.036      1\n",
       "-0.120      1\n",
       "-0.790      1\n",
       "-0.976      1\n",
       "-0.998      1\n",
       "-0.999      1\n",
       " 0.516      1\n",
       "-0.001      1\n",
       "-0.846      1\n",
       "-0.958      1\n",
       "-0.917      1\n",
       "-0.996      1\n",
       "-0.952      1\n",
       " 0.979      1\n",
       " 0.998      1\n",
       " 0.975      1\n",
       "Name: residual, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residuals['residual'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1/ (1 * -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master",
   "language": "python",
   "name": "master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
