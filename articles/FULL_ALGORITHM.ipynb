{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import operator\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "import pymit\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from category_encoders import one_hot, target_encoder\n",
    "created_features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'poker'\n",
    "target = 'Class'\n",
    "positive_target = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'datasets/{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att_1</th>\n",
       "      <th>att_2</th>\n",
       "      <th>att_3</th>\n",
       "      <th>att_4</th>\n",
       "      <th>att_5</th>\n",
       "      <th>att_6</th>\n",
       "      <th>att_7</th>\n",
       "      <th>att_8</th>\n",
       "      <th>att_9</th>\n",
       "      <th>att_10</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{0 1</td>\n",
       "      <td>1 10</td>\n",
       "      <td>2 1</td>\n",
       "      <td>3 11</td>\n",
       "      <td>4 1</td>\n",
       "      <td>5 13</td>\n",
       "      <td>6 1</td>\n",
       "      <td>7 12</td>\n",
       "      <td>8 1</td>\n",
       "      <td>9 1</td>\n",
       "      <td>10 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{0 2</td>\n",
       "      <td>1 11</td>\n",
       "      <td>2 2</td>\n",
       "      <td>3 13</td>\n",
       "      <td>4 2</td>\n",
       "      <td>5 10</td>\n",
       "      <td>6 2</td>\n",
       "      <td>7 12</td>\n",
       "      <td>8 2</td>\n",
       "      <td>9 1</td>\n",
       "      <td>10 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{0 3</td>\n",
       "      <td>1 12</td>\n",
       "      <td>2 3</td>\n",
       "      <td>3 11</td>\n",
       "      <td>4 3</td>\n",
       "      <td>5 13</td>\n",
       "      <td>6 3</td>\n",
       "      <td>7 10</td>\n",
       "      <td>8 3</td>\n",
       "      <td>9 1</td>\n",
       "      <td>10 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{0 4</td>\n",
       "      <td>1 10</td>\n",
       "      <td>2 4</td>\n",
       "      <td>3 11</td>\n",
       "      <td>4 4</td>\n",
       "      <td>5 1</td>\n",
       "      <td>6 4</td>\n",
       "      <td>7 13</td>\n",
       "      <td>8 4</td>\n",
       "      <td>9 12</td>\n",
       "      <td>10 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{0 4</td>\n",
       "      <td>1 1</td>\n",
       "      <td>2 4</td>\n",
       "      <td>3 13</td>\n",
       "      <td>4 4</td>\n",
       "      <td>5 12</td>\n",
       "      <td>6 4</td>\n",
       "      <td>7 11</td>\n",
       "      <td>8 4</td>\n",
       "      <td>9 10</td>\n",
       "      <td>10 9}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025005</th>\n",
       "      <td>{0 3</td>\n",
       "      <td>1 1</td>\n",
       "      <td>2 1</td>\n",
       "      <td>3 12</td>\n",
       "      <td>4 2</td>\n",
       "      <td>5 9</td>\n",
       "      <td>6 4</td>\n",
       "      <td>7 9</td>\n",
       "      <td>8 2</td>\n",
       "      <td>9 6</td>\n",
       "      <td>10 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025006</th>\n",
       "      <td>{0 3</td>\n",
       "      <td>1 3</td>\n",
       "      <td>2 4</td>\n",
       "      <td>3 5</td>\n",
       "      <td>4 2</td>\n",
       "      <td>5 7</td>\n",
       "      <td>6 1</td>\n",
       "      <td>7 4</td>\n",
       "      <td>8 4</td>\n",
       "      <td>9 3</td>\n",
       "      <td>10 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025007</th>\n",
       "      <td>{0 1</td>\n",
       "      <td>1 11</td>\n",
       "      <td>2 4</td>\n",
       "      <td>3 7</td>\n",
       "      <td>4 3</td>\n",
       "      <td>5 9</td>\n",
       "      <td>6 1</td>\n",
       "      <td>7 13</td>\n",
       "      <td>8 2</td>\n",
       "      <td>9 7</td>\n",
       "      <td>10 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025008</th>\n",
       "      <td>{0 3</td>\n",
       "      <td>1 11</td>\n",
       "      <td>2 1</td>\n",
       "      <td>3 8</td>\n",
       "      <td>4 1</td>\n",
       "      <td>5 1</td>\n",
       "      <td>6 3</td>\n",
       "      <td>7 13</td>\n",
       "      <td>8 2</td>\n",
       "      <td>9 8</td>\n",
       "      <td>10 1}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025009</th>\n",
       "      <td>{0 2</td>\n",
       "      <td>1 5</td>\n",
       "      <td>2 2</td>\n",
       "      <td>3 9</td>\n",
       "      <td>4 4</td>\n",
       "      <td>5 9</td>\n",
       "      <td>6 2</td>\n",
       "      <td>7 3</td>\n",
       "      <td>8 3</td>\n",
       "      <td>9 3</td>\n",
       "      <td>10 2}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1025010 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        att_1 att_2 att_3 att_4 att_5 att_6 att_7 att_8 att_9 att_10  class\n",
       "0        {0 1  1 10   2 1  3 11   4 1  5 13   6 1  7 12   8 1    9 1  10 9}\n",
       "1        {0 2  1 11   2 2  3 13   4 2  5 10   6 2  7 12   8 2    9 1  10 9}\n",
       "2        {0 3  1 12   2 3  3 11   4 3  5 13   6 3  7 10   8 3    9 1  10 9}\n",
       "3        {0 4  1 10   2 4  3 11   4 4   5 1   6 4  7 13   8 4   9 12  10 9}\n",
       "4        {0 4   1 1   2 4  3 13   4 4  5 12   6 4  7 11   8 4   9 10  10 9}\n",
       "...       ...   ...   ...   ...   ...   ...   ...   ...   ...    ...    ...\n",
       "1025005  {0 3   1 1   2 1  3 12   4 2   5 9   6 4   7 9   8 2    9 6  10 1}\n",
       "1025006  {0 3   1 3   2 4   3 5   4 2   5 7   6 1   7 4   8 4    9 3  10 1}\n",
       "1025007  {0 1  1 11   2 4   3 7   4 3   5 9   6 1  7 13   8 2    9 7  10 1}\n",
       "1025008  {0 3  1 11   2 1   3 8   4 1   5 1   6 3  7 13   8 2    9 8  10 1}\n",
       "1025009  {0 2   1 5   2 2   3 9   4 4   5 9   6 2   7 3   8 3    9 3  10 2}\n",
       "\n",
       "[1025010 rows x 11 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, target, positive_target):\n",
    "    \n",
    "    def clean_target(df, target, positive_target):\n",
    "        df.loc[df[target] != positive_target, target] = 0\n",
    "        df.loc[df[target] == positive_target, target] = 1\n",
    "        y_all = df[[target]].astype(int)\n",
    "        X_all = df.drop([target], axis=1)\n",
    "        return X_all, y_all\n",
    "    \n",
    "    def nulls(df, th_del=0.8, th_mean=0.2):\n",
    "        null = df.isnull().sum().sort_values()[::-1]\n",
    "        null = null[null > 0] / len(df)\n",
    "        null = pd.concat([df[null.index].dtypes, null],axis=1)\n",
    "        null.columns=['dtype', 'ratio_null']\n",
    "        null.index.name = 'feature'\n",
    "        if null.empty:\n",
    "            return df\n",
    "        \n",
    "        for idx, row in null.iterrows():\n",
    "            if row['ratio_null'] > th_del:\n",
    "                df = df.drop(idx, axis=1)\n",
    "            \n",
    "            elif row['ratio_null'] < th_mean:\n",
    "                df[idx] = df.fillna(df[idx].mean)\n",
    "            \n",
    "            else:\n",
    "                if row['dtype'] == 'object':\n",
    "                    df[idx] = df[idx].fillna('my_nan_value')\n",
    "                else:\n",
    "                    df[idx] = df[idx].fillna(df[idx].min()*1000)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_groups(df, target, max_group_size=10):\n",
    "        is_object = ['object']\n",
    "        object_features = list(df.select_dtypes(include=is_object).columns)\n",
    "        if target in object_features:\n",
    "            object_features.remove(target)\n",
    "        for col in object_features:\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except:\n",
    "                len_unique = len(df[col].unique())\n",
    "                if len_unique < max_group_size:\n",
    "                    df = df.rename({col: f\"{col}_group\"},axis=1)\n",
    "                else:\n",
    "                    df = df.rename({col: f\"{col}_mean_encode\"},axis=1)\n",
    "        return df\n",
    "                    \n",
    "        \n",
    "    \n",
    "    df = nulls(df.copy())\n",
    "    df = clean_groups(df.copy(), target)\n",
    "    X_all, y_all = clean_target(df.copy(), target, positive_target)\n",
    "    return X_all, y_all\n",
    "\n",
    "def prepare_bases_to_modeling(X_train_ori, X_test_ori):\n",
    "    \n",
    "    one_hot_cols = [x for x in X_train_ori.columns if 'group' in x]\n",
    "    mean_encoding_cols = [x for x in X_train_ori.columns if 'encode' in x]\n",
    "\n",
    "    if len(one_hot_cols) > 0:\n",
    "        enc = one_hot.OneHotEncoder(cols=one_hot_cols, drop_invariant=True)\n",
    "        X_train_ori = enc.fit_transform(X_train_ori.copy())\n",
    "        X_test_ori = enc.transform(X_test_ori.copy())\n",
    "#         X_train = X_train.drop(ORIGINAL_FEATURES, axis=1)\n",
    "#         X_train = pd.concat([X_train, X_train_ori], axis=1)\n",
    "#         X_test = pd.concat([X_test, X_test_ori], axis=1)\n",
    "\n",
    "    if len(mean_encoding_cols) > 0:\n",
    "        enc = target_encoder.TargetEncoder(cols=mean_encoding_cols, drop_invariant=True)\n",
    "        X_train_ori = training_numeric_dataset = enc.fit_transform(X_train_ori.copy(), y_train)\n",
    "        X_test_ori = testing_numeric_dataset = enc.transform(X_test_ori.copy())\n",
    "#         X_train = X_train.drop(ORIGINAL_FEATURES, axis=1)\n",
    "#         X_train = pd.concat([X_train, X_train_ori], axis=1)\n",
    "#         X_test = pd.concat([X_test, X_test_ori], axis=1)\n",
    "    \n",
    "    return X_train_ori, X_test_ori\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = clean_df(df.copy(), target, positive_target)\n",
    "X_train_ori, X_test_ori, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.3, random_state = 0)\n",
    "X_train_ori, X_test_ori = prepare_bases_to_modeling(X_train_ori.copy(), X_test_ori.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def numpy_discretize(X_train, X_test, gran=10, retry=True):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 10\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    global created_features_dict\n",
    "\n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    # cehca se nao tem _disc somente nos ultimos caracteres\n",
    "    numeric_features = [feat for feat in numeric_features.columns if 'disc' not in feat[-4:]]\n",
    "    X_train_numeric_np = X_train[numeric_features].T.to_numpy()\n",
    "    X_test_numeric_np = X_test[numeric_features].T.to_numpy()\n",
    "    # cacheando posicoes das features\n",
    "    dict_feature_order = {}\n",
    "    for feat in numeric_features:\n",
    "        dict_feature_order[feat] = X_train.columns.get_loc(feat)\n",
    "    shape_X_train = X_train.shape[0]\n",
    "    shape_X_test = X_test.shape[0]\n",
    "    feat_count = 0\n",
    "    with tqdm(total=len(numeric_features)) as pbar:\n",
    "        for feat in numeric_features:\n",
    "            feat_index = dict_feature_order[feat]\n",
    "            this_gran = gran\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    D_train = np.zeros([shape_X_train, 1])\n",
    "                    D_test = np.zeros([shape_X_test, 1])\n",
    "                    # calc numpy histogram and apply to features\n",
    "                    hist, bin_edges = np.histogram(X_train_numeric_np[feat_index], bins=this_gran)\n",
    "                    D_train[:, 0] = np.digitize(X_train_numeric_np[feat_index], bin_edges, right=False)\n",
    "                    D_test[:, 0] = np.digitize(X_test_numeric_np[feat_index], bin_edges, right=False)\n",
    "\n",
    "                    # apply back to pandas\n",
    "                    X_train[f\"{feat}_disc\"] = D_train\n",
    "                    X_test[f\"{feat}_disc\"] = D_test\n",
    "\n",
    "                    success = True\n",
    "                except:\n",
    "#                     traceback.print_exc()\n",
    "                    if retry:\n",
    "#                         print(f\"Not possible to correct work on cut {feat} > {this_gran}\")\n",
    "                        this_gran = this_gran - 1\n",
    "                    else:\n",
    "                        this_gran = 1\n",
    "                        \n",
    "                    if this_gran <= 1:\n",
    "                        success = True\n",
    "\n",
    "                if success and this_gran > 1:\n",
    "                    #upoad global dict with feature info\n",
    "                    created_features_dict[f\"{feat}_disc\"] = {\n",
    "                        \"num_of_source_features\": 1,\n",
    "                        \"source_feature_name\": [feat],\n",
    "                        \"source_feature_type\": ['numeric'],\n",
    "                        \"target_feature_type\": ['discrete'],\n",
    "                        \"operator\": \"discretizer\"\n",
    "                    }\n",
    "\n",
    "            feat_count += 1\n",
    "            pbar.update(1)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "\n",
    "def min_max_scaler(X_train, X_test):\n",
    "    global created_features_dict\n",
    "    \n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = list(X_train.select_dtypes(include=is_numeric).columns)\n",
    "    numeric_features = [x for x in numeric_features if 'disc' not in x[-4:]]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train[numeric_features])\n",
    "    norm_feats = [f\"{x}_norm\" for x in ORIGINAL_FEATURES if 'disc' not in x[-4:] and x in numeric_features]\n",
    "    \n",
    "    for feat in [x for x in ORIGINAL_FEATURES if 'disc' not in x[-4:]]:\n",
    "        #upoad global dict with feature info\n",
    "        created_features_dict[f\"{feat}_norm\"] = {\n",
    "            \"num_of_source_features\": 1,\n",
    "            \"source_feature_name\": [feat],\n",
    "            \"source_feature_type\": ['numeric'],\n",
    "            \"target_feature_type\": ['numeric'],\n",
    "            \"operator\": \"normalizer\"\n",
    "        }\n",
    "    X_train = X_train.reindex(columns=X_train.columns.tolist() + norm_feats)\n",
    "    X_test = X_test.reindex(columns=X_test.columns.tolist() + norm_feats)\n",
    "    X_train.loc[:, norm_feats] = scaler.transform(X_train[numeric_features])\n",
    "    X_test.loc[:, norm_feats] = scaler.transform(X_test[numeric_features])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def binary_operators(df):\n",
    "    global created_features_dict\n",
    "    \n",
    "    # calc all pair columns\n",
    "    all_columns = list(df)\n",
    "    all_columns = [x for x in all_columns if \"disc\" not in x and \"group\" not in x and \"encode\" not in x]\n",
    "    pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "    tmp_dfs = []\n",
    "    with tqdm(total=len(pairwise_cols)) as pbar:\n",
    "        for pair in pairwise_cols:\n",
    "            tmp_df = df[[pair[0], pair[1]]].copy()\n",
    "            \n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_sum\"] = tmp_df[pair[0]] + tmp_df[pair[1]]\n",
    "\n",
    "            #upoad global dict with feature info\n",
    "            type_pair1 = 'disc' if 'disc' in pair[0] else 'numeric'\n",
    "            type_pair2 = 'disc' if 'disc' in pair[1] else 'numeric'\n",
    "\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_sum\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_sum\"\n",
    "            }        \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_sub\"] = tmp_df[pair[0]] - tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_sub\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_sub\"\n",
    "            }             \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_mul\"] = tmp_df[pair[0]] * tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_mul\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_mul\"\n",
    "            }             \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_div\"] = tmp_df[pair[0]] / tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_div\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_div\"\n",
    "            }   \n",
    "            tmp_df = tmp_df.replace([np.inf, -np.inf], np.nan)\n",
    "            tmp_dfs.append(tmp_df)\n",
    "            pbar.update(1)\n",
    "        tmp_df_concat = pd.concat(tmp_dfs, axis=1)\n",
    "        df = pd.concat([df, tmp_df_concat], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def high_order_operators(df):\n",
    "    \n",
    "    def _update_dict(group_col, columns, op):\n",
    "        global created_features_dict\n",
    "        \n",
    "        for feat in columns:\n",
    "            created_features_dict[f'{feat}_group_by_{col}_and_{op}'] = {\n",
    "            \"num_of_source_features\": 2,\n",
    "            \"source_feature_name\": [group_col, feat],\n",
    "            \"source_feature_type\": ['discrete', 'numeric'],\n",
    "            \"target_feature_type\": ['numeric'],\n",
    "            \"operator\": f\"group_{op}\"\n",
    "            }\n",
    "            \n",
    "    group_columns = [col for col in df.columns if \"disc\" in col or \"group\" in col]\n",
    "    to_group_columns = [col for col in df.columns if \"disc\" not in col and \"group\" not in col]\n",
    "    all_dfs = pd.DataFrame()\n",
    "    for col in group_columns:\n",
    "        print(f\"Grouping {col}\")\n",
    "        \n",
    "        df_avg = df[to_group_columns+[col]].groupby(col).transform('mean').add_suffix(f'_group_by_{col}_and_mean')\n",
    "        _update_dict(col, to_group_columns, 'mean')\n",
    "        df_min = df[to_group_columns+[col]].groupby(col).transform('min').add_suffix(f'_group_by_{col}_and_min')\n",
    "        _update_dict(col, to_group_columns, 'min')\n",
    "        df_max = df[to_group_columns+[col]].groupby(col).transform('max').add_suffix(f'_group_by_{col}_and_max')\n",
    "        _update_dict(col, to_group_columns, 'max')\n",
    "        \n",
    "        all_dfs = pd.concat([all_dfs, df_avg, df_min, df_max], axis=1,  sort=False)\n",
    "    all_dfs = pd.concat([df,all_dfs], axis=1)\n",
    "    return all_dfs\n",
    "\n",
    "def _entropy_based_measures(X_train, y_train, target):\n",
    "\n",
    "    df_mutual_info = pd.DataFrame()\n",
    "    fail_count = 0\n",
    "    with tqdm(total=len( X_train.columns)) as pbar:\n",
    "        for feat in X_train.columns:\n",
    "            try:\n",
    "                df_mutual_info[feat] = [pymit.I(X_train[feat].values, y_train[target].values , bins=[10,2])]\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                fail_count += 1\n",
    "    print(fail_count)\n",
    "    return df_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 687.69it/s]\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_FEATURES = X_train_ori.columns\n",
    "\n",
    "# discretize\n",
    "X_train, X_test = numpy_discretize(X_train_ori.copy(), X_test_ori.copy(), gran=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train, X_test = min_max_scaler(X_train.copy(), X_test.copy())\n",
    "\n",
    "step1_train = X_train.copy()\n",
    "step1_test = X_test.copy()\n",
    "step1_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping V2_group_1\n",
      "Grouping V2_group_2\n",
      "Grouping V1_disc\n",
      "Grouping V2_group_1_disc\n",
      "Grouping V2_group_2_disc\n",
      "Grouping V3_disc\n",
      "Grouping V4_disc\n",
      "Grouping V5_disc\n",
      "Grouping V6_disc\n",
      "Grouping V7_disc\n",
      "Grouping V8_disc\n",
      "Grouping V9_disc\n",
      "Grouping V10_disc\n",
      "Grouping V2_group_1_norm\n",
      "Grouping V2_group_2_norm\n",
      "Grouping V2_group_1\n",
      "Grouping V2_group_2\n",
      "Grouping V1_disc\n",
      "Grouping V2_group_1_disc\n",
      "Grouping V2_group_2_disc\n",
      "Grouping V3_disc\n",
      "Grouping V4_disc\n",
      "Grouping V5_disc\n",
      "Grouping V6_disc\n",
      "Grouping V7_disc\n",
      "Grouping V8_disc\n",
      "Grouping V9_disc\n",
      "Grouping V10_disc\n",
      "Grouping V2_group_1_norm\n",
      "Grouping V2_group_2_norm\n"
     ]
    }
   ],
   "source": [
    "X_train = high_order_operators(X_train[step1_features].copy())\n",
    "X_test = high_order_operators(X_test[step1_features].copy())\n",
    "\n",
    "step2_train = X_train.copy()\n",
    "step2_test = X_test.copy()\n",
    "step2_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [00:00<00:00, 204.35it/s]\n",
      "100%|██████████| 153/153 [00:00<00:00, 232.12it/s]\n"
     ]
    }
   ],
   "source": [
    "# binary operators\n",
    "X_train = binary_operators(X_train[step1_features].copy())\n",
    "X_test = binary_operators(X_test[step1_features].copy())\n",
    "\n",
    "step3_train = X_train.copy()\n",
    "step3_test = X_test.copy()\n",
    "step3_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([step1_train, step2_train, step3_train], axis=1).dropna(axis=1, thresh=0.03*X_train.shape[0])\n",
    "X_test = pd.concat([step1_test, step2_test, step3_test], axis=1)[X_train.columns]\n",
    "X_train = X_train.loc[:,~X_train.columns.duplicated()]\n",
    "X_test = X_test.loc[:,~X_test.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1444/1444 [00:02<00:00, 691.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.12 s, sys: 27 ms, total: 2.14 s\n",
      "Wall time: 2.12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# discretize\n",
    "X_train, X_test = numpy_discretize(X_train.copy(), X_test.copy(), gran=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 2643/2760 [00:03<00:00, 697.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1929"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy_filter = _entropy_based_measures(X_train, y_train, target)\n",
    "features_to_keep = np.round(entropy_filter.T.sort_values(by=0), 2)\n",
    "to_keep = list(features_to_keep.loc[features_to_keep[0]>0].index)\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_based_meta_features(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Applied in the original set!\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _general_information(X):\n",
    "        dataset_info_df= pd.DataFrame()\n",
    "\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        num_numeric_attr = X.select_dtypes(include=numerics).shape[1]\n",
    "        num_duscrete_attr = X.shape[1] - num_numeric_attr\n",
    "            \n",
    "        \n",
    "        dataset_info_df['num_instances'] = [X.shape[0]]\n",
    "        dataset_info_df['num_features'] = X.shape[1]\n",
    "        \n",
    "        dataset_info_df['num_numeric_attr'] = num_numeric_attr\n",
    "        dataset_info_df['num_discrete_attr'] = num_duscrete_attr\n",
    "        dataset_info_df['ratio_numeric_attr'] = num_numeric_attr/ (num_numeric_attr+num_duscrete_attr)\n",
    "        dataset_info_df['ratio_discrete_attr'] = num_duscrete_attr/ (num_numeric_attr+num_duscrete_attr)\n",
    "        \n",
    "        return dataset_info_df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def _initial_evaluation(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn import metrics\n",
    "        \n",
    "        def acc(y_true, y_pred):\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=0.5 else 1, y_pred))\n",
    "            return accuracy_score(y_true, y_pred)\n",
    "\n",
    "        def f1(y_true, y_pred, th):\n",
    "            from sklearn.metrics import f1_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return f1_score(y_true, y_pred)\n",
    "\n",
    "        def precision(y_true, y_pred, th):\n",
    "            from sklearn.metrics import precision_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return precision_score(y_true, y_pred, average='macro') \n",
    "\n",
    "        def recall(y_true, y_pred, th):\n",
    "            from sklearn.metrics import recall_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        def auc(y_true, y_pred):\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        df_initial_evaluation = pd.DataFrame()\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "        for th in [0.4, 0.45, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "            df_initial_evaluation[f'f1_{th}'] = [f1(y_test, y_pred, th)]\n",
    "            df_initial_evaluation[f'precision_{th}'] = precision(y_test, y_pred, th)\n",
    "            df_initial_evaluation[f'recall_{th}'] = recall(y_test, y_pred, th)\n",
    "\n",
    "        df_initial_evaluation['auc'] = auc(y_test, y_pred)\n",
    "        \n",
    "        df_initial_evaluation['avg_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].min(axis=1)\n",
    "    \n",
    "        df_initial_evaluation['avg_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].min(axis=1)\n",
    "    \n",
    "        df_initial_evaluation['avg_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].min(axis=1)\n",
    "    \n",
    "        return df_initial_evaluation\n",
    "    \n",
    "    \n",
    "    def _entropy_based_measures(X_train, y_train):\n",
    "\n",
    "        df_mutual_info = pd.DataFrame()\n",
    "\n",
    "        for feat in X_train.columns:\n",
    "            df_mutual_info[feat] = [pymit.I(X_train[feat].values, y_train[target].values , bins=[10,2])]\n",
    "        \n",
    "        df_mutual_info['avg_mi'] = df_mutual_info.mean(axis=1)\n",
    "        df_mutual_info['std_mi'] = df_mutual_info.std(axis=1)\n",
    "        df_mutual_info['min_mi'] = df_mutual_info.min(axis=1)\n",
    "        df_mutual_info['max_mi'] = df_mutual_info.max(axis=1)\n",
    "        return df_mutual_info[['avg_mi', 'std_mi', 'min_mi', 'max_mi']]\n",
    "    \n",
    "    \n",
    "    def _feature_diversity(X_train):\n",
    "        \n",
    "        df_feature_diversity = pd.DataFrame()\n",
    "        \n",
    "        disc_columns = [col for col in X_train if 'disc' in col and 'group' not in col and 'encode' not in col]\n",
    "        numeric_columns = [col for col in X_train if 'disc' not in col and 'group' not in col and 'encode' not in col]\n",
    "        \n",
    "        numeric_pairs = list(itertools.combinations(numeric_columns, 2))\n",
    "        all_t = []\n",
    "        for pair in numeric_pairs:\n",
    "            t_pair = stats.ttest_rel(X_train[pair[0]].values, X_train[pair[1]].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "        \n",
    "        all_chi = []\n",
    "        disc_pairs = list(itertools.combinations(disc_columns, 2))\n",
    "        for pair in disc_pairs:\n",
    "            contingency = pd.crosstab(X_train[pair[0]].values, X_train[pair[1]].values) \n",
    "            chi, _, _, _ = stats.chi2_contingency(contingency) \n",
    "            all_chi.append(chi)\n",
    "        \n",
    "        if len(all_t) == 0:\n",
    "            df_feature_diversity['avg_t'] = [-99]\n",
    "            df_feature_diversity['std_t'] = -99\n",
    "            df_feature_diversity['max_t'] = -99\n",
    "            df_feature_diversity['min_t'] = -99\n",
    "        else:\n",
    "            df_feature_diversity['avg_t'] = [np.mean(all_t)]\n",
    "            df_feature_diversity['std_t'] = np.std(all_t)\n",
    "            df_feature_diversity['max_t'] = np.max(all_t)\n",
    "            df_feature_diversity['min_t'] = np.min(all_t)\n",
    "            \n",
    "        if len(all_chi) == 0:\n",
    "            df_feature_diversity['avg_chi'] = -99\n",
    "            df_feature_diversity['std_chi'] = -99\n",
    "            df_feature_diversity['max_chi'] = -99\n",
    "            df_feature_diversity['min_chi'] = -99\n",
    "        else:        \n",
    "            df_feature_diversity['avg_chi'] = np.mean(all_chi)\n",
    "            df_feature_diversity['std_chi'] = np.std(all_chi)\n",
    "            df_feature_diversity['max_chi'] = np.max(all_chi)\n",
    "            df_feature_diversity['min_chi'] = np.min(all_chi)\n",
    "\n",
    "        return df_feature_diversity\n",
    "    \n",
    "    dataset_info_df = _general_information(X_train.copy())\n",
    "    dataset_initial_eval = _initial_evaluation(X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy())\n",
    "    dataset_entropy_info = _entropy_based_measures(X_train.copy(), y_train.copy())\n",
    "    dataset_feature_diversity = _feature_diversity(X_train.copy())\n",
    "\n",
    "    \n",
    "    df = pd.concat([dataset_info_df, dataset_initial_eval, dataset_entropy_info, dataset_feature_diversity], axis=1)\n",
    "    return df\n",
    "        \n",
    "        \n",
    "def candidate_mi_and_stattest(operator_feat, X_train, y_train, X_train_ori, target):\n",
    "    # passo 1 da parte de features candidatas\n",
    "    tests_df = pd.DataFrame()\n",
    "    \n",
    "    this_feat = created_features_dict[operator_feat]\n",
    "    original_features = X_train_ori.columns\n",
    "\n",
    "    all_t = []\n",
    "    for original_feat in original_features:\n",
    "        tmp_df_stat_tests = pd.DataFrame()\n",
    "        tmp_df_stat_tests['feature_name'] = [operator_feat]\n",
    "        \n",
    "        # nao faz o teste na feature que deu origem a nova\n",
    "        if original_feat not in this_feat['source_feature_name']:\n",
    "            t_pair = stats.ttest_rel(X_train[operator_feat].values, X_train_ori[original_feat].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "    \n",
    "    try:\n",
    "        mutual_info = pymit.I(X_train[operator_feat].values, y_train[target].values , bins=[10,2])\n",
    "    except:\n",
    "        print(operator_feat)\n",
    "              \n",
    "    \n",
    "    if len(all_t) == 0:\n",
    "        tmp_df_stat_tests[f'{original_feat}_avg_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_std_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_max_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_min_t'] = -99\n",
    "    else:\n",
    "        tmp_df_stat_tests[f'{original_feat}_avg_t'] = np.mean(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_std_t'] = np.std(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_max_t'] = np.max(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_min_t'] = np.min(all_t)\n",
    "    \n",
    "    tmp_df_stat_tests[f'feat_mutual_info'] = mutual_info\n",
    "      \n",
    "    return tmp_df_stat_tests  \n",
    "\n",
    "\n",
    "def generic_meta_features(operator_feature, X_train):\n",
    "    # passo 2 das features candidatas\n",
    "    # https://github.com/giladkatz/ExploreKit/blob/master/src/main/java/explorekit/Evaluation/MLFeatureExtraction/OperatorAssignmentBasedAttributes.java\n",
    "    op_dict = created_features_dict[operator_feature]\n",
    "    df_generic_meta_feats = pd.DataFrame()\n",
    "    df_generic_meta_feats['feature_name'] = [operator_feature]\n",
    "    df_generic_meta_feats['num_sources'] = op_dict['num_of_source_features']\n",
    "    df_generic_meta_feats['num_numeric_sources'] = len([x for x in op_dict['source_feature_type'] if 'numeric' in x])\n",
    "    df_generic_meta_feats['num_discrete_sources'] = len([x for x in op_dict['source_feature_type'] if 'discrete' in x])\n",
    "    df_generic_meta_feats['discretizer_in_use'] = True if op_dict['operator']=='discretizer' else False\n",
    "    df_generic_meta_feats['normalizer_in_use'] = True if op_dict['operator']=='normalizer' else False\n",
    "    df_generic_meta_feats['group_in_use'] = True if 'group' in op_dict['operator'] else False\n",
    "    df_generic_meta_feats['binary_in_use'] = True if 'binary' in op_dict['operator'] else False\n",
    "    \n",
    "    # discrete sources\n",
    "    indices_discrete = [i for i, x in enumerate(op_dict['source_feature_type']) if x == \"discrete\"]\n",
    "    if len(indices_discrete) >= 1:\n",
    "        discrete_columns = [op_dict['source_feature_name'][i] for i in indices_discrete]\n",
    "        X_train_numpy = X_train[discrete_columns].astype(float).to_numpy()\n",
    "        df_generic_meta_feats['max_discrete_source_value'] = X_train_numpy.max()\n",
    "        df_generic_meta_feats['min_discrete_source_value'] = X_train_numpy.min()\n",
    "        df_generic_meta_feats['avg_discrete_source_value'] = X_train_numpy.mean()\n",
    "        df_generic_meta_feats['std_discrete_source_value'] = X_train_numpy.std()\n",
    "        \n",
    "        all_chi = []\n",
    "        for discrete_feat in discrete_columns:\n",
    "            # transform target feature in discrete\n",
    "            if 'disc' not in operator_feature:\n",
    "                this_feat_discrete, _ = numpy_discretize(X_train[[operator_feature]].copy(),\n",
    "                                                      X_train[[operator_feature]].copy(), gran=10)\n",
    "                contingency = pd.crosstab(X_train[discrete_feat].values, this_feat_discrete[f\"{operator_feature}_disc\"].values) \n",
    "            else:\n",
    "                this_feat_discrete = X_train[[operator_feature]]\n",
    "                contingency = pd.crosstab(X_train[discrete_feat].values, this_feat_discrete[f\"{operator_feature}\"].values) \n",
    "            \n",
    "            chi, _, _, _ = stats.chi2_contingency(contingency) \n",
    "            all_chi.append(chi)\n",
    "        \n",
    "        df_generic_meta_feats['max_chi_source_opattr_value'] = np.max(chi)\n",
    "        df_generic_meta_feats['min_chi_source_opattr_value'] = np.min(chi)\n",
    "        df_generic_meta_feats['avg_chi_source_opattr_value'] = np.mean(chi)\n",
    "        df_generic_meta_feats['std_chi_source_opattr_value'] = np.std(chi)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        df_generic_meta_feats['max_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['min_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['avg_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['std_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['max_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['min_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['avg_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['std_chi_source_opattr_value'] = 0\n",
    "        \n",
    "    # numeric sources\n",
    "    indices_numeric = [i for i, x in enumerate(op_dict['source_feature_type']) if x == \"numeric\"]\n",
    "    if len(indices_numeric) >= 1:\n",
    "        numeric_columns = [ op_dict['source_feature_name'][i] for i in indices_numeric]\n",
    "        try:\n",
    "            X_train_numpy = X_train[numeric_columns].to_numpy()\n",
    "            df_generic_meta_feats['max_numeric_source_value'] = np.max(X_train_numpy)\n",
    "            df_generic_meta_feats['min_numeric_source_value'] = np.min(X_train_numpy)\n",
    "            df_generic_meta_feats['avg_numeric_source_value'] = np.mean(X_train_numpy)\n",
    "            df_generic_meta_feats['std_numeric_source_value'] = np.std(X_train_numpy)\n",
    "        except:\n",
    "            print(X_train_numpy)\n",
    "            print(numeric_columns)\n",
    "            print(operator_feature)\n",
    "        all_t = []\n",
    "        for src_feat in numeric_columns:\n",
    "            t_pair = stats.ttest_rel(X_train[operator_feature].values,\n",
    "                                     X_train[src_feat].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "            \n",
    "        df_generic_meta_feats['max_ttest_source_opattr_value'] = np.max(all_t)\n",
    "        df_generic_meta_feats['min_ttest_source_opattr_value'] = np.min(all_t)\n",
    "        df_generic_meta_feats['avg_ttest_source_opattr_value'] = np.mean(all_t)\n",
    "        df_generic_meta_feats['std_ttest_source_opattr_value'] = np.std(all_t)\n",
    "           \n",
    "    else:\n",
    "        df_generic_meta_feats['max_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['min_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['avg_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['std_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['max_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['min_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['avg_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['std_ttest_source_opattr_value'] = 0\n",
    "\n",
    "    return df_generic_meta_feats\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/ipykernel_launcher.py:61: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "dataset_based_meta_features = dataset_based_meta_features(X_train_ori.copy(), X_test_ori.copy(), y_train.copy(), y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitered_keep_dict = {k: v for k, v in created_features_dict.items() if k in to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1920/1929 [01:21<00:00, 23.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 22s, sys: 1.72 s, total: 1min 23s\n",
      "Wall time: 1min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t_test_statistic_candidate_df = pd.DataFrame()\n",
    "general_meta_feature_candidates = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(to_keep)) as pbar:\n",
    "    for k, v in fitered_keep_dict.items():\n",
    "        all_features = list([k] + v['source_feature_name'])\n",
    "        tmp_t_test_statistic_candidate_df = candidate_mi_and_stattest(k, X_train[all_features].copy(), y_train.copy(), X_train_ori.copy(), target)\n",
    "        tmp_general_meta_feature_candidates = generic_meta_features(k, X_train[all_features].copy())\n",
    "\n",
    "        t_test_statistic_candidate_df = t_test_statistic_candidate_df.append(tmp_t_test_statistic_candidate_df)\n",
    "        general_meta_feature_candidates = general_meta_feature_candidates.append(tmp_general_meta_feature_candidates)\n",
    "        pbar.update(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = general_meta_feature_candidates.merge(t_test_statistic_candidate_df, on='feature_name')\n",
    "final_df.index = final_df['feature_name']\n",
    "final_df = final_df.drop(['feature_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 152 ms, sys: 0 ns, total: 152 ms\n",
      "Wall time: 149 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7542530157748221"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "base_clf = classifiers.random_forest_classifier(train_set= [X_train_ori, y_train],\n",
    "                         test_set= [X_test_ori, y_test],\n",
    "                         features= X_train_ori.columns,\n",
    "                         target= 'Class',\n",
    "                         test_metrics= ['auc'],\n",
    "                         project_name= dataset_name\n",
    "                         ) \n",
    "base_auc = base_clf['calc_metrics']['auc']\n",
    "base_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1920 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'V1_disc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2645\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'V1_disc'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2798\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2799\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2800\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2802\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2646\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2648\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2649\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'V1_disc'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_feature_error_diff = {}\n",
    "with tqdm(total=final_df.shape[0]) as pbar:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        this_X_train = X_train_ori.copy()\n",
    "        this_X_test = X_test_ori.copy()\n",
    "\n",
    "        this_X_train[idx] = X_train[idx]\n",
    "        this_X_test[idx] = X_test[idx]\n",
    "\n",
    "        this_clf = classifiers.random_forest_classifier(train_set= [this_X_train, y_train],\n",
    "                             test_set= [this_X_test, y_test],\n",
    "                             features= this_X_train.columns,\n",
    "                             target= 'Class',\n",
    "                             test_metrics= ['auc'],\n",
    "                             project_name= 'explore_kit'\n",
    "                             ) \n",
    "        this_auc = this_clf['calc_metrics']['auc']\n",
    "        error_diff = this_auc - base_auc\n",
    "        dict_feature_error_diff[idx] = error_diff\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = final_df.reset_index()\n",
    "tt['feature_goodness'] = tt['feature_name']\n",
    "tt['feature_goodness'] = tt['feature_goodness'].map(dict_feature_error_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    1097.000000\n",
       "mean        0.003852\n",
       "std         0.002185\n",
       "min         0.001000\n",
       "25%         0.002000\n",
       "50%         0.004000\n",
       "75%         0.005000\n",
       "max         0.015000\n",
       "Name: feature_goodness, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt['feature_goodness'] = np.round(tt['feature_goodness'],3)\n",
    "tt2 = tt.loc[tt['feature_goodness'] > 0]\n",
    "tt2['feature_goodness'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_value =  tt2['feature_goodness'].quantile(.75)\n",
    "th_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = list({i[1]:i[0] for i in sorted(zip(dict_feature_error_diff.values(), dict_feature_error_diff.keys()), reverse=True)[:300]}.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_features = {k: v for k, v in dict_feature_error_diff.items() if v > th_value}\n",
    "keep = list(only_features.keys())\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_info(final_df, dict_feature_error_diff, dataset_name, base_clf, keep, dataset_based_meta_features):\n",
    "    import joblib\n",
    "    joblib.dump(keep, f'ExploreKit/{dataset_name}_{len(keep)}_features_to_keep')\n",
    "    joblib.dump(dict_feature_error_diff, f'ExploreKit/{dataset_name}_dict_feature_error_diff')\n",
    "    joblib.dump(base_clf, f'ExploreKit/{dataset_name}_base_clf')\n",
    "    # save meta feature csv\n",
    "    final_df = final_df.reset_index()\n",
    "    final_df.to_csv(f'ExploreKit/{dataset_name}_meta_features.csv', index=False)\n",
    "    final_df.index = final_df['feature_name']\n",
    "    final_df = final_df.drop(['feature_name'], axis=1)\n",
    "    \n",
    "    # save dataset feature dict\n",
    "    joblib.dump(dict_feature_error_diff, f'ExploreKit/{dataset_name}.featuredict')\n",
    "\n",
    "    # join and save final dataset for mL modeling\n",
    "    for col in dataset_based_meta_features.columns:\n",
    "        final_df[col] = list(dataset_based_meta_features[col].values) * final_df.shape[0]\n",
    "    final_df['dataset'] = dataset_name\n",
    "\n",
    "    tt = final_df.reset_index()\n",
    "    tt['feature_goodness'] = tt['feature_name']\n",
    "    tt['feature_goodness'] = tt['feature_goodness'].map(dict_feature_error_diff)\n",
    "    joblib.dump(tt, f'ExploreKit/{dataset_name}_meta_ml_modeling.df')\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[keep]\n",
    "X_test = X_test[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_discretize_multi_gran(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    feat_count = 0\n",
    "    for feat in numeric_features:\n",
    "        if feat_count % 50 == 0:\n",
    "            print(f\" Working in {feat}\")\n",
    "        X_train_np = X_train[[feat]].to_numpy()\n",
    "        X_test_np = X_test[[feat]].to_numpy()\n",
    "        for gran in range(3, max_gran+1):\n",
    "            try:\n",
    "                D_train = np.zeros([X_train.shape[0], 1])\n",
    "                D_test = np.zeros([X_test.shape[0], 1])\n",
    "                # calc numpy histogram and apply to features\n",
    "                hist, bin_edges = np.histogram(X_train_np[:, 0], bins=gran)\n",
    "                D_train[:, 0] = np.digitize(X_train_np[:,0], bin_edges, right=False)\n",
    "                D_test[:, 0] = np.digitize(X_test_np[:,0], bin_edges, right=False)\n",
    "\n",
    "                # apply back to pandas\n",
    "                X_train[f\"{feat}_{gran}\"] = D_train\n",
    "                X_test[f\"{feat}_{gran}\"] = D_test\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        \n",
    "        feat_count += 1\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 347 features...\n",
      " Working in V3_disc\n",
      " Working in V10_norm_group_by_V3_disc_and_max\n",
      " Working in V3_x_V7_norm_op_sub\n",
      " Working in V10_x_V7_norm_op_sum\n",
      " Working in V6_group_by_V3_disc_and_min_disc\n",
      " Working in V4_group_by_V9_disc_and_max_disc\n",
      " Working in V5_x_V7_op_mul_disc\n"
     ]
    }
   ],
   "source": [
    "X_train_discrete, X_test_discrete = numpy_discretize_multi_gran(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjmi_selector(X, y, bins, max_features):\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    Y = y.to_numpy().ravel()\n",
    "\n",
    "    [tmp, features] = X.shape\n",
    "    D = np.zeros([tmp, features])\n",
    "\n",
    "    for i in range(features):\n",
    "        N, E = np.histogram(X[:,i], bins=bins)\n",
    "        D[:,i] = np.digitize(X[:,i], E, right=False)\n",
    "\n",
    "    selected_features = []\n",
    "    j_h = 0\n",
    "    hjmi = None\n",
    "    for i in range(0,max_features):\n",
    "        JMI = np.zeros([features], dtype=np.float)\n",
    "        for X_k in range(features):\n",
    "            if X_k in selected_features:\n",
    "                continue\n",
    "            jmi_1 = pymit.I(D[:,X_k], Y, bins=[bins,2])\n",
    "            jmi_2 = 0\n",
    "            for X_j in selected_features:\n",
    "                tmp1 = pymit.I(D[:,X_k], D[:,X_j], bins=[bins,bins])\n",
    "                tmp2 = pymit.I_cond(D[:,X_k], D[:,X_j], Y, bins=[bins,bins,2])\n",
    "                jmi_2 += tmp1 - tmp2\n",
    "            if len(selected_features) == 0:\n",
    "                JMI[X_k] += j_h + jmi_1\n",
    "            else:\n",
    "                JMI[X_k] += j_h + jmi_1 - jmi_2/len(selected_features)\n",
    "        \n",
    "        f = JMI.argmax()\n",
    "        j_h = JMI[f]\n",
    "        if (hjmi == None) or ((j_h - hjmi)/hjmi > 0.03):\n",
    "            r = 0\n",
    "            if hjmi != None:\n",
    "                r = ((j_h - hjmi)/hjmi) \n",
    "\n",
    "            hjmi = j_h\n",
    "            selected_features.append(f)\n",
    "            print(\"{:0>3d} {:>3d} {} - {}\".format(len(selected_features), f, j_h, r))\n",
    "        else:\n",
    "            return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001 1287 0.09097169818788184 - 0\n",
      "002  30 0.16833801526943193 - 0.8504438042012489\n",
      "003 1231 0.26500459087672573 - 0.574240913156633\n",
      "004 1223 0.3499484823957439 - 0.3205374338534843\n",
      "005 662 0.4333308545285113 - 0.23827042072573792\n",
      "006 1222 0.5215574455989416 - 0.20360099020972292\n",
      "007 2679 0.6096639823689883 - 0.16892968840444364\n",
      "008  31 0.696922710775761 - 0.14312593646701738\n",
      "009 1327 0.7832160268791036 - 0.12382049654729657\n",
      "010 1319 0.8677481144479807 - 0.10792946603214157\n",
      "011  38 0.9495022531542637 - 0.09421413581323772\n",
      "012 1119 1.0301281363763743 - 0.0849138408616409\n",
      "013 2743 1.11312347320363 - 0.08056797392138394\n",
      "014 2079 1.1952675876674206 - 0.07379604908283484\n",
      "015 2711 1.280295201079021 - 0.07113688540449156\n",
      "016  39 1.3627846967988322 - 0.06443005929436418\n",
      "017 1326 1.4457222805465713 - 0.06085890452289246\n",
      "018 1230 1.525927000790955 - 0.05547726650104704\n",
      "019 711 1.6050239242185422 - 0.05183532592751025\n",
      "020 678 1.6804525439771483 - 0.04699532425682122\n",
      "021 1221 1.757054059145277 - 0.04558386099189387\n",
      "022  29 1.8328309195945192 - 0.0431272219854772\n",
      "023 1286 1.9083762000272593 - 0.041217812087900145\n",
      "024 1316 1.9812724145237626 - 0.03819803165406383\n",
      "025  37 2.053050823657044 - 0.036228440171633305\n",
      "026 2047 2.125969467234519 - 0.03551721308466533\n",
      "027 1295 2.195803925290596 - 0.032848288337328814\n",
      "028 1318 2.264986429125195 - 0.03150668556412353\n",
      "029 935 2.3334990186754445 - 0.03024856514337317\n",
      "CPU times: user 35min 54s, sys: 110 ms, total: 35min 54s\n",
      "Wall time: 35min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected_features = hjmi_selector(X_train_discrete.copy(), y_train.copy(), bins=10, max_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = X_train_discrete[X_train_discrete.columns[selected_features]]\n",
    "filtered_test = X_test_discrete[X_test_discrete.columns[selected_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fangorn.training import classifiers\n",
    "\n",
    "this_clf = classifiers.random_forest_classifier(train_set= [filtered_train, y_train],\n",
    "                     test_set= [filtered_test, y_test],\n",
    "                     features= filtered_test.columns,\n",
    "                     target= target,\n",
    "                     test_metrics= ['auc'],\n",
    "                     project_name= 'explore_kit'\n",
    "                     ) \n",
    "this_auc = this_clf['calc_metrics']['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7388648314259202"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V4_norm_x_V5_norm_op_sum_10</th>\n",
       "      <th>V7_group_by_V1_disc_and_mean_9</th>\n",
       "      <th>V3_norm_x_V5_norm_op_sum_10</th>\n",
       "      <th>V1_norm_x_V4_norm_op_sum_10</th>\n",
       "      <th>V6_group_by_V8_disc_and_mean_9</th>\n",
       "      <th>V1_norm_x_V4_norm_op_sum_9</th>\n",
       "      <th>V1_norm_x_V9_norm_op_sub_disc_10</th>\n",
       "      <th>V7_group_by_V1_disc_and_mean_10</th>\n",
       "      <th>V5_norm_x_V6_norm_op_sum_10</th>\n",
       "      <th>V4_norm_x_V10_norm_op_sub_10</th>\n",
       "      <th>...</th>\n",
       "      <th>V6_norm_group_by_V8_disc_and_mean_9</th>\n",
       "      <th>V1_norm_x_V4_norm_op_sum_8</th>\n",
       "      <th>V7_group_by_V1_disc_and_mean_8</th>\n",
       "      <th>V4_norm_x_V5_norm_op_sum_9</th>\n",
       "      <th>V4_norm_x_V10_norm_op_sub_7</th>\n",
       "      <th>V7_norm_group_by_V1_disc_and_mean_8</th>\n",
       "      <th>V8_norm_group_by_V2_group_2_norm_and_max_disc_10</th>\n",
       "      <th>V4_norm_x_V5_norm_op_sub_10</th>\n",
       "      <th>V4_norm_x_V10_norm_op_sub_9</th>\n",
       "      <th>V5_x_V9_op_sub_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>559</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>408 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     V4_norm_x_V5_norm_op_sum_10  V7_group_by_V1_disc_and_mean_9  \\\n",
       "271                          2.0                             8.0   \n",
       "318                          2.0                             7.0   \n",
       "552                          1.0                             6.0   \n",
       "579                          1.0                             6.0   \n",
       "196                          3.0                             4.0   \n",
       "..                           ...                             ...   \n",
       "277                          1.0                             4.0   \n",
       "9                            1.0                             3.0   \n",
       "359                          1.0                            10.0   \n",
       "192                          1.0                             4.0   \n",
       "559                          9.0                             2.0   \n",
       "\n",
       "     V3_norm_x_V5_norm_op_sum_10  V1_norm_x_V4_norm_op_sum_10  \\\n",
       "271                          2.0                          1.0   \n",
       "318                          2.0                          4.0   \n",
       "552                          1.0                          4.0   \n",
       "579                          1.0                          4.0   \n",
       "196                          4.0                          6.0   \n",
       "..                           ...                          ...   \n",
       "277                          1.0                          6.0   \n",
       "9                            2.0                          5.0   \n",
       "359                          1.0                          6.0   \n",
       "192                          2.0                          6.0   \n",
       "559                          7.0                         10.0   \n",
       "\n",
       "     V6_group_by_V8_disc_and_mean_9  V1_norm_x_V4_norm_op_sum_9  \\\n",
       "271                             5.0                         1.0   \n",
       "318                             4.0                         4.0   \n",
       "552                             6.0                         4.0   \n",
       "579                             6.0                         3.0   \n",
       "196                             6.0                         5.0   \n",
       "..                              ...                         ...   \n",
       "277                             6.0                         5.0   \n",
       "9                               5.0                         4.0   \n",
       "359                             3.0                         6.0   \n",
       "192                             5.0                         5.0   \n",
       "559                             7.0                         9.0   \n",
       "\n",
       "     V1_norm_x_V9_norm_op_sub_disc_10  V7_group_by_V1_disc_and_mean_10  \\\n",
       "271                               2.0                              9.0   \n",
       "318                               2.0                              8.0   \n",
       "552                               1.0                              6.0   \n",
       "579                               1.0                              6.0   \n",
       "196                               4.0                              4.0   \n",
       "..                                ...                              ...   \n",
       "277                               1.0                              4.0   \n",
       "9                                 2.0                              3.0   \n",
       "359                               1.0                             11.0   \n",
       "192                               2.0                              4.0   \n",
       "559                               7.0                              2.0   \n",
       "\n",
       "     V5_norm_x_V6_norm_op_sum_10  V4_norm_x_V10_norm_op_sub_10  ...  \\\n",
       "271                          3.0                           3.0  ...   \n",
       "318                          2.0                           3.0  ...   \n",
       "552                          1.0                           3.0  ...   \n",
       "579                          1.0                           3.0  ...   \n",
       "196                          3.0                           5.0  ...   \n",
       "..                           ...                           ...  ...   \n",
       "277                          1.0                           4.0  ...   \n",
       "9                            2.0                           3.0  ...   \n",
       "359                          1.0                           2.0  ...   \n",
       "192                          2.0                           3.0  ...   \n",
       "559                          2.0                          11.0  ...   \n",
       "\n",
       "     V6_norm_group_by_V8_disc_and_mean_9  V1_norm_x_V4_norm_op_sum_8  \\\n",
       "271                                  5.0                         1.0   \n",
       "318                                  4.0                         3.0   \n",
       "552                                  6.0                         3.0   \n",
       "579                                  6.0                         3.0   \n",
       "196                                  6.0                         5.0   \n",
       "..                                   ...                         ...   \n",
       "277                                  6.0                         5.0   \n",
       "9                                    5.0                         4.0   \n",
       "359                                  3.0                         5.0   \n",
       "192                                  5.0                         5.0   \n",
       "559                                  7.0                         8.0   \n",
       "\n",
       "     V7_group_by_V1_disc_and_mean_8  V4_norm_x_V5_norm_op_sum_9  \\\n",
       "271                             8.0                         2.0   \n",
       "318                             6.0                         2.0   \n",
       "552                             5.0                         1.0   \n",
       "579                             5.0                         1.0   \n",
       "196                             3.0                         3.0   \n",
       "..                              ...                         ...   \n",
       "277                             3.0                         1.0   \n",
       "9                               3.0                         1.0   \n",
       "359                             9.0                         1.0   \n",
       "192                             3.0                         1.0   \n",
       "559                             2.0                         8.0   \n",
       "\n",
       "     V4_norm_x_V10_norm_op_sub_7  V7_norm_group_by_V1_disc_and_mean_8  \\\n",
       "271                          2.0                                  8.0   \n",
       "318                          2.0                                  6.0   \n",
       "552                          2.0                                  5.0   \n",
       "579                          2.0                                  5.0   \n",
       "196                          3.0                                  3.0   \n",
       "..                           ...                                  ...   \n",
       "277                          3.0                                  3.0   \n",
       "9                            2.0                                  3.0   \n",
       "359                          2.0                                  9.0   \n",
       "192                          2.0                                  3.0   \n",
       "559                          8.0                                  2.0   \n",
       "\n",
       "     V8_norm_group_by_V2_group_2_norm_and_max_disc_10  \\\n",
       "271                                               2.0   \n",
       "318                                               1.0   \n",
       "552                                               1.0   \n",
       "579                                               1.0   \n",
       "196                                               4.0   \n",
       "..                                                ...   \n",
       "277                                               1.0   \n",
       "9                                                 2.0   \n",
       "359                                               1.0   \n",
       "192                                               2.0   \n",
       "559                                               2.0   \n",
       "\n",
       "     V4_norm_x_V5_norm_op_sub_10  V4_norm_x_V10_norm_op_sub_9  \\\n",
       "271                          5.0                          3.0   \n",
       "318                          6.0                          3.0   \n",
       "552                          5.0                          3.0   \n",
       "579                          6.0                          3.0   \n",
       "196                          4.0                          4.0   \n",
       "..                           ...                          ...   \n",
       "277                          6.0                          3.0   \n",
       "9                            5.0                          3.0   \n",
       "359                          6.0                          2.0   \n",
       "192                          5.0                          3.0   \n",
       "559                         11.0                         10.0   \n",
       "\n",
       "     V5_x_V9_op_sub_10  \n",
       "271                3.0  \n",
       "318                1.0  \n",
       "552                1.0  \n",
       "579                1.0  \n",
       "196                4.0  \n",
       "..                 ...  \n",
       "277                1.0  \n",
       "9                  2.0  \n",
       "359                1.0  \n",
       "192                2.0  \n",
       "559                2.0  \n",
       "\n",
       "[408 rows x 29 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
