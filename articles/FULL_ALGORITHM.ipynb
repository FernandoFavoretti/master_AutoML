{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import operator\n",
    "import itertools\n",
    "import scipy.stats as stats\n",
    "import pymit\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "from category_encoders import one_hot, target_encoder\n",
    "created_features_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_values = []\n",
    "# with open('datasets/vehicleNorm.csv') as fp:\n",
    "#     line = fp.readline()\n",
    "#     cnt = 1\n",
    "#     while line:\n",
    "#         if cnt == 1:\n",
    "#             header = line.split(\",\")\n",
    "#             header = [x.replace(\"\\\"\", \"\") for x in header]\n",
    "#             header = [x.replace(\"\\n\", \"\") for x in header]\n",
    "#             print(header)\n",
    "#         else:\n",
    "#             line = line.split(\",\")\n",
    "#             line = [x.split(\" \")[-1] for x in line] \n",
    "#             line = [float(x.replace(\"}\\n\", \"\")) for x in line]\n",
    "#             all_values.append(line)\n",
    "#         line = fp.readline()\n",
    "#         cnt +=1\n",
    "# df = pd.DataFrame(all_values, columns=header)     \n",
    "# df.to_csv('datasets/vehicleNorm.csv', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'ligacoes'\n",
    "target='atendeu'\n",
    "positive_target=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f'datasets/{dataset_name}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_df(df, taret, positive_target):\n",
    "    \n",
    "    def remove_collinear_features(x, threshold=0.8):\n",
    "        '''\n",
    "        Objective:\n",
    "            Remove collinear features in a dataframe with a correlation coefficient\n",
    "            greater than the threshold. Removing collinear features can help a model \n",
    "            to generalize and improves the interpretability of the model.\n",
    "\n",
    "        Inputs: \n",
    "            x: features dataframe\n",
    "            threshold: features with correlations greater than this value are removed\n",
    "\n",
    "        Output: \n",
    "            dataframe that contains only the non-highly-collinear features\n",
    "        '''\n",
    "\n",
    "        # Calculate the correlation matrix\n",
    "        corr_matrix = x.corr()\n",
    "        iters = range(len(corr_matrix.columns) - 1)\n",
    "        drop_cols = []\n",
    "\n",
    "        # Iterate through the correlation matrix and compare correlations\n",
    "        for i in iters:\n",
    "            for j in range(i+1):\n",
    "                item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
    "                col = item.columns\n",
    "                row = item.index\n",
    "                val = abs(item.values)\n",
    "\n",
    "                # If correlation exceeds the threshold\n",
    "                if val >= threshold:\n",
    "                    # Print the correlated features and the correlation value\n",
    "                    print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
    "                    drop_cols.append(col.values[0])\n",
    "\n",
    "        # Drop one of each pair of correlated columns\n",
    "        drops = set(drop_cols)\n",
    "        print(f\"Dropped cols: {drops} by corr\")\n",
    "        x = x.drop(columns=drops)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def clean_target(df, target, positive_target):\n",
    "        df.loc[df[target] != positive_target, target] = 0\n",
    "        df.loc[df[target] == positive_target, target] = 1\n",
    "        y_all = df[[target]].astype(int)\n",
    "        X_all = df.drop([target], axis=1)\n",
    "        return X_all, y_all\n",
    "    \n",
    "    def nulls(df, th_del=0.8, th_mean=0.2):\n",
    "        null = df.isnull().sum().sort_values()[::-1]\n",
    "        null = null[null > 0] / len(df)\n",
    "        null = pd.concat([df[null.index].dtypes, null],axis=1)\n",
    "        null.columns=['dtype', 'ratio_null']\n",
    "        null.index.name = 'feature'\n",
    "        if null.empty:\n",
    "            return df\n",
    "        \n",
    "        for idx, row in null.iterrows():\n",
    "            if row['ratio_null'] > th_del:\n",
    "                df = df.drop(idx, axis=1)\n",
    "            \n",
    "            elif row['ratio_null'] < th_mean and  row['dtype'] != 'object':\n",
    "                df[idx] = df[idx].fillna(df[idx].mean())\n",
    "            \n",
    "            else:\n",
    "                if row['dtype'] == 'object':\n",
    "                    df[idx] = df[idx].fillna('my_nan_value')\n",
    "                else:\n",
    "                    df[idx] = df[idx].fillna(df[idx].min()*1000)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def clean_groups(df, target, max_group_size=0):\n",
    "        is_object = ['object']\n",
    "        object_features = list(df.select_dtypes(include=is_object).columns)\n",
    "        if target in object_features:\n",
    "            object_features.remove(target)\n",
    "        for col in object_features:\n",
    "            try:\n",
    "                df[col] = df[col].astype(float)\n",
    "            except:\n",
    "                len_unique = len(df[col].unique())\n",
    "                if len_unique < max_group_size:\n",
    "                    df = df.rename({col: f\"{col}_group\"},axis=1)\n",
    "                else:\n",
    "                    df = df.rename({col: f\"{col}_mean_encode\"},axis=1)\n",
    "        return df\n",
    "                    \n",
    "        \n",
    "    \n",
    "    df = nulls(df.copy())\n",
    "    df = clean_groups(df.copy(), target)\n",
    "    df = remove_collinear_features(df.copy(), threshold=0.9)\n",
    "    X_all, y_all = clean_target(df.copy(), target, positive_target)\n",
    "    return X_all, y_all\n",
    "\n",
    "def prepare_bases_to_modeling(X_train_ori, X_test_ori):\n",
    "    \n",
    "    one_hot_cols = [x for x in X_train_ori.columns if 'group' in x]\n",
    "    mean_encoding_cols = [x for x in X_train_ori.columns if 'encode' in x]\n",
    "\n",
    "    if len(one_hot_cols) > 0:\n",
    "        enc = one_hot.OneHotEncoder(cols=one_hot_cols, drop_invariant=True)\n",
    "        X_train_ori = enc.fit_transform(X_train_ori.copy())\n",
    "        X_test_ori = enc.transform(X_test_ori.copy())\n",
    "#         X_train = X_train.drop(ORIGINAL_FEATURES, axis=1)\n",
    "#         X_train = pd.concat([X_train, X_train_ori], axis=1)\n",
    "#         X_test = pd.concat([X_test, X_test_ori], axis=1)\n",
    "\n",
    "    if len(mean_encoding_cols) > 0:\n",
    "        enc = target_encoder.TargetEncoder(cols=mean_encoding_cols, drop_invariant=True)\n",
    "        X_train_ori = training_numeric_dataset = enc.fit_transform(X_train_ori.copy(), y_train)\n",
    "        X_test_ori = testing_numeric_dataset = enc.transform(X_test_ori.copy())\n",
    "#         X_train = X_train.drop(ORIGINAL_FEATURES, axis=1)\n",
    "#         X_train = pd.concat([X_train, X_train_ori], axis=1)\n",
    "#         X_test = pd.concat([X_test, X_test_ori], axis=1)\n",
    "    \n",
    "    return X_train_ori, X_test_ori\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = clean_df(df.copy(), target, positive_target)\n",
    "X_train_ori, X_test_ori, y_train, y_test = train_test_split(X_all, y_all, test_size = 0.3, random_state = 0)\n",
    "X_train_ori, X_test_ori = prepare_bases_to_modeling(X_train_ori.copy(), X_test_ori.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_discretize(X_train, X_test, gran=7, retry=True):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 10\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    global created_features_dict\n",
    "\n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    # cehca se nao tem _disc somente nos ultimos caracteres\n",
    "    numeric_features = [feat for feat in numeric_features.columns if 'disc' not in feat[-4:]]\n",
    "    X_train_numeric_np = X_train[numeric_features].T.to_numpy()\n",
    "    X_test_numeric_np = X_test[numeric_features].T.to_numpy()\n",
    "    # cacheando posicoes das features\n",
    "    dict_feature_order = {}\n",
    "    for feat in numeric_features:\n",
    "        dict_feature_order[feat] = X_train.columns.get_loc(feat)\n",
    "    shape_X_train = X_train.shape[0]\n",
    "    shape_X_test = X_test.shape[0]\n",
    "    feat_count = 0\n",
    "    with tqdm(total=len(numeric_features)) as pbar:\n",
    "        for feat in numeric_features:\n",
    "            feat_index = dict_feature_order[feat]\n",
    "            this_gran = gran\n",
    "            success = False\n",
    "            while not success:\n",
    "                try:\n",
    "                    D_train = np.zeros([shape_X_train, 1])\n",
    "                    D_test = np.zeros([shape_X_test, 1])\n",
    "                    # calc numpy histogram and apply to features\n",
    "                    hist, bin_edges = np.histogram(X_train_numeric_np[feat_index], bins=this_gran)\n",
    "                    D_train[:, 0] = np.digitize(X_train_numeric_np[feat_index], bin_edges, right=False)\n",
    "                    D_test[:, 0] = np.digitize(X_test_numeric_np[feat_index], bin_edges, right=False)\n",
    "\n",
    "                    # apply back to pandas\n",
    "                    X_train[f\"{feat}_disc\"] = D_train\n",
    "                    X_test[f\"{feat}_disc\"] = D_test\n",
    "\n",
    "                    success = True\n",
    "                except:\n",
    "#                     traceback.print_exc()\n",
    "                    if retry:\n",
    "#                         print(f\"Not possible to correct work on cut {feat} > {this_gran}\")\n",
    "                        this_gran = this_gran - 1\n",
    "                    else:\n",
    "                        this_gran = 1\n",
    "                        \n",
    "                    if this_gran <= 1:\n",
    "                        success = True\n",
    "\n",
    "                if success and this_gran > 1:\n",
    "                    #upoad global dict with feature info\n",
    "                    created_features_dict[f\"{feat}_disc\"] = {\n",
    "                        \"num_of_source_features\": 1,\n",
    "                        \"source_feature_name\": [feat],\n",
    "                        \"source_feature_type\": ['numeric'],\n",
    "                        \"target_feature_type\": ['discrete'],\n",
    "                        \"operator\": \"discretizer\"\n",
    "                    }\n",
    "\n",
    "            feat_count += 1\n",
    "            pbar.update(1)\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "\n",
    "def min_max_scaler(X_train, X_test):\n",
    "    global created_features_dict\n",
    "    \n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = list(X_train.select_dtypes(include=is_numeric).columns)\n",
    "    numeric_features = [x for x in numeric_features if 'disc' not in x[-4:]]\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train[numeric_features])\n",
    "    norm_feats = [f\"{x}_norm\" for x in ORIGINAL_FEATURES if 'disc' not in x[-4:] and x in numeric_features]\n",
    "    \n",
    "    for feat in [x for x in ORIGINAL_FEATURES if 'disc' not in x[-4:]]:\n",
    "        #upoad global dict with feature info\n",
    "        created_features_dict[f\"{feat}_norm\"] = {\n",
    "            \"num_of_source_features\": 1,\n",
    "            \"source_feature_name\": [feat],\n",
    "            \"source_feature_type\": ['numeric'],\n",
    "            \"target_feature_type\": ['numeric'],\n",
    "            \"operator\": \"normalizer\"\n",
    "        }\n",
    "    X_train = X_train.reindex(columns=X_train.columns.tolist() + norm_feats)\n",
    "    X_test = X_test.reindex(columns=X_test.columns.tolist() + norm_feats)\n",
    "    X_train.loc[:, norm_feats] = scaler.transform(X_train[numeric_features])\n",
    "    X_test.loc[:, norm_feats] = scaler.transform(X_test[numeric_features])\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def binary_operators(df):\n",
    "    global created_features_dict\n",
    "    \n",
    "    # calc all pair columns\n",
    "    all_columns = list(df)\n",
    "    all_columns = [x for x in all_columns if \"disc\" not in x and \"group\" not in x and \"encode\" not in x]\n",
    "    pairwise_cols = list(itertools.combinations(all_columns, 2))\n",
    "    tmp_dfs = []\n",
    "    with tqdm(total=len(pairwise_cols)) as pbar:\n",
    "        for pair in pairwise_cols:\n",
    "            tmp_df = df[[pair[0], pair[1]]].copy()\n",
    "            \n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_sum\"] = tmp_df[pair[0]] + tmp_df[pair[1]]\n",
    "\n",
    "            #upoad global dict with feature info\n",
    "            type_pair1 = 'disc' if 'disc' in pair[0] else 'numeric'\n",
    "            type_pair2 = 'disc' if 'disc' in pair[1] else 'numeric'\n",
    "\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_sum\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_sum\"\n",
    "            }        \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_sub\"] = tmp_df[pair[0]] - tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_sub\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_sub\"\n",
    "            }             \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_mul\"] = tmp_df[pair[0]] * tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_mul\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_mul\"\n",
    "            }             \n",
    "\n",
    "            tmp_df[f\"{pair[0]}_x_{pair[1]}_op_div\"] = tmp_df[pair[0]] / tmp_df[pair[1]]\n",
    "            created_features_dict[f\"{pair[0]}_x_{pair[1]}_op_div\"] = {\n",
    "                \"num_of_source_features\": 2,\n",
    "                \"source_feature_name\": [pair[0], pair[1]],\n",
    "                \"source_feature_type\": [type_pair1, type_pair2],\n",
    "                \"target_feature_type\": ['numeric'],\n",
    "                \"operator\": \"binary_div\"\n",
    "            }   \n",
    "            tmp_df = tmp_df.replace([np.inf, -np.inf], np.nan)\n",
    "            tmp_dfs.append(tmp_df)\n",
    "            pbar.update(1)\n",
    "        tmp_df_concat = pd.concat(tmp_dfs, axis=1)\n",
    "        df = pd.concat([df, tmp_df_concat], axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def high_order_operators(df):\n",
    "    \n",
    "    def _update_dict(group_col, columns, op):\n",
    "        global created_features_dict\n",
    "        \n",
    "        for feat in columns:\n",
    "            created_features_dict[f'{feat}_group_by_{col}_and_{op}'] = {\n",
    "            \"num_of_source_features\": 2,\n",
    "            \"source_feature_name\": [group_col, feat],\n",
    "            \"source_feature_type\": ['discrete', 'numeric'],\n",
    "            \"target_feature_type\": ['numeric'],\n",
    "            \"operator\": f\"group_{op}\"\n",
    "            }\n",
    "            \n",
    "    group_columns = [col for col in df.columns if \"disc\" in col or \"group\" in col]\n",
    "    to_group_columns = [col for col in df.columns if \"disc\" not in col and \"group\" not in col]\n",
    "    all_dfs = pd.DataFrame()\n",
    "    for col in group_columns:\n",
    "        print(f\"Grouping {col}\")\n",
    "        \n",
    "        df_avg = df[to_group_columns+[col]].groupby(col).transform('mean').add_suffix(f'_group_by_{col}_and_mean')\n",
    "        _update_dict(col, to_group_columns, 'mean')\n",
    "        df_min = df[to_group_columns+[col]].groupby(col).transform('min').add_suffix(f'_group_by_{col}_and_min')\n",
    "        _update_dict(col, to_group_columns, 'min')\n",
    "        df_max = df[to_group_columns+[col]].groupby(col).transform('max').add_suffix(f'_group_by_{col}_and_max')\n",
    "        _update_dict(col, to_group_columns, 'max')\n",
    "        \n",
    "        all_dfs = pd.concat([all_dfs, df_avg, df_min, df_max], axis=1,  sort=False)\n",
    "    all_dfs = pd.concat([df,all_dfs], axis=1)\n",
    "    return all_dfs\n",
    "\n",
    "def _entropy_based_measures(X_train, y_train, target):\n",
    "\n",
    "    df_mutual_info = pd.DataFrame()\n",
    "    fail_count = 0\n",
    "    with tqdm(total=len( X_train.columns)) as pbar:\n",
    "        for feat in X_train.columns:\n",
    "            try:\n",
    "                df_mutual_info[feat] = [pymit.I(X_train[feat].values, y_train[target].values , bins=[10,2])]\n",
    "                pbar.update(1)\n",
    "            except:\n",
    "                fail_count += 1\n",
    "    print(fail_count)\n",
    "    return df_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 39/39 [00:00<00:00, 172.02it/s]\n"
     ]
    }
   ],
   "source": [
    "ORIGINAL_FEATURES = X_train_ori.columns\n",
    "\n",
    "# discretize\n",
    "X_train, X_test = numpy_discretize(X_train_ori.copy(), X_test_ori.copy(), gran=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X_train, X_test = min_max_scaler(X_train.copy(), X_test.copy())\n",
    "\n",
    "step1_train = X_train.copy()\n",
    "step1_test = X_test.copy()\n",
    "step1_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grouping xp_transacoes_movto_mov_bolsa_15_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_bolsa_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_fundo_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_15_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_stddev_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_cv_disc\n",
      "Grouping xp_visitas_naveg_visits_15_sum_disc\n",
      "Grouping xp_visitas_naveg_visits_15_avg_disc\n",
      "Grouping xp_visitas_naveg_visits_15_stddev_disc\n",
      "Grouping xp_visitas_naveg_visits_15_cv_disc\n",
      "Grouping xp_visitas_naveg_visits_30_avg_disc\n",
      "Grouping xp_visitas_naveg_visits_30_stddev_disc\n",
      "Grouping xp_visitas_naveg_visits_30_cv_disc\n",
      "Grouping xp_salesforce_cadastral_atendimento_b2c_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_atendimento_b2c_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_bens_imoveis_15_disc\n",
      "Grouping xp_salesforce_cadastral_bens_moveis_15_disc\n",
      "Grouping xp_salesforce_cadastral_dias_sem_relacionamento_15_disc\n",
      "Grouping xp_salesforce_cadastral_estado_residencia_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_estado_residencia_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_idade_ativacao_15_disc\n",
      "Grouping xp_salesforce_cadastral_idade_cliente_15_disc\n",
      "Grouping xp_salesforce_cadastral_net_15_disc\n",
      "Grouping xp_salesforce_cadastral_pld_15_disc\n",
      "Grouping xp_salesforce_cadastral_renda_15_disc\n",
      "Grouping xp_salesforce_cadastral_saldo_conta_15_disc\n",
      "Grouping xp_salesforce_cadastral_segmento_cliente_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_segmento_cliente_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_suitability_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_suitability_30_mean_encode_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_sum_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_min_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_max_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_count_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_stddev_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_cv_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_30_min_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_30_stddev_disc\n",
      "Grouping xp_transacoes_movto_mov_bolsa_15_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_bolsa_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_fundo_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_15_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_sum_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_stddev_disc\n",
      "Grouping xp_transacoes_movto_mov_teds_30_cv_disc\n",
      "Grouping xp_visitas_naveg_visits_15_sum_disc\n",
      "Grouping xp_visitas_naveg_visits_15_avg_disc\n",
      "Grouping xp_visitas_naveg_visits_15_stddev_disc\n",
      "Grouping xp_visitas_naveg_visits_15_cv_disc\n",
      "Grouping xp_visitas_naveg_visits_30_avg_disc\n",
      "Grouping xp_visitas_naveg_visits_30_stddev_disc\n",
      "Grouping xp_visitas_naveg_visits_30_cv_disc\n",
      "Grouping xp_salesforce_cadastral_atendimento_b2c_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_atendimento_b2c_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_bens_imoveis_15_disc\n",
      "Grouping xp_salesforce_cadastral_bens_moveis_15_disc\n",
      "Grouping xp_salesforce_cadastral_dias_sem_relacionamento_15_disc\n",
      "Grouping xp_salesforce_cadastral_estado_residencia_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_estado_residencia_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_idade_ativacao_15_disc\n",
      "Grouping xp_salesforce_cadastral_idade_cliente_15_disc\n",
      "Grouping xp_salesforce_cadastral_net_15_disc\n",
      "Grouping xp_salesforce_cadastral_pld_15_disc\n",
      "Grouping xp_salesforce_cadastral_renda_15_disc\n",
      "Grouping xp_salesforce_cadastral_saldo_conta_15_disc\n",
      "Grouping xp_salesforce_cadastral_segmento_cliente_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_segmento_cliente_30_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_suitability_15_mean_encode_disc\n",
      "Grouping xp_salesforce_cadastral_suitability_30_mean_encode_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_sum_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_min_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_max_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_count_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_stddev_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_15_cv_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_30_min_disc\n",
      "Grouping xp_consolidate_position_available_money_val_disponivel_30_stddev_disc\n"
     ]
    }
   ],
   "source": [
    "X_train = high_order_operators(X_train[step1_features].copy())\n",
    "X_test = high_order_operators(X_test[step1_features].copy())\n",
    "\n",
    "step2_train = X_train.copy()\n",
    "step2_test = X_test.copy()\n",
    "step2_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1891/1891 [00:47<00:00, 40.11it/s]\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3417, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-16-b5d1140005be>\", line 2, in <module>\n",
      "    X_train = binary_operators(X_train[step1_features].copy())\n",
      "  File \"<ipython-input-12-9e00e6c00f2a>\", line 157, in binary_operators\n",
      "    df = pd.concat([df, tmp_df_concat], axis=1)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/reshape/concat.py\", line 281, in concat\n",
      "    sort=sort,\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/reshape/concat.py\", line 360, in __init__\n",
      "    obj._consolidate(inplace=True)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\", line 5365, in _consolidate\n",
      "    self._consolidate_inplace()\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\", line 5347, in _consolidate_inplace\n",
      "    self._protect_consolidate(f)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\", line 5336, in _protect_consolidate\n",
      "    result = f()\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\", line 5345, in f\n",
      "    self._data = self._data.consolidate()\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 940, in consolidate\n",
      "    bm._consolidate_inplace()\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 945, in _consolidate_inplace\n",
      "    self.blocks = tuple(_consolidate(self.blocks))\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\", line 1887, in _consolidate\n",
      "    list(group_blocks), dtype=dtype, _can_consolidate=_can_consolidate\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/blocks.py\", line 3104, in _merge_blocks\n",
      "    new_values = new_values[argsort]\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2044, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 1169, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/inspect.py\", line 742, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/posixpath.py\", line 388, in realpath\n",
      "    path, ok = _joinrealpath(filename[:0], filename, {})\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/posixpath.py\", line 422, in _joinrealpath\n",
      "    if not islink(newpath):\n",
      "  File \"/home/fernando.favoretti/anaconda3/envs/basic/lib/python3.7/posixpath.py\", line 171, in islink\n",
      "    st = os.lstat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b5d1140005be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# binary operators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_operators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep1_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_operators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep1_features\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-9e00e6c00f2a>\u001b[0m in \u001b[0;36mbinary_operators\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mtmp_df_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp_dfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp_df_concat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m     )\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m             \u001b[0mndims\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(self, inplace)\u001b[0m\n\u001b[1;32m   5364\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5365\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5366\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5347\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5335\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5336\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5337\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5344\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5345\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1886\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1887\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1888\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   3103\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3104\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3105\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2043\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2044\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2046\u001b[0m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0;32m-> 2047\u001b[0;31m                                             value, tb, tb_offset=tb_offset)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_showtraceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m         return FormattedTB.structured_traceback(\n\u001b[0;32m-> 1436\u001b[0;31m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0m\u001b[1;32m   1437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1334\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             return VerboseTB.structured_traceback(\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             )\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'Minimal'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0;32m-> 1193\u001b[0;31m                                                                tb_offset)\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mColors\u001b[0m  \u001b[0;31m# just a shorthand + quicker name lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1150\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "# binary operators\n",
    "X_train = binary_operators(X_train[step1_features].copy())\n",
    "X_test = binary_operators(X_test[step1_features].copy())\n",
    "\n",
    "step3_train = X_train.copy()\n",
    "step3_test = X_test.copy()\n",
    "step3_features = X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train\n",
    "del X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([step1_train, step2_train, step3_train], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(135850, 22056)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'step1_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-e7226cfcd647>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstep1_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep2_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep3_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'step1_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test = pd.concat([step1_test, step2_test, step3_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.loc[:,~X_train.columns.duplicated()]\n",
    "X_test = X_test.loc[:,~X_test.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# discretize\n",
    "X_train, X_test = numpy_discretize(X_train.copy(), X_test.copy(), gran=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_filter = _entropy_based_measures(X_train, y_train, target)\n",
    "features_to_keep = np.round(entropy_filter.T.sort_values(by=0), 3)\n",
    "to_keep = list(features_to_keep.loc[features_to_keep[0]>0].index)\n",
    "len(to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_based_meta_features(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Applied in the original set!\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def _general_information(X):\n",
    "        dataset_info_df= pd.DataFrame()\n",
    "\n",
    "        numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "        num_numeric_attr = X.select_dtypes(include=numerics).shape[1]\n",
    "        num_duscrete_attr = X.shape[1] - num_numeric_attr\n",
    "            \n",
    "        \n",
    "        dataset_info_df['num_instances'] = [X.shape[0]]\n",
    "        dataset_info_df['num_features'] = X.shape[1]\n",
    "        \n",
    "        dataset_info_df['num_numeric_attr'] = num_numeric_attr\n",
    "        dataset_info_df['num_discrete_attr'] = num_duscrete_attr\n",
    "        dataset_info_df['ratio_numeric_attr'] = num_numeric_attr/ (num_numeric_attr+num_duscrete_attr)\n",
    "        dataset_info_df['ratio_discrete_attr'] = num_duscrete_attr/ (num_numeric_attr+num_duscrete_attr)\n",
    "        \n",
    "        return dataset_info_df\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    def _initial_evaluation(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        from sklearn import metrics\n",
    "        \n",
    "        def acc(y_true, y_pred):\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=0.5 else 1, y_pred))\n",
    "            return accuracy_score(y_true, y_pred)\n",
    "\n",
    "        def f1(y_true, y_pred, th):\n",
    "            from sklearn.metrics import f1_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return f1_score(y_true, y_pred)\n",
    "\n",
    "        def precision(y_true, y_pred, th):\n",
    "            from sklearn.metrics import precision_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return precision_score(y_true, y_pred, average='macro') \n",
    "\n",
    "        def recall(y_true, y_pred, th):\n",
    "            from sklearn.metrics import recall_score\n",
    "            y_pred = list(map(lambda k: 0 if k<=th else 1, y_pred))\n",
    "            return recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        def auc(y_true, y_pred):\n",
    "            from sklearn.metrics import roc_auc_score\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "        df_initial_evaluation = pd.DataFrame()\n",
    "        clf = RandomForestClassifier()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "        for th in [0.4, 0.45, 0.5, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "            df_initial_evaluation[f'f1_{th}'] = [f1(y_test, y_pred, th)]\n",
    "            df_initial_evaluation[f'precision_{th}'] = precision(y_test, y_pred, th)\n",
    "            df_initial_evaluation[f'recall_{th}'] = recall(y_test, y_pred, th)\n",
    "\n",
    "        df_initial_evaluation['auc'] = auc(y_test, y_pred)\n",
    "        \n",
    "        df_initial_evaluation['avg_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_f1'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'f1' in col]].min(axis=1)\n",
    "    \n",
    "        df_initial_evaluation['avg_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_precision'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'precision' in col]].min(axis=1)\n",
    "    \n",
    "        df_initial_evaluation['avg_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].mean(axis=1)\n",
    "        df_initial_evaluation['std_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].std(axis=1)\n",
    "        df_initial_evaluation['max_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].max(axis=1)\n",
    "        df_initial_evaluation['min_recall'] = df_initial_evaluation[[col for col in df_initial_evaluation.columns if 'recall' in col]].min(axis=1)\n",
    "    \n",
    "        return df_initial_evaluation\n",
    "    \n",
    "    \n",
    "    def _entropy_based_measures(X_train, y_train):\n",
    "\n",
    "        df_mutual_info = pd.DataFrame()\n",
    "\n",
    "        for feat in X_train.columns:\n",
    "            df_mutual_info[feat] = [pymit.I(X_train[feat].values, y_train[target].values , bins=[10,2])]\n",
    "        \n",
    "        df_mutual_info['avg_mi'] = df_mutual_info.mean(axis=1)\n",
    "        df_mutual_info['std_mi'] = df_mutual_info.std(axis=1)\n",
    "        df_mutual_info['min_mi'] = df_mutual_info.min(axis=1)\n",
    "        df_mutual_info['max_mi'] = df_mutual_info.max(axis=1)\n",
    "        return df_mutual_info[['avg_mi', 'std_mi', 'min_mi', 'max_mi']]\n",
    "    \n",
    "    \n",
    "    def _feature_diversity(X_train):\n",
    "        \n",
    "        df_feature_diversity = pd.DataFrame()\n",
    "        \n",
    "        disc_columns = [col for col in X_train if 'disc' in col and 'group' not in col and 'encode' not in col]\n",
    "        numeric_columns = [col for col in X_train if 'disc' not in col and 'group' not in col and 'encode' not in col]\n",
    "        \n",
    "        numeric_pairs = list(itertools.combinations(numeric_columns, 2))\n",
    "        all_t = []\n",
    "        for pair in numeric_pairs:\n",
    "            t_pair = stats.ttest_rel(X_train[pair[0]].values, X_train[pair[1]].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "        \n",
    "        all_chi = []\n",
    "        disc_pairs = list(itertools.combinations(disc_columns, 2))\n",
    "        for pair in disc_pairs:\n",
    "            contingency = pd.crosstab(X_train[pair[0]].values, X_train[pair[1]].values) \n",
    "            chi, _, _, _ = stats.chi2_contingency(contingency) \n",
    "            all_chi.append(chi)\n",
    "        \n",
    "        if len(all_t) == 0:\n",
    "            df_feature_diversity['avg_t'] = [-99]\n",
    "            df_feature_diversity['std_t'] = -99\n",
    "            df_feature_diversity['max_t'] = -99\n",
    "            df_feature_diversity['min_t'] = -99\n",
    "        else:\n",
    "            df_feature_diversity['avg_t'] = [np.mean(all_t)]\n",
    "            df_feature_diversity['std_t'] = np.std(all_t)\n",
    "            df_feature_diversity['max_t'] = np.max(all_t)\n",
    "            df_feature_diversity['min_t'] = np.min(all_t)\n",
    "            \n",
    "        if len(all_chi) == 0:\n",
    "            df_feature_diversity['avg_chi'] = -99\n",
    "            df_feature_diversity['std_chi'] = -99\n",
    "            df_feature_diversity['max_chi'] = -99\n",
    "            df_feature_diversity['min_chi'] = -99\n",
    "        else:        \n",
    "            df_feature_diversity['avg_chi'] = np.mean(all_chi)\n",
    "            df_feature_diversity['std_chi'] = np.std(all_chi)\n",
    "            df_feature_diversity['max_chi'] = np.max(all_chi)\n",
    "            df_feature_diversity['min_chi'] = np.min(all_chi)\n",
    "\n",
    "        return df_feature_diversity\n",
    "    \n",
    "    dataset_info_df = _general_information(X_train.copy())\n",
    "    dataset_initial_eval = _initial_evaluation(X_train.copy(), X_test.copy(), y_train.copy(), y_test.copy())\n",
    "    dataset_entropy_info = _entropy_based_measures(X_train.copy(), y_train.copy())\n",
    "    dataset_feature_diversity = _feature_diversity(X_train.copy())\n",
    "\n",
    "    \n",
    "    df = pd.concat([dataset_info_df, dataset_initial_eval, dataset_entropy_info, dataset_feature_diversity], axis=1)\n",
    "    return df\n",
    "        \n",
    "        \n",
    "def candidate_mi_and_stattest(operator_feat, X_train, y_train, X_train_ori, target):\n",
    "    # passo 1 da parte de features candidatas\n",
    "    tests_df = pd.DataFrame()\n",
    "    \n",
    "    this_feat = created_features_dict[operator_feat]\n",
    "    original_features = X_train_ori.columns\n",
    "\n",
    "    all_t = []\n",
    "    for original_feat in original_features:\n",
    "        tmp_df_stat_tests = pd.DataFrame()\n",
    "        tmp_df_stat_tests['feature_name'] = [operator_feat]\n",
    "        \n",
    "        # nao faz o teste na feature que deu origem a nova\n",
    "        if original_feat not in this_feat['source_feature_name']:\n",
    "            t_pair = stats.ttest_rel(X_train[operator_feat].values, X_train_ori[original_feat].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "    \n",
    "    try:\n",
    "        mutual_info = pymit.I(X_train[operator_feat].values, y_train[target].values , bins=[10,2])\n",
    "    except:\n",
    "        print(operator_feat)\n",
    "              \n",
    "    \n",
    "    if len(all_t) == 0:\n",
    "        tmp_df_stat_tests[f'{original_feat}_avg_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_std_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_max_t'] = -99\n",
    "        tmp_df_stat_tests[f'{original_feat}_min_t'] = -99\n",
    "    else:\n",
    "        tmp_df_stat_tests[f'{original_feat}_avg_t'] = np.mean(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_std_t'] = np.std(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_max_t'] = np.max(all_t)\n",
    "        tmp_df_stat_tests[f'{original_feat}_min_t'] = np.min(all_t)\n",
    "    \n",
    "    tmp_df_stat_tests[f'feat_mutual_info'] = mutual_info\n",
    "      \n",
    "    return tmp_df_stat_tests  \n",
    "\n",
    "\n",
    "def generic_meta_features(operator_feature, X_train):\n",
    "    # passo 2 das features candidatas\n",
    "    # https://github.com/giladkatz/ExploreKit/blob/master/src/main/java/explorekit/Evaluation/MLFeatureExtraction/OperatorAssignmentBasedAttributes.java\n",
    "    op_dict = created_features_dict[operator_feature]\n",
    "    df_generic_meta_feats = pd.DataFrame()\n",
    "    df_generic_meta_feats['feature_name'] = [operator_feature]\n",
    "    df_generic_meta_feats['num_sources'] = op_dict['num_of_source_features']\n",
    "    df_generic_meta_feats['num_numeric_sources'] = len([x for x in op_dict['source_feature_type'] if 'numeric' in x])\n",
    "    df_generic_meta_feats['num_discrete_sources'] = len([x for x in op_dict['source_feature_type'] if 'discrete' in x])\n",
    "    df_generic_meta_feats['discretizer_in_use'] = True if op_dict['operator']=='discretizer' else False\n",
    "    df_generic_meta_feats['normalizer_in_use'] = True if op_dict['operator']=='normalizer' else False\n",
    "    df_generic_meta_feats['group_in_use'] = True if 'group' in op_dict['operator'] else False\n",
    "    df_generic_meta_feats['binary_in_use'] = True if 'binary' in op_dict['operator'] else False\n",
    "    \n",
    "    # discrete sources\n",
    "    indices_discrete = [i for i, x in enumerate(op_dict['source_feature_type']) if x == \"discrete\"]\n",
    "    if len(indices_discrete) >= 1:\n",
    "        discrete_columns = [op_dict['source_feature_name'][i] for i in indices_discrete]\n",
    "        X_train_numpy = X_train[discrete_columns].astype(float).to_numpy()\n",
    "        df_generic_meta_feats['max_discrete_source_value'] = X_train_numpy.max()\n",
    "        df_generic_meta_feats['min_discrete_source_value'] = X_train_numpy.min()\n",
    "        df_generic_meta_feats['avg_discrete_source_value'] = X_train_numpy.mean()\n",
    "        df_generic_meta_feats['std_discrete_source_value'] = X_train_numpy.std()\n",
    "        \n",
    "        all_chi = []\n",
    "        for discrete_feat in discrete_columns:\n",
    "            # transform target feature in discrete\n",
    "            if 'disc' not in operator_feature:\n",
    "                this_feat_discrete, _ = numpy_discretize(X_train[[operator_feature]].copy(),\n",
    "                                                      X_train[[operator_feature]].copy(), gran=10)\n",
    "                contingency = pd.crosstab(X_train[discrete_feat].values, this_feat_discrete[f\"{operator_feature}_disc\"].values) \n",
    "            else:\n",
    "                this_feat_discrete = X_train[[operator_feature]]\n",
    "                contingency = pd.crosstab(X_train[discrete_feat].values, this_feat_discrete[f\"{operator_feature}\"].values) \n",
    "            \n",
    "            chi, _, _, _ = stats.chi2_contingency(contingency) \n",
    "            all_chi.append(chi)\n",
    "        \n",
    "        df_generic_meta_feats['max_chi_source_opattr_value'] = np.max(chi)\n",
    "        df_generic_meta_feats['min_chi_source_opattr_value'] = np.min(chi)\n",
    "        df_generic_meta_feats['avg_chi_source_opattr_value'] = np.mean(chi)\n",
    "        df_generic_meta_feats['std_chi_source_opattr_value'] = np.std(chi)\n",
    "            \n",
    "        \n",
    "    else:\n",
    "        df_generic_meta_feats['max_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['min_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['avg_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['std_discrete_source_value'] = 0\n",
    "        df_generic_meta_feats['max_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['min_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['avg_chi_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['std_chi_source_opattr_value'] = 0\n",
    "        \n",
    "    # numeric sources\n",
    "    indices_numeric = [i for i, x in enumerate(op_dict['source_feature_type']) if x == \"numeric\"]\n",
    "    if len(indices_numeric) >= 1:\n",
    "        numeric_columns = [ op_dict['source_feature_name'][i] for i in indices_numeric]\n",
    "        try:\n",
    "            X_train_numpy = X_train[numeric_columns].to_numpy()\n",
    "            df_generic_meta_feats['max_numeric_source_value'] = np.max(X_train_numpy)\n",
    "            df_generic_meta_feats['min_numeric_source_value'] = np.min(X_train_numpy)\n",
    "            df_generic_meta_feats['avg_numeric_source_value'] = np.mean(X_train_numpy)\n",
    "            df_generic_meta_feats['std_numeric_source_value'] = np.std(X_train_numpy)\n",
    "        except:\n",
    "            print(X_train_numpy)\n",
    "            print(numeric_columns)\n",
    "            print(operator_feature)\n",
    "        all_t = []\n",
    "        for src_feat in numeric_columns:\n",
    "            t_pair = stats.ttest_rel(X_train[operator_feature].values,\n",
    "                                     X_train[src_feat].values)[0]\n",
    "            all_t.append(t_pair)\n",
    "            \n",
    "        df_generic_meta_feats['max_ttest_source_opattr_value'] = np.max(all_t)\n",
    "        df_generic_meta_feats['min_ttest_source_opattr_value'] = np.min(all_t)\n",
    "        df_generic_meta_feats['avg_ttest_source_opattr_value'] = np.mean(all_t)\n",
    "        df_generic_meta_feats['std_ttest_source_opattr_value'] = np.std(all_t)\n",
    "           \n",
    "    else:\n",
    "        df_generic_meta_feats['max_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['min_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['avg_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['std_numeric_source_value'] = 0\n",
    "        df_generic_meta_feats['max_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['min_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['avg_ttest_source_opattr_value'] = 0\n",
    "        df_generic_meta_feats['std_ttest_source_opattr_value'] = 0\n",
    "\n",
    "    return df_generic_meta_feats\n",
    "        \n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_based_meta_features = dataset_based_meta_features(X_train_ori.copy(), X_test_ori.copy(), y_train.copy(), y_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.fillna(X_train.mean())\n",
    "X_test = X_test.fillna(X_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitered_keep_dict = {k: v for k, v in created_features_dict.items() if k in to_keep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "t_test_statistic_candidate_df = pd.DataFrame()\n",
    "general_meta_feature_candidates = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=len(to_keep)) as pbar:\n",
    "    for k, v in fitered_keep_dict.items():\n",
    "        all_features = list([k] + v['source_feature_name'])\n",
    "        tmp_t_test_statistic_candidate_df = candidate_mi_and_stattest(k, X_train[all_features].copy(), y_train.copy(), X_train_ori.copy(), target)\n",
    "        tmp_general_meta_feature_candidates = generic_meta_features(k, X_train[all_features].copy())\n",
    "\n",
    "        t_test_statistic_candidate_df = t_test_statistic_candidate_df.append(tmp_t_test_statistic_candidate_df)\n",
    "        general_meta_feature_candidates = general_meta_feature_candidates.append(tmp_general_meta_feature_candidates)\n",
    "        pbar.update(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = general_meta_feature_candidates.merge(t_test_statistic_candidate_df, on='feature_name')\n",
    "final_df.index = final_df['feature_name']\n",
    "final_df = final_df.drop(['feature_name'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 166 ms, sys: 3.93 ms, total: 170 ms\n",
      "Wall time: 178 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8999999999999999"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "from fangorn.training import classifiers\n",
    "\n",
    "base_clf = classifiers.random_forest_classifier(train_set= [X_train_ori, y_train],\n",
    "                         test_set= [X_test_ori, y_test],\n",
    "                         features= X_train_ori.columns,\n",
    "                         target= 'Class',\n",
    "                         test_metrics= ['auc'],\n",
    "                         project_name= dataset_name\n",
    "                         ) \n",
    "base_auc = base_clf['calc_metrics']['auc']\n",
    "base_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4306/4306 [10:46<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 45s, sys: 4.72 s, total: 10min 50s\n",
      "Wall time: 10min 46s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_feature_error_diff = {}\n",
    "with tqdm(total=final_df.shape[0]) as pbar:\n",
    "    for idx, row in final_df.iterrows():\n",
    "        this_X_train = X_train_ori.copy()\n",
    "        this_X_test = X_test_ori.copy()\n",
    "\n",
    "        this_X_train[idx] = X_train[idx]\n",
    "        this_X_test[idx] = X_test[idx]\n",
    "\n",
    "        this_clf = classifiers.random_forest_classifier(train_set= [this_X_train, y_train],\n",
    "                             test_set= [this_X_test, y_test],\n",
    "                             features= this_X_train.columns,\n",
    "                             target= 'Class',\n",
    "                             test_metrics= ['auc'],\n",
    "                             project_name= 'explore_kit'\n",
    "                             ) \n",
    "        this_auc = this_clf['calc_metrics']['auc']\n",
    "        error_diff = this_auc - base_auc\n",
    "        dict_feature_error_diff[idx] = error_diff\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = final_df.reset_index()\n",
    "tt['feature_goodness'] = tt['feature_name']\n",
    "tt['feature_goodness'] = tt['feature_goodness'].map(dict_feature_error_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    3853.000000\n",
       "mean        0.003002\n",
       "std         0.001059\n",
       "min         0.001000\n",
       "25%         0.002000\n",
       "50%         0.003000\n",
       "75%         0.004000\n",
       "max         0.009000\n",
       "Name: feature_goodness, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt['feature_goodness'] = np.round(tt['feature_goodness'],3)\n",
    "tt2 = tt.loc[tt['feature_goodness'] > 0]\n",
    "tt2['feature_goodness'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th_value =  tt2['feature_goodness'].quantile(.75)\n",
    "th_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep = list({i[1]:i[0] for i in sorted(zip(dict_feature_error_diff.values(), dict_feature_error_diff.keys()), reverse=True)[:300]}.keys())\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3869"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "only_features = {k: v for k, v in dict_feature_error_diff.items() if v > 0}\n",
    "keep = list(only_features.keys())\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def save_dataset_info(final_df, dict_feature_error_diff, dataset_name, base_clf, keep, dataset_based_meta_features):\n",
    "    import joblib\n",
    "    joblib.dump(keep, f'ExploreKit/{dataset_name}_{len(keep)}_features_to_keep')\n",
    "    joblib.dump(dict_feature_error_diff, f'ExploreKit/{dataset_name}_dict_feature_error_diff')\n",
    "    joblib.dump(base_clf, f'ExploreKit/{dataset_name}_base_clf')\n",
    "    # save meta feature csv\n",
    "    final_df = final_df.reset_index()\n",
    "    final_df.to_csv(f'ExploreKit/{dataset_name}_meta_features.csv', index=False)\n",
    "    final_df.index = final_df['feature_name']\n",
    "    final_df = final_df.drop(['feature_name'], axis=1)\n",
    "    \n",
    "    # save dataset feature dict\n",
    "    joblib.dump(dict_feature_error_diff, f'ExploreKit/{dataset_name}.featuredict')\n",
    "\n",
    "    # join and save final dataset for mL modeling\n",
    "    for col in dataset_based_meta_features.columns:\n",
    "        final_df[col] = list(dataset_based_meta_features[col].values) * final_df.shape[0]\n",
    "    final_df['dataset'] = dataset_name\n",
    "\n",
    "    tt = final_df.reset_index()\n",
    "    tt['feature_goodness'] = tt['feature_name']\n",
    "    tt['feature_goodness'] = tt['feature_goodness'].map(dict_feature_error_diff)\n",
    "    joblib.dump(tt, f'ExploreKit/{dataset_name}_meta_ml_modeling.df')\n",
    "    \n",
    "    return True\n",
    "\n",
    "base_clf.pop('pred_func')\n",
    "save_dataset_info(final_df, dict_feature_error_diff, dataset_name, base_clf, keep, dataset_based_meta_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_sources</th>\n",
       "      <th>num_numeric_sources</th>\n",
       "      <th>num_discrete_sources</th>\n",
       "      <th>discretizer_in_use</th>\n",
       "      <th>normalizer_in_use</th>\n",
       "      <th>group_in_use</th>\n",
       "      <th>binary_in_use</th>\n",
       "      <th>max_discrete_source_value</th>\n",
       "      <th>min_discrete_source_value</th>\n",
       "      <th>avg_discrete_source_value</th>\n",
       "      <th>...</th>\n",
       "      <th>std_numeric_source_value</th>\n",
       "      <th>max_ttest_source_opattr_value</th>\n",
       "      <th>min_ttest_source_opattr_value</th>\n",
       "      <th>avg_ttest_source_opattr_value</th>\n",
       "      <th>std_ttest_source_opattr_value</th>\n",
       "      <th>A15_avg_t</th>\n",
       "      <th>A15_std_t</th>\n",
       "      <th>A15_max_t</th>\n",
       "      <th>A15_min_t</th>\n",
       "      <th>feat_mutual_info</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>A1_group_1_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457440</td>\n",
       "      <td>19.641555</td>\n",
       "      <td>19.641555</td>\n",
       "      <td>19.641555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.749381</td>\n",
       "      <td>6.387720</td>\n",
       "      <td>19.110680</td>\n",
       "      <td>-5.388592</td>\n",
       "      <td>0.000651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A1_group_2_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.461006</td>\n",
       "      <td>38.321946</td>\n",
       "      <td>38.321946</td>\n",
       "      <td>38.321946</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32.145733</td>\n",
       "      <td>9.894304</td>\n",
       "      <td>37.755605</td>\n",
       "      <td>-5.365745</td>\n",
       "      <td>0.000722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A2_mean_encode_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219660</td>\n",
       "      <td>45.594340</td>\n",
       "      <td>45.594340</td>\n",
       "      <td>45.594340</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.148262</td>\n",
       "      <td>13.753573</td>\n",
       "      <td>45.343619</td>\n",
       "      <td>-5.383226</td>\n",
       "      <td>0.284968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A3_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5.162138</td>\n",
       "      <td>-16.969445</td>\n",
       "      <td>-16.969445</td>\n",
       "      <td>-16.969445</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20.964788</td>\n",
       "      <td>8.805834</td>\n",
       "      <td>28.622142</td>\n",
       "      <td>-5.397496</td>\n",
       "      <td>0.058093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A4_group_1_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.432112</td>\n",
       "      <td>43.829684</td>\n",
       "      <td>43.829684</td>\n",
       "      <td>43.829684</td>\n",
       "      <td>0.0</td>\n",
       "      <td>37.115588</td>\n",
       "      <td>11.069880</td>\n",
       "      <td>43.829684</td>\n",
       "      <td>-5.363285</td>\n",
       "      <td>0.036568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15_x_A8_norm_op_sub_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3920.788931</td>\n",
       "      <td>-5.378551</td>\n",
       "      <td>-5.378551</td>\n",
       "      <td>-5.378551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>108.275576</td>\n",
       "      <td>42.707906</td>\n",
       "      <td>142.849797</td>\n",
       "      <td>-5.379006</td>\n",
       "      <td>0.094184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15_x_A8_norm_op_mul_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>733.942559</td>\n",
       "      <td>-3.804605</td>\n",
       "      <td>-3.804605</td>\n",
       "      <td>-3.804605</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.902954</td>\n",
       "      <td>15.279133</td>\n",
       "      <td>39.706153</td>\n",
       "      <td>-16.636023</td>\n",
       "      <td>0.029240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15_x_A11_norm_op_sum_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3920.810226</td>\n",
       "      <td>-5.404439</td>\n",
       "      <td>-5.404439</td>\n",
       "      <td>-5.404439</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.002722</td>\n",
       "      <td>11.675431</td>\n",
       "      <td>29.021582</td>\n",
       "      <td>-15.895196</td>\n",
       "      <td>0.170537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15_x_A11_norm_op_sub_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3920.791569</td>\n",
       "      <td>-5.380246</td>\n",
       "      <td>-5.380246</td>\n",
       "      <td>-5.380246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>130.324480</td>\n",
       "      <td>54.734898</td>\n",
       "      <td>188.492132</td>\n",
       "      <td>-5.380443</td>\n",
       "      <td>0.164580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A15_x_A11_norm_op_mul_disc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>353.897891</td>\n",
       "      <td>-4.462223</td>\n",
       "      <td>-4.462223</td>\n",
       "      <td>-4.462223</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.604874</td>\n",
       "      <td>13.381971</td>\n",
       "      <td>33.842335</td>\n",
       "      <td>-16.447213</td>\n",
       "      <td>0.044298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4306 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            num_sources  num_numeric_sources  \\\n",
       "feature_name                                                   \n",
       "A1_group_1_disc                       1                    1   \n",
       "A1_group_2_disc                       1                    1   \n",
       "A2_mean_encode_disc                   1                    1   \n",
       "A3_disc                               1                    1   \n",
       "A4_group_1_disc                       1                    1   \n",
       "...                                 ...                  ...   \n",
       "A15_x_A8_norm_op_sub_disc             1                    1   \n",
       "A15_x_A8_norm_op_mul_disc             1                    1   \n",
       "A15_x_A11_norm_op_sum_disc            1                    1   \n",
       "A15_x_A11_norm_op_sub_disc            1                    1   \n",
       "A15_x_A11_norm_op_mul_disc            1                    1   \n",
       "\n",
       "                            num_discrete_sources  discretizer_in_use  \\\n",
       "feature_name                                                           \n",
       "A1_group_1_disc                                0                True   \n",
       "A1_group_2_disc                                0                True   \n",
       "A2_mean_encode_disc                            0                True   \n",
       "A3_disc                                        0                True   \n",
       "A4_group_1_disc                                0                True   \n",
       "...                                          ...                 ...   \n",
       "A15_x_A8_norm_op_sub_disc                      0                True   \n",
       "A15_x_A8_norm_op_mul_disc                      0                True   \n",
       "A15_x_A11_norm_op_sum_disc                     0                True   \n",
       "A15_x_A11_norm_op_sub_disc                     0                True   \n",
       "A15_x_A11_norm_op_mul_disc                     0                True   \n",
       "\n",
       "                            normalizer_in_use  group_in_use  binary_in_use  \\\n",
       "feature_name                                                                 \n",
       "A1_group_1_disc                         False         False          False   \n",
       "A1_group_2_disc                         False         False          False   \n",
       "A2_mean_encode_disc                     False         False          False   \n",
       "A3_disc                                 False         False          False   \n",
       "A4_group_1_disc                         False         False          False   \n",
       "...                                       ...           ...            ...   \n",
       "A15_x_A8_norm_op_sub_disc               False         False          False   \n",
       "A15_x_A8_norm_op_mul_disc               False         False          False   \n",
       "A15_x_A11_norm_op_sum_disc              False         False          False   \n",
       "A15_x_A11_norm_op_sub_disc              False         False          False   \n",
       "A15_x_A11_norm_op_mul_disc              False         False          False   \n",
       "\n",
       "                            max_discrete_source_value  \\\n",
       "feature_name                                            \n",
       "A1_group_1_disc                                   0.0   \n",
       "A1_group_2_disc                                   0.0   \n",
       "A2_mean_encode_disc                               0.0   \n",
       "A3_disc                                           0.0   \n",
       "A4_group_1_disc                                   0.0   \n",
       "...                                               ...   \n",
       "A15_x_A8_norm_op_sub_disc                         0.0   \n",
       "A15_x_A8_norm_op_mul_disc                         0.0   \n",
       "A15_x_A11_norm_op_sum_disc                        0.0   \n",
       "A15_x_A11_norm_op_sub_disc                        0.0   \n",
       "A15_x_A11_norm_op_mul_disc                        0.0   \n",
       "\n",
       "                            min_discrete_source_value  \\\n",
       "feature_name                                            \n",
       "A1_group_1_disc                                   0.0   \n",
       "A1_group_2_disc                                   0.0   \n",
       "A2_mean_encode_disc                               0.0   \n",
       "A3_disc                                           0.0   \n",
       "A4_group_1_disc                                   0.0   \n",
       "...                                               ...   \n",
       "A15_x_A8_norm_op_sub_disc                         0.0   \n",
       "A15_x_A8_norm_op_mul_disc                         0.0   \n",
       "A15_x_A11_norm_op_sum_disc                        0.0   \n",
       "A15_x_A11_norm_op_sub_disc                        0.0   \n",
       "A15_x_A11_norm_op_mul_disc                        0.0   \n",
       "\n",
       "                            avg_discrete_source_value  ...  \\\n",
       "feature_name                                           ...   \n",
       "A1_group_1_disc                                   0.0  ...   \n",
       "A1_group_2_disc                                   0.0  ...   \n",
       "A2_mean_encode_disc                               0.0  ...   \n",
       "A3_disc                                           0.0  ...   \n",
       "A4_group_1_disc                                   0.0  ...   \n",
       "...                                               ...  ...   \n",
       "A15_x_A8_norm_op_sub_disc                         0.0  ...   \n",
       "A15_x_A8_norm_op_mul_disc                         0.0  ...   \n",
       "A15_x_A11_norm_op_sum_disc                        0.0  ...   \n",
       "A15_x_A11_norm_op_sub_disc                        0.0  ...   \n",
       "A15_x_A11_norm_op_mul_disc                        0.0  ...   \n",
       "\n",
       "                            std_numeric_source_value  \\\n",
       "feature_name                                           \n",
       "A1_group_1_disc                             0.457440   \n",
       "A1_group_2_disc                             0.461006   \n",
       "A2_mean_encode_disc                         0.219660   \n",
       "A3_disc                                     5.162138   \n",
       "A4_group_1_disc                             0.432112   \n",
       "...                                              ...   \n",
       "A15_x_A8_norm_op_sub_disc                3920.788931   \n",
       "A15_x_A8_norm_op_mul_disc                 733.942559   \n",
       "A15_x_A11_norm_op_sum_disc               3920.810226   \n",
       "A15_x_A11_norm_op_sub_disc               3920.791569   \n",
       "A15_x_A11_norm_op_mul_disc                353.897891   \n",
       "\n",
       "                            max_ttest_source_opattr_value  \\\n",
       "feature_name                                                \n",
       "A1_group_1_disc                                 19.641555   \n",
       "A1_group_2_disc                                 38.321946   \n",
       "A2_mean_encode_disc                             45.594340   \n",
       "A3_disc                                        -16.969445   \n",
       "A4_group_1_disc                                 43.829684   \n",
       "...                                                   ...   \n",
       "A15_x_A8_norm_op_sub_disc                       -5.378551   \n",
       "A15_x_A8_norm_op_mul_disc                       -3.804605   \n",
       "A15_x_A11_norm_op_sum_disc                      -5.404439   \n",
       "A15_x_A11_norm_op_sub_disc                      -5.380246   \n",
       "A15_x_A11_norm_op_mul_disc                      -4.462223   \n",
       "\n",
       "                            min_ttest_source_opattr_value  \\\n",
       "feature_name                                                \n",
       "A1_group_1_disc                                 19.641555   \n",
       "A1_group_2_disc                                 38.321946   \n",
       "A2_mean_encode_disc                             45.594340   \n",
       "A3_disc                                        -16.969445   \n",
       "A4_group_1_disc                                 43.829684   \n",
       "...                                                   ...   \n",
       "A15_x_A8_norm_op_sub_disc                       -5.378551   \n",
       "A15_x_A8_norm_op_mul_disc                       -3.804605   \n",
       "A15_x_A11_norm_op_sum_disc                      -5.404439   \n",
       "A15_x_A11_norm_op_sub_disc                      -5.380246   \n",
       "A15_x_A11_norm_op_mul_disc                      -4.462223   \n",
       "\n",
       "                            avg_ttest_source_opattr_value  \\\n",
       "feature_name                                                \n",
       "A1_group_1_disc                                 19.641555   \n",
       "A1_group_2_disc                                 38.321946   \n",
       "A2_mean_encode_disc                             45.594340   \n",
       "A3_disc                                        -16.969445   \n",
       "A4_group_1_disc                                 43.829684   \n",
       "...                                                   ...   \n",
       "A15_x_A8_norm_op_sub_disc                       -5.378551   \n",
       "A15_x_A8_norm_op_mul_disc                       -3.804605   \n",
       "A15_x_A11_norm_op_sum_disc                      -5.404439   \n",
       "A15_x_A11_norm_op_sub_disc                      -5.380246   \n",
       "A15_x_A11_norm_op_mul_disc                      -4.462223   \n",
       "\n",
       "                            std_ttest_source_opattr_value   A15_avg_t  \\\n",
       "feature_name                                                            \n",
       "A1_group_1_disc                                       0.0   14.749381   \n",
       "A1_group_2_disc                                       0.0   32.145733   \n",
       "A2_mean_encode_disc                                   0.0   36.148262   \n",
       "A3_disc                                               0.0   20.964788   \n",
       "A4_group_1_disc                                       0.0   37.115588   \n",
       "...                                                   ...         ...   \n",
       "A15_x_A8_norm_op_sub_disc                             0.0  108.275576   \n",
       "A15_x_A8_norm_op_mul_disc                             0.0   18.902954   \n",
       "A15_x_A11_norm_op_sum_disc                            0.0   16.002722   \n",
       "A15_x_A11_norm_op_sub_disc                            0.0  130.324480   \n",
       "A15_x_A11_norm_op_mul_disc                            0.0   16.604874   \n",
       "\n",
       "                            A15_std_t   A15_max_t  A15_min_t  feat_mutual_info  \n",
       "feature_name                                                                    \n",
       "A1_group_1_disc              6.387720   19.110680  -5.388592          0.000651  \n",
       "A1_group_2_disc              9.894304   37.755605  -5.365745          0.000722  \n",
       "A2_mean_encode_disc         13.753573   45.343619  -5.383226          0.284968  \n",
       "A3_disc                      8.805834   28.622142  -5.397496          0.058093  \n",
       "A4_group_1_disc             11.069880   43.829684  -5.363285          0.036568  \n",
       "...                               ...         ...        ...               ...  \n",
       "A15_x_A8_norm_op_sub_disc   42.707906  142.849797  -5.379006          0.094184  \n",
       "A15_x_A8_norm_op_mul_disc   15.279133   39.706153 -16.636023          0.029240  \n",
       "A15_x_A11_norm_op_sum_disc  11.675431   29.021582 -15.895196          0.170537  \n",
       "A15_x_A11_norm_op_sub_disc  54.734898  188.492132  -5.380443          0.164580  \n",
       "A15_x_A11_norm_op_mul_disc  13.381971   33.842335 -16.447213          0.044298  \n",
       "\n",
       "[4306 rows x 28 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_discretize_multi_gran(X_train, X_test, max_gran=10):\n",
    "    \"\"\"\n",
    "    multi-granularity discretization\n",
    "    method. The basic idea is simple: instead of using a fine-tuned\n",
    "    granularity, we discretize each numerical feature into several, rather\n",
    "    than only one, categorical features, each with a different granularity.\n",
    "    \n",
    "    min granularity = 3\n",
    "    \n",
    "    Sometimes de edge values did not permit to execute correct discretization\n",
    "    if this happens the step is not executed\n",
    "    \"\"\"\n",
    "    \n",
    "    # separa dados numericos que precisam de binarizacao\n",
    "    is_numeric = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    numeric_features = X_train.select_dtypes(include=is_numeric)\n",
    "    discrete_features = []\n",
    "    print(f\"Discretizing {len(numeric_features.columns)} features...\")\n",
    "    feat_count = 0\n",
    "    for feat in numeric_features:\n",
    "        if feat_count % 50 == 0:\n",
    "            print(f\" Working in {feat}\")\n",
    "        X_train_np = X_train[[feat]].to_numpy()\n",
    "        X_test_np = X_test[[feat]].to_numpy()\n",
    "        for gran in range(3, max_gran+1):\n",
    "            try:\n",
    "                D_train = np.zeros([X_train.shape[0], 1])\n",
    "                D_test = np.zeros([X_test.shape[0], 1])\n",
    "                # calc numpy histogram and apply to features\n",
    "                hist, bin_edges = np.histogram(X_train_np[:, 0], bins=gran)\n",
    "                D_train[:, 0] = np.digitize(X_train_np[:,0], bin_edges, right=False)\n",
    "                D_test[:, 0] = np.digitize(X_test_np[:,0], bin_edges, right=False)\n",
    "\n",
    "                # apply back to pandas\n",
    "                X_train[f\"{feat}_{gran}\"] = D_train\n",
    "                X_test[f\"{feat}_{gran}\"] = D_test\n",
    "            except:\n",
    "                print(f\"Not possible to correct work on cut {feat} > {gran}\")\n",
    "                break\n",
    "        \n",
    "        feat_count += 1\n",
    "        X_train = X_train.drop(feat, axis=1)\n",
    "        X_test = X_test.drop(feat, axis=1)\n",
    "        \n",
    "    return X_train, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discretizing 32 features...\n",
      " Working in xp_transacoes_movto_mov_bolsa_15_sum\n"
     ]
    }
   ],
   "source": [
    "X_train_discrete, X_test_discrete = numpy_discretize_multi_gran(X_train.copy(), X_test.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hjmi_selector(X, y, bins, max_features):\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    Y = y.to_numpy().ravel()\n",
    "\n",
    "    [tmp, features] = X.shape\n",
    "    D = np.zeros([tmp, features])\n",
    "\n",
    "    for i in range(features):\n",
    "        N, E = np.histogram(X[:,i], bins=bins)\n",
    "        D[:,i] = np.digitize(X[:,i], E, right=False)\n",
    "\n",
    "    selected_features = []\n",
    "    j_h = 0\n",
    "    hjmi = None\n",
    "    for i in range(0,max_features):\n",
    "        JMI = np.zeros([features], dtype=np.float)\n",
    "        for X_k in range(features):\n",
    "            if X_k in selected_features:\n",
    "                continue\n",
    "            jmi_1 = pymit.I(D[:,X_k], Y, bins=[bins,2])\n",
    "            jmi_2 = 0\n",
    "            for X_j in selected_features:\n",
    "                tmp1 = pymit.I(D[:,X_k], D[:,X_j], bins=[bins,bins])\n",
    "                tmp2 = pymit.I_cond(D[:,X_k], D[:,X_j], Y, bins=[bins,bins,2])\n",
    "                jmi_2 += tmp1 - tmp2\n",
    "            if len(selected_features) == 0:\n",
    "                JMI[X_k] += j_h + jmi_1\n",
    "            else:\n",
    "                JMI[X_k] += j_h + jmi_1 - jmi_2/len(selected_features)\n",
    "        \n",
    "        f = JMI.argmax()\n",
    "        j_h = JMI[f]\n",
    "        if (hjmi == None) or ((j_h - hjmi)/hjmi > 0.03):\n",
    "            r = 0\n",
    "            if hjmi != None:\n",
    "                r = ((j_h - hjmi)/hjmi) \n",
    "\n",
    "            hjmi = j_h\n",
    "            selected_features.append(f)\n",
    "            print(\"{:0>3d} {:>3d} {} - {}\".format(len(selected_features), f, j_h, r))\n",
    "        else:\n",
    "            return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-b902b0a4cd00>\u001b[0m in \u001b[0;36mhjmi_selector\u001b[0;34m(X, y, bins, max_features)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mD\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdigitize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36mhistogram\u001b[0;34m(a, bins, range, normed, weights, density)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ravel_and_check_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 795\u001b[0;31m     \u001b[0mbin_edges\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muniform_bins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_bin_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;31m# Histogram is an integer or a float array depending on the weights.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m_get_bin_edges\u001b[0;34m(a, bins, range, weights)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'`bins` must be positive, when an integer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mfirst_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_outer_edges\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbins\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/basic/lib/python3.7/site-packages/numpy/lib/histograms.py\u001b[0m in \u001b[0;36m_get_outer_edges\u001b[0;34m(a, range)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mfirst_edge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_edge\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_edge\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_edge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m             raise ValueError(\n\u001b[1;32m    327\u001b[0m                 \"autodetected range of [{}, {}] is not finite\".format(first_edge, last_edge))\n",
      "\u001b[0;31mTypeError\u001b[0m: ufunc 'isfinite' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "selected_features = hjmi_selector(X_train_discrete.copy(), y_train.copy(), bins=10, max_features=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_train = X_train_discrete[X_train_discrete.columns[selected_features]]\n",
    "filtered_test = X_test_discrete[X_test_discrete.columns[selected_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fangorn.training import classifiers\n",
    "\n",
    "this_clf = classifiers.random_forest_classifier(train_set= [filtered_train, y_train],\n",
    "                     test_set= [filtered_test, y_test],\n",
    "                     features= filtered_test.columns,\n",
    "                     target= target,\n",
    "                     test_metrics= ['auc'],\n",
    "                     project_name= 'explore_kit'\n",
    "                     ) \n",
    "this_auc = this_clf['calc_metrics']['auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8220322886989553"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['theta2_x_theta3_op_sum_10', 'theta3_x_theta2_norm_op_sub_10',\n",
       "       'theta2_x_theta3_norm_op_sum_10', 'theta2_norm_x_theta3_norm_op_sum_9',\n",
       "       'theta3_x_theta2_norm_op_sum_10', 'theta1_x_tau1_norm_op_sum_disc_10',\n",
       "       'theta2_norm_x_theta3_norm_op_sum_10', 'theta2_disc_10',\n",
       "       'theta1_norm_x_tau1_norm_op_sum_disc_10', 'theta2_x_theta3_op_sum_9',\n",
       "       'theta2_x_theta3_op_sub_10', 'theta1_x_tau1_norm_op_sum_disc_9',\n",
       "       'theta1_norm_x_tau1_norm_op_sum_disc_9',\n",
       "       'theta2_norm_x_theta3_norm_op_mul_10', 'theta3_disc_10',\n",
       "       'theta2_x_theta3_op_sum_8', 'theta2_x_theta3_op_sum_7',\n",
       "       'theta2_norm_x_theta3_norm_op_sum_8',\n",
       "       'theta1_norm_x_tau1_norm_op_mul_disc_10',\n",
       "       'theta2_norm_x_theta3_norm_op_sum_7',\n",
       "       'theta2_x_theta1_norm_op_sum_disc_10',\n",
       "       'theta2_norm_x_theta3_norm_op_mul_9',\n",
       "       'theta1_x_tau1_norm_op_sum_disc_8'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_train.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
