{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Tuple, Union\n",
    "\n",
    "import os\n",
    "import configparser\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_file(filename):\n",
    "    ''' Get all information {attribute = value} pairs from the public.info file'''\n",
    "    info = {}\n",
    "    with open (filename, \"r\") as info_file:\n",
    "        lines = info_file.readlines()\n",
    "        features_list = list(map(lambda x: tuple(x.strip(\"\\'\").split(\" = \")), lines))\n",
    "\n",
    "        for (key, value) in features_list:\n",
    "            info[key] = value.rstrip().strip(\"'\").strip(' ')\n",
    "            if info[key].isdigit(): # if we have a number, we want it to be an integer\n",
    "                info[key] = int(info[key])\n",
    "    return info     \n",
    "\n",
    "\n",
    "def read_prepare_data(dataset_to_work: str) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Read dataset from automl challenge returnig pandas dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    def _label_powerset(y_train: pd.DataFrame, inv_map: dict = None):\n",
    "        \"\"\"\n",
    "        Execute a label powerset Transformation\n",
    "        \"\"\"\n",
    "        y_train['tmp_all'] = y_train.astype(str).sum(axis=1)\n",
    "\n",
    "        if not inv_map:\n",
    "            map_values = pd.DataFrame(y_train['tmp_all'].value_counts()).reset_index().drop(['tmp_all'], axis=1).to_dict()['index']\n",
    "            inv_map = {v: k for k, v in map_values.items()}\n",
    "\n",
    "        y_train['class'] = y_train['tmp_all'].map(inv_map)\n",
    "        return y_train[['class']], inv_map\n",
    "\n",
    "    config_parser = configparser.ConfigParser()\n",
    "    config_parser.read(\"data_links.ini\")    \n",
    "    # caminho base dos arquivos baixados\n",
    "    data_path = dict(config_parser.items('ML_CHALLENGE_DATA_PATH'))['path']\n",
    "    # datasets baixados\n",
    "\n",
    "    dataset_path = (f\"{data_path}/{dataset_to_work}/{dataset_to_work}\")\n",
    "    valid_path = (f\"{data_path}/valid_solution/{dataset_to_work}\")\n",
    "    info_dict = get_info_from_file(dataset_path+\"_public.info\")\n",
    "    \n",
    "    task = info_dict['task']\n",
    "\n",
    "    X_all = pd.read_csv(dataset_path+\"_train.data\", sep=\" \", header=None, usecols=[i for i in range(0,info_dict['feat_num'])] )\n",
    "    y_all =  pd.read_csv(dataset_path+\"_train.solution\", sep=\" \", header=None, usecols=[i for i in range(0,info_dict['target_num'])] )\n",
    "\n",
    "#     X_valid = pd.read_csv(dataset_path+\"_valid.data\", sep=\" \", header=None, usecols=[i for i in range(0,info_dict['feat_num'])] )\n",
    "#     y_valid = pd.read_csv(valid_path+\"_valid.solution\", sep=\" \", header=None, usecols=[i for i in range(0,info_dict['target_num'])] )\n",
    "\n",
    "    if task == 'multilabel.classification':\n",
    "        # transforma em problema multiclasse\n",
    "        # conta os valroes de cada classe, transforma em um dict, e troca os indices do dict para mapearmos cada classe (Label Powerset)\n",
    "        y_all, inv_map = _label_powerset(y_train)\n",
    "#         y_valid, _ = _label_powerset(y_valid, inv_map) \n",
    "        \n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working in christine\n",
      "working in jasmine\n",
      "working in philippine\n",
      "working in madeline\n",
      "working in sylvine\n"
     ]
    }
   ],
   "source": [
    "config_parser = configparser.ConfigParser()\n",
    "config_parser.read(\"data_links.ini\") \n",
    "for dataset in list(dict(config_parser.items('ML_CHALLENGE_DATA_LINKS')).keys()):\n",
    "    print(f\"working in {dataset}\")\n",
    "    X_all, y_all =read_prepare_data(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
